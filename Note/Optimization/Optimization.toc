\contentsline {chapter}{\numberline {1}Extreme Value and Coercive Functions}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Bolzano-Weierstrass theorem}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Sequence Convergence}{1}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Bolzano-Weierstrass Theorem: Compact set $S$, $\exists $ subsequence converges to $\vec {x}^*\in S$}{1}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Extreme Value Theorem: continuous $f$ compact set $\rightarrow \mathbb {R}$ has global-min}{1}{subsection.1.1.3}%
\contentsline {subsection}{\numberline {1.1.4}Corollary: Non-empty and Compact level set $\{x \mid f(x) \leq c\}$ $\Rightarrow $ $\exists $ $f$'s global-min/max}{1}{subsection.1.1.4}%
\contentsline {section}{\numberline {1.2}Coercive function}{2}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Def: Coercive function}{2}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Coercive function $f$ $\Rightarrow $ $\exists $ global-min}{2}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Lemma: $f: \mathbb {R}^n \rightarrow \mathbb {R}$ is coercive $\Leftrightarrow $ $\lim _{\|\vec {x}\| \rightarrow \infty }f(\vec {x})=+\infty $ for all possible directions.}{2}{subsection.1.2.3}%
\contentsline {subsection}{\numberline {1.2.4}Lemma: sum of \textit {coercive functions} is a coercive function}{2}{subsection.1.2.4}%
\contentsline {subsection}{\numberline {1.2.5}Lemma: \textit {coercive function} $+$ \textit {bounded below function} is a coercive function}{3}{subsection.1.2.5}%
\contentsline {subsection}{\numberline {1.2.6}Get coercive function: convex $f$, $f^\varepsilon (\vec {x})=f(\vec {x})+\varepsilon \|\vec {x}\|^2$ is coercive}{3}{subsection.1.2.6}%
\contentsline {section}{\numberline {1.3}Polynomials Coercive}{3}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Quadratic forms: $A_{n\times n}$ is positive definite $\Leftrightarrow $ quadratic form $f(\vec {x})=\vec {x}^TA \vec {x}$ is coercive}{3}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Higher-degree polynomials}{4}{subsection.1.3.2}%
\contentsline {chapter}{\numberline {2}Unconstrained Optimization}{5}{chapter.2}%
\contentsline {section}{\numberline {2.1}Basic Definitions}{5}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Optimization in a Set}{5}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}Minimizer}{5}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}Stationary Point, Saddle Point}{5}{subsection.2.1.3}%
\contentsline {subsection}{\numberline {2.1.4}Conditions for Global Minimizer: (1) exists global-minimizer; (2) has the minimum value in all stationary points}{6}{subsection.2.1.4}%
\contentsline {section}{\numberline {2.2}Special Situation: Optimization in $\mathbb {R}$}{6}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Necessary condition of local-min: $f'(x^*)=0$}{6}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Sufficient condition of local-min: $f'(x^*)=0, f''(x^*)\geq 0$}{7}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Sufficient condition of global-min: $f'(x^*)=0$ and $f''(x)\geq 0,\forall x\in I$}{7}{subsection.2.2.3}%
\contentsline {section}{\numberline {2.3}Restriction to a Line}{7}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Definition: $\phi _{\vec {u}}(t)=f(\vec {x}+t\vec {u}),\ \vec {x},\vec {u}\in \mathbb {R}^n, t\in \mathbb {R}$}{7}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Derivatives: $\phi '_{\vec {u}}(t)=\nabla f(\vec {x}+t\vec {u})\vec {u}$, $\phi ^{''}_{\vec {u}} (t)=\vec {u}^T {Hf}(\vec {x}+t\vec {u})\vec {u}$}{7}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Lemma: $x^*$ is a global minimizer of $f$ $\Leftrightarrow $ $t=0$ is the global minimizer of $\phi _{\vec {u}}(t)=f(\vec {x}+t\vec {u})$, $\forall \vec {u}\in \mathbb {R}^n$}{8}{subsection.2.3.3}%
\contentsline {section}{\numberline {2.4}General: Optimization in $\mathbb {R}^n$}{8}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Local-min Necessary Condition $1$: $\nabla f$ is continuous, $x^*$ is a local minimizer $\Rightarrow \nabla f(x^*)=0$}{8}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Local-min Necessary Condition $2$: $Hf$ is continuous, $x^*$ is a local minimizer $\Rightarrow \nabla ^2 f(x^*)\succeq 0$}{9}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Local-min Sufficient Condition $1$: $Hf$ is continuous, $\nabla f(\vec {x}^*)=0$, $\vec {u}^T Hf(\vec {x}) \vec {u}\geq 0,\forall \vec {u}\in \mathbb {R}^n$ and $\exists r>0, \|\vec {x}-\vec {x}^*\|<r$ $\Rightarrow $ $\vec {x}^*$ is a local minimizer}{10}{subsection.2.4.3}%
\contentsline {subsection}{\numberline {2.4.4}Local-min Sufficient Condition $1'$: $Hf$ is continuous, $\nabla f(\vec {x}^*)=0$, $\nabla ^2 f(\vec {x}^*)\succ 0$ $\Rightarrow $ $\vec {x}^*$ is a local minimizer}{10}{subsection.2.4.4}%
\contentsline {subsection}{\numberline {2.4.5}Global-min Sufficient Condition: $Hf$ is continuous, $\nabla f(\vec {x}^*)=0$, $\nabla ^2f(\vec {x})\succeq 0,\forall \vec {x}$ $\Rightarrow $ $\vec {x}^*$ is a global minimizer}{11}{subsection.2.4.5}%
\contentsline {subsection}{\numberline {2.4.6}$Hf(\vec {x}^*)$ is indefinite $\Rightarrow $ $\vec {x}^*$ is saddle point}{11}{subsection.2.4.6}%
\contentsline {subsection}{\numberline {2.4.7}$Hf(\vec {x}^*)\succ 0$/$\prec 0$ $\Rightarrow $ critical point $\vec {x}^*$ is strictly local-min/local-max}{11}{subsection.2.4.7}%
\contentsline {subsection}{\numberline {2.4.8}Steps to Find Minimum in $\mathbb {R}^n$}{12}{subsection.2.4.8}%
\contentsline {section}{\numberline {2.5}Existence of Global-min}{12}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}(Bolzano-)Weierstrass Theorem: Compact set $X$ $\Rightarrow $ $\exists $ global-min/max}{12}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}Def: Coercive function}{13}{subsection.2.5.2}%
\contentsline {subsection}{\numberline {2.5.3}Coercive function $f$ $\Rightarrow $ $\exists $ global-min}{13}{subsection.2.5.3}%
\contentsline {subsection}{\numberline {2.5.4}Method of finding-global-min-among-stationary-points (FGMSP)}{13}{subsection.2.5.4}%
\contentsline {chapter}{\numberline {3}Convexity}{14}{chapter.3}%
\contentsline {section}{\numberline {3.1}Convex Set}{14}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Prop: convex sets $e_1,e_2,...e_n$, then $\cap _{i=1}^ne_i$ is convex}{14}{subsection.3.1.1}%
\contentsline {section}{\numberline {3.2}Convex Hull}{15}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Convex Hull $conv(S)$ is the set of all convex combinations of points in $S$}{15}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Theorem: convex set $S$, convex combination $\lambda _1 x_1+\cdots +\lambda _k x_k\in S$, $\forall x_1,...,x_k\in S$}{15}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}Corollary: $conv(S)$ is the smallest convex set containing $S$}{15}{subsection.3.2.3}%
\contentsline {section}{\numberline {3.3}Optimization over Convex Set}{15}{section.3.3}%
\contentsline {section}{\numberline {3.4}Convex Function}{16}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Definition: $f$ is convex $\Leftrightarrow $ $f(\alpha x+(1-\alpha ) y) \leq \alpha f(x)+(1-\alpha ) f(y), \forall x, y \in C, \forall \alpha \in [0,1]$}{16}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}First-order: $f$ is convex $\Leftrightarrow $ $f(z) \geq f(x)+(z-x)^{T} \nabla f(x), \forall x, z \in C$}{16}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Prop: local-min$\Rightarrow \nabla f(x^*)^T(x-x^*)\geq 0,\forall x\in \&$ $\Leftrightarrow $ global-min in convex }{17}{subsection.3.4.3}%
\contentsline {subsection}{\numberline {3.4.4}Second-order: $f$ is convex $\Leftrightarrow $ $\nabla ^{2} f(x) \succeq 0,\ \forall x \in C$}{18}{subsection.3.4.4}%
\contentsline {subsection}{\numberline {3.4.5}Sufficient Condition of Strictly Convex: $\nabla ^{2} f(x) \succ 0$}{19}{subsection.3.4.5}%
\contentsline {subsection}{\numberline {3.4.6}Prop: Max and Linear combination of convex functions are also convex}{19}{subsection.3.4.6}%
\contentsline {section}{\numberline {3.5}Lemma: function $f$ is a convex function iff $\phi (t)=f(\vec {x}+t\vec {u})$ is convex of $t$}{20}{section.3.5}%
\contentsline {section}{\numberline {3.6}Proposition: Convex function $f$, $\nabla f(x^*)=0$ $\Rightarrow $ global-min}{20}{section.3.6}%
\contentsline {section}{\numberline {3.7}Application: Unconstrained Quadratic Optimization}{20}{section.3.7}%
\contentsline {section}{\numberline {3.8}Theorem: If $f$ is convex and $g$ is convex and increasing, $(g\cdot f)(x)$ is convex}{22}{section.3.8}%
\contentsline {section}{\numberline {3.9}Corollary: $f$ is linear, $g$ is convex (not necessarily increasing) $\Rightarrow $ $g\cdot f$ is convex}{23}{section.3.9}%
\contentsline {section}{\numberline {3.10}Epigraph and Jensen's Inequality}{24}{section.3.10}%
\contentsline {subsection}{\numberline {3.10.1}Def: epigraph ${epi}(f)=\{(x,y)\in C\times \mathbb {R}:y\geq f(x)\}$}{24}{subsection.3.10.1}%
\contentsline {subsection}{\numberline {3.10.2}Lemma: $f$ is convex function $\Leftrightarrow $ ${epi}(f)$ is a convex set}{24}{subsection.3.10.2}%
\contentsline {subsection}{\numberline {3.10.3}Jensen's Inequality: $f(\DOTSB \sum@ \slimits@ _{i=1}^k\lambda _i x_i)\leq \DOTSB \sum@ \slimits@ _{i=1}^k\lambda _if(x_i)$}{24}{subsection.3.10.3}%
\contentsline {section}{\numberline {3.11}Subgradients of Convex Functions}{25}{section.3.11}%
\contentsline {subsection}{\numberline {3.11.1}Sub-gradient $\vec {d}$: $f(\vec {x})\geq f(\vec {x}^*)+\vec {d}\cdot (\vec {x}-\vec {x}^*), \forall \vec {x}\in C$}{25}{subsection.3.11.1}%
\contentsline {subsection}{\numberline {3.11.2}Sub-differential: set of all sub-gradient}{26}{subsection.3.11.2}%
\contentsline {subsection}{\numberline {3.11.3}More examples}{27}{subsection.3.11.3}%
\contentsline {chapter}{\numberline {4}Geometric Programming (GP)}{29}{chapter.4}%
\contentsline {section}{\numberline {4.1}Arithmetic Mean-Geometric Mean Inequality (A-G inequality) $\delta _1x_2+\delta _2x_2+\cdots +\delta _nx_n\geq x_1^{\delta _1}x_2^{\delta _2}\cdots x_n^{\delta _n}$}{29}{section.4.1}%
\contentsline {section}{\numberline {4.2}Unconstrained Geometric Programs}{30}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Def: Posynomial}{30}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}General Strategy: A-G inequality}{30}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Dual of the Unconstrained GP}{30}{subsection.4.2.3}%
\contentsline {chapter}{\numberline {5}Polynomial Interpolation}{33}{chapter.5}%
\contentsline {section}{\numberline {5.1}Method 1: $M\vec {a}=\vec {y}$}{33}{section.5.1}%
\contentsline {section}{\numberline {5.2}Method 2: Lagrange Interpolation Formula}{33}{section.5.2}%
\contentsline {section}{\numberline {5.3}Lines of Best Fit}{34}{section.5.3}%
\contentsline {section}{\numberline {5.4}Least-Square Problem (Overconstrainted $A \vec {x}= \vec {b}$)}{34}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}Lemma: closest point $\Leftrightarrow $ $(A\vec {x}^*-\vec {y})\bot \vec {a},\ \forall \vec {a}\in V$}{34}{subsection.5.4.1}%
\contentsline {subsection}{\numberline {5.4.2}Theorem: $\vec {x}^*=(A^TA)^{-1}A^T\vec {y}=A^+ \vec {y}$}{35}{subsection.5.4.2}%
\contentsline {subsection}{\numberline {5.4.3}Def: Projection Matrix: $P=AA^+$; Projection of $\vec {y}$ on $V$: $A \vec {x}^*=P \vec {y}$}{35}{subsection.5.4.3}%
\contentsline {subsection}{\numberline {5.4.4}Special Case: Projection on vector ${Proj}_{\vec {a}}(\vec {y})=\frac {(\vec {a}\cdot \vec {y})\vec {a}}{\|\vec {a}\|^2}$}{36}{subsection.5.4.4}%
\contentsline {subsection}{\numberline {5.4.5}Theorem: Projection Matrix $=$ Sum of outer products of orthonormal basis}{36}{subsection.5.4.5}%
\contentsline {subsection}{\numberline {5.4.6}Corollary: $Q$ has orthonormal columns $\Rightarrow $ $\vec {x}^*=Q^T \vec {y}$. ($Q^+=Q^T$)}{37}{subsection.5.4.6}%
\contentsline {subsection}{\numberline {5.4.7}The Gram-Schmidt process}{37}{subsection.5.4.7}%
\contentsline {section}{\numberline {5.5}Minimum-norm problems (Underconstrainted $A \vec {x}= \vec {b}$)}{38}{section.5.5}%
\contentsline {subsection}{\numberline {5.5.1}Applying the least-squares technique}{38}{subsection.5.5.1}%
\contentsline {subsection}{\numberline {5.5.2}The short cut method}{39}{subsection.5.5.2}%
\contentsline {subsection}{\numberline {5.5.3}The short cut method with $H$-norm}{40}{subsection.5.5.3}%
\contentsline {chapter}{\numberline {6}The Closest Point: Projection}{42}{chapter.6}%
\contentsline {section}{\numberline {6.1}Set Theory Basis}{42}{section.6.1}%
\contentsline {section}{\numberline {6.2}Def: Projection $[z]^\&$}{42}{section.6.2}%
\contentsline {section}{\numberline {6.3}\underline {Unique} projection $[z]^\&$ on \underline {closed convex} subset of $\mathbb {R}^n$}{43}{section.6.3}%
\contentsline {section}{\numberline {6.4}Obtuse Angle Criterion: $[z]^\&$ is projection $\Leftrightarrow $ $(z-[z]^\&)^T(y-[z]^\&)\leq 0, \forall y\in \&$}{43}{section.6.4}%
\contentsline {subsection}{\numberline {6.4.1}Special Case (Linear subspace): Orthogonality Principle $(z-[z]^\&)^Tx= 0,\forall x\in \&$}{44}{subsection.6.4.1}%
\contentsline {section}{\numberline {6.5}Prop: Projection is Non-expansive $\|[x]^\&-[z]^\&\|\leq \|x-z\|,\forall x,z\in \mathbb {R}^n$}{45}{section.6.5}%
\contentsline {section}{\numberline {6.6}Separating Hyperplane Theorem \small {(Lec 13 @ Econ 204)}}{46}{section.6.6}%
\contentsline {section}{\numberline {6.7}Bolzano-Weierstrass theorem}{47}{section.6.7}%
\contentsline {subsection}{\numberline {6.7.1}Sequence Convergence}{47}{subsection.6.7.1}%
\contentsline {subsection}{\numberline {6.7.2}Bolzano-Weierstrass Theorem: Compact set $S$, $\exists $ subsequence converges to $\vec {x}^*\in S$}{47}{subsection.6.7.2}%
\contentsline {subsection}{\numberline {6.7.3}Extreme Value Theorem: continuous $f$, compact set $\rightarrow \mathbb {R}$ has global-min}{47}{subsection.6.7.3}%
\contentsline {subsection}{\numberline {6.7.4}Support Theorem: $z\in bd(C)$, $\exists \vec {u}$ s.t. $\vec {u}\cdot \vec {x}\leq \vec {u}\cdot \vec {z}, \forall \vec {x}\in C$}{47}{subsection.6.7.4}%
\contentsline {chapter}{\numberline {7}Convex Programming}{49}{chapter.7}%
\contentsline {section}{\numberline {7.1}Perturbation of Constraints}{49}{section.7.1}%
\contentsline {subsection}{\numberline {7.1.1}Theorem: convex program $\Rightarrow $ $MP(\vec {z})$ is a convex function on convex domain}{50}{subsection.7.1.1}%
\contentsline {section}{\numberline {7.2}Karush-Kuhn-Tucker theorem}{50}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}Slater's condition: $P$ is super-consistent if $\exists \vec {x}^*\in S$ s.t. $\vec {g}(\vec {x}^*)<\vec {0}$}{50}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}Lemma: Convex $P$ is super-consistent $\Rightarrow $ $\exists $ sensitivity vector $\vec {\lambda }\geq 0$ s.t. $MP(\vec {z})\geq MP(\vec {0})-\vec {\lambda }\cdot \vec {z},\forall \vec {z}$}{51}{subsection.7.2.2}%
\contentsline {subsection}{\numberline {7.2.3}Goalposts Lemma: $\vec {x}^{(0)}$ is feasible for $P(\vec {g}(\vec {x}^{(0)}))$ and $MP(\vec {g}(\vec {x}^{(0)}))\leq f(\vec {x}^{(0)})$}{51}{subsection.7.2.3}%
\contentsline {subsection}{\numberline {7.2.4}KKT Theorem: Saddle Point Form}{52}{subsection.7.2.4}%
\contentsline {subsection}{\numberline {7.2.5}KKT Theorem: Gradient Form}{54}{subsection.7.2.5}%
\contentsline {subsection}{\numberline {7.2.6}Relationship between KKT and Optimal Solution}{55}{subsection.7.2.6}%
\contentsline {section}{\numberline {7.3}KKT Duality}{55}{section.7.3}%
\contentsline {subsection}{\numberline {7.3.1}KKT Duality: If $P$ has optimal solution $\vec {x}^*$ and sensitivity vector $\vec {\lambda }^*$ $\Rightarrow h(\vec {\lambda ^*})=f(\vec {x}^*)$}{55}{subsection.7.3.1}%
\contentsline {subsection}{\numberline {7.3.2}Dual Program}{56}{subsection.7.3.2}%
\contentsline {subsection}{\numberline {7.3.3}The Duality Gap}{57}{subsection.7.3.3}%
\contentsline {section}{\numberline {7.4}Equality Constraints}{57}{section.7.4}%
\contentsline {subsection}{\numberline {7.4.1}KKT with equality constraints}{58}{subsection.7.4.1}%
\contentsline {subsection}{\numberline {7.4.2}Penalty method with equality constraints}{58}{subsection.7.4.2}%
\contentsline {subsection}{\numberline {7.4.3}Super-consistent with equality constraintes}{58}{subsection.7.4.3}%
\contentsline {chapter}{\numberline {8}Constrained Geometric Programming}{60}{chapter.8}%
\contentsline {section}{\numberline {8.1}Standard Form of Geometric Programming}{60}{section.8.1}%
\contentsline {section}{\numberline {8.2}Dual Geometric Program}{60}{section.8.2}%
\contentsline {subsection}{\numberline {8.2.1}An Example}{60}{subsection.8.2.1}%
\contentsline {subsection}{\numberline {8.2.2}The geometric programming dual, in general}{63}{subsection.8.2.2}%
\contentsline {subsection}{\numberline {8.2.3}Using a dual solution to find a primal solution}{65}{subsection.8.2.3}%
\contentsline {section}{\numberline {8.3}Equality constraints in geometric programming}{66}{section.8.3}%
\contentsline {chapter}{\numberline {9}Penalty Method}{67}{chapter.9}%
\contentsline {section}{\numberline {9.1}The Courant-Beltrami penalty function}{67}{section.9.1}%
\contentsline {subsection}{\numberline {9.1.1}Example}{68}{subsection.9.1.1}%
\contentsline {subsection}{\numberline {9.1.2}Guarantees assuming convergence}{68}{subsection.9.1.2}%
\contentsline {subsection}{\numberline {9.1.3}Guaranteeing convergence: (1)feasible $P$; (2)continuous $g_i$; (3)coercive $f$}{69}{subsection.9.1.3}%
\contentsline {section}{\numberline {9.2}The Penalty Method and KKT Duality}{69}{section.9.2}%
\contentsline {subsection}{\numberline {9.2.1}Thm: (1)feasible $P$; (2)continuous $g_i$; (3)coercive $f$ $\Rightarrow MP=MD$}{69}{subsection.9.2.1}%
\contentsline {subsection}{\numberline {9.2.2}If $f$ not coercive}{71}{subsection.9.2.2}%
\contentsline {chapter}{\numberline {10}Optimization with Equality Constraints}{72}{chapter.10}%
\contentsline {section}{\numberline {10.1}Basic}{72}{section.10.1}%
\contentsline {section}{\numberline {10.2}Lagrange Mutiplier Theorem}{72}{section.10.2}%
\contentsline {subsection}{\numberline {10.2.1}First-order necessary condition: $\exists \lambda , \nabla f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla h_i(x^*)=0$}{72}{subsection.10.2.1}%
\contentsline {subsection}{\numberline {10.2.2}Second-order necessary condition: $z^T\left (\nabla ^2 f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla ^2 h_i(x^*)\right )z\geq 0,\forall z\in V(x^*)$}{74}{subsection.10.2.2}%
\contentsline {subsection}{\numberline {10.2.3}Sufficient Condition: $\exists \lambda $ 1. $\nabla f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla h_i(x^*)=0$ 2. $z^T\big (\nabla ^2 f(x^*)+\DOTSB \sum@ \slimits@ _{i=1}^m\lambda _i \nabla ^2 h_i(x^*)\big )z> 0,\forall z\in V(x^*),z\neq 0$}{75}{subsection.10.2.3}%
\contentsline {subsection}{\numberline {10.2.4}Lagrangian Function}{75}{subsection.10.2.4}%
\contentsline {subsection}{\numberline {10.2.5}Example}{76}{subsection.10.2.5}%
\contentsline {subsection}{\numberline {10.2.6}Sensitivity Analysis $f(x^*(u))=f(x^*)-\lambda ^Tu+O(\|u\|)$}{76}{subsection.10.2.6}%
\contentsline {subsection}{\numberline {10.2.7}Linear Constraints}{77}{subsection.10.2.7}%
\contentsline {chapter}{\numberline {11}Optimization with Inequality Constraints}{80}{chapter.11}%
\contentsline {section}{\numberline {11.1}Basic}{80}{section.11.1}%
\contentsline {subsection}{\numberline {11.1.1}Active vs. Inactive Inequality Constraints}{80}{subsection.11.1.1}%
\contentsline {subsection}{\numberline {11.1.2}ICP $\rightarrow $ ECP}{80}{subsection.11.1.2}%
\contentsline {subsection}{\numberline {11.1.3}Intuition $\mu _j\geq 0, \forall j\in A(x^*)$}{80}{subsection.11.1.3}%
\contentsline {subsection}{\numberline {11.1.4}Complementary Slackness}{81}{subsection.11.1.4}%
\contentsline {section}{\numberline {11.2}Karush–Kuhn–Tucker (KKT) Necessary Conditions}{81}{section.11.2}%
\contentsline {section}{\numberline {11.3}Karush–Kuhn–Tucker (KKT) Sufficient Conditions}{83}{section.11.3}%
\contentsline {section}{\numberline {11.4}General Sufficiency Condition}{86}{section.11.4}%
\contentsline {section}{\numberline {11.5}Barrier Method}{87}{section.11.5}%
\contentsline {section}{\numberline {11.6}An Exmaple Using KKT or Barrier}{88}{section.11.6}%
\contentsline {subsection}{\numberline {11.6.1}Solution using KKT conditions}{88}{subsection.11.6.1}%
\contentsline {subsection}{\numberline {11.6.2}Solution using logarithmic barrier}{89}{subsection.11.6.2}%
\contentsline {section}{\numberline {11.7}Penalty Method (For ECP)}{89}{section.11.7}%
\contentsline {chapter}{\numberline {12}Duality}{91}{chapter.12}%
\contentsline {section}{\numberline {12.1}Weak Duality Theorem: $\max _{(\lambda ,\mu )\in G}D(\lambda ,\mu )\leq \min _{x\in F}f(x)$}{91}{section.12.1}%
\contentsline {section}{\numberline {12.2}Strong Duality Theorem: under some conditions, $\max _{(\lambda ,\mu )\in G}D(\lambda ,\mu )= \min _{x\in F}f(x)$}{92}{section.12.2}%
\contentsline {subsection}{\numberline {12.2.1}Slater's sufficient condition for strong duality}{94}{subsection.12.2.1}%
\contentsline {subsection}{\numberline {12.2.2}Example}{94}{subsection.12.2.2}%
\contentsline {section}{\numberline {12.3}Dual of Linear Program}{95}{section.12.3}%
\contentsline {chapter}{\numberline {13}Strongly Convexity}{97}{chapter.13}%
\contentsline {section}{\numberline {13.1}$\mu $-Strongly Convex: $ \langle \nabla f(w)-\nabla f(v), w-v\rangle \geq \mu \|w-v\|^{2}$}{97}{section.13.1}%
\contentsline {section}{\numberline {13.2}$\mu $-strongly convex $\Leftrightarrow \nabla ^{2} f(x) \succeq \mu I\Leftrightarrow $"$f(x)-\frac {m}{2}\|x\|^2$ is convex"}{97}{section.13.2}%
\contentsline {section}{\numberline {13.3}Lemma: Strongly convexity $\Rightarrow $ Strictly convexity}{97}{section.13.3}%
\contentsline {section}{\numberline {13.4}Lemma: $\nabla ^2 f(x)\succeq mI$ $\Rightarrow $ $f(y)\geq f(x)+\nabla f(x)^T(y-x)+\frac {m}{2}\|y-x\|^2$}{98}{section.13.4}%
\contentsline {chapter}{\numberline {14}Lipschitz Gradient ($L$-Smooth)}{99}{chapter.14}%
\contentsline {section}{\numberline {14.1}Theorem: $-MI\preceq \nabla ^2 f(x)\preceq MI$ $\Rightarrow $ $f$ is $M$-smooth}{99}{section.14.1}%
\contentsline {section}{\numberline {14.2}Descent Lemma: $f$ is $L$-smooth $\Rightarrow $ $f(y)\leq f(x)+\nabla f(x)^T(y-x)+\frac {L}{2}\|y-x\|^2$}{100}{section.14.2}%
\contentsline {section}{\numberline {14.3}Co-coercivity Condition: $(\nabla f(x)-\nabla f(y))^T(x-y)\geq \frac {1}{L}\|\nabla f(x)-\nabla f(y)\|^2$}{101}{section.14.3}%
