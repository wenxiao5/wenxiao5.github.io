\documentclass[11pt]{elegantbook}
\definecolor{structurecolor}{RGB}{40,58,129}
\linespread{1.6}
\setlength{\footskip}{20pt}
\setlength{\parindent}{0pt}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\elegantnewtheorem{proof}{Proof}{}{Proof}
\elegantnewtheorem{claim}{Claim}{prostyle}{Claim}
\DeclareMathOperator{\col}{col}
\title{\textbf{Dynamic Programming}}
\author{Wenxiao Yang}
\institute{Department of Mathematics, University of Illinois at Urbana-Champaign}
\date{2022}
\setcounter{tocdepth}{2}
\cover{cover.jpg}
\extrainfo{All models are wrong, but some are useful.}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}

\addbibresource[location=local]{reference.bib} % bib

\begin{document}

\maketitle
\frontmatter
\tableofcontents
\mainmatter


\chapter{Multi-Armed Bandits}
\section{Multi-Armed Bandit Problem}
\begin{definition}[Multi-Armed Bandit Problem]
    A decision-maker ("gambler") chooses one of $n$ actions ("arms") in each time step. Chosen arm produces random payoff from unknown distribution. Goal: Maximize expected total payoff.
\end{definition}

Every time, the DM pulls an arm $i\in\{1,...,n\}$, then he observes a reward $r_t$ which follows a distribution.
\begin{enumerate}[$\bullet$]
    \item Each arm has a \textbf{type} that \underline{determines its payoff distribution}.
    \item Gambler has a \underline{prior distribution} over \textbf{types} for each arm.
    \item \textbf{Types} are independent random variables.
    \item Objective: maximize expected discounted reward $\sum_{t=0}^\infty \gamma^t r_t$
\end{enumerate}
\begin{definition}[State]
    After pulling arm $i$ some number of times, gambler has a \underline{posterior distribution} over types. Call this the \textbf{state} of arm $i$ (i.e., the posterior distribution over types after getting information).\\
    \underline{State can be expected reward} when pulled is governed by state.
\end{definition}
The player has two controls: either play the process or not. If the player chooses to play the bandit process $i$, $i = 1,...,n$, the state of the bandit process $i$ evolves in a Markovian manner while the state of all other processes remains frozen (i.e., it does not change). Such a bandit process is called a Markovian bandit process.










\end{document}