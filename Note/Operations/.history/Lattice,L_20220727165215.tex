%!TEX program = xelatex
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{authblk}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{verbatim}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{indentfirst}
\usepackage{amssymb}
\setlength{\parindent}{0pt}
\usetikzlibrary{shapes,snakes}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\DeclareMathOperator{\col}{col}
\usepackage{booktabs}
\newtheorem{theorem}{Theorem}
\newtheorem{note}{Note}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{hyperref}
\newcommand{\code}{	exttt}
\geometry{a4paper,scale=0.8}
\title{Lattice Programming, $L^\natural$-Convexity, and little Revenue Management}
\author[*]{Wenxiao Yang}
\affil[*]{Department of Mathematics, University of Illinois at Urbana-Champaign}
\date{2022}
\begin{document}
\maketitle
\tableofcontents
\newpage







\section{Lattice Programming}
\subsection{Lattice}
\begin{definition}
    $(X,\geq)$ is a \textbf{lattice} if for any $x,y\in X$,

    \quad $x\vee y=\inf\{z\in X | x\leq z, y\leq z\}\in X$

    \quad $x\wedge y=\sup \{z\in X| x\geq z,y\geq z\}\in X$
\end{definition}

\begin{definition}
    $(X',\geq)$ is a \textbf{sublattice} of $(X,\geq)$: inherit $x\vee y$, $x\wedge y$ from $X$.
\end{definition}

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.5]{lattice1.png}
    \caption{}
    \label{}
\end{figure}\end{center}

\begin{example}
Lattices:
\begin{enumerate}
    \item $\{0,1\}^n$
    \item $Z^n$
    \item a chain is a lattice (whose elements are ordered)
    \item Intersection of two lattices
\end{enumerate}
\end{example}

\subsection{Supermodularity}
\subsubsection{Definition: Supermodular $g(x \vee y)+g(x \wedge y) \geq g(x)+g(y), \forall x, y \in X$}
\begin{definition}
    A function $g: X \rightarrow \bar{\Re}(=\Re \cup\{+\infty\})$ is submodular if $$g(x \vee y)+g(x \wedge y) \leq g(x)+g(y), \forall x, y \in X$$
    $g$ is supermodular if $-g$ is submodular.
\end{definition}
\begin{center}
    \begin{tikzpicture}[domain=0:3.25]
    \draw[-](0,0)--(4,0) node[below] {$y$};
    \draw[-](4,0)--(4,2) node[above] {$x\vee y$};
    \draw[-](4,2)--(0,2) node[above] {$x$};
    \draw[-](0,2)--(0,0) node[below] {$x\wedge y$};
    \end{tikzpicture}
\end{center}
\begin{claim}
    $\operatorname{dom}(g)=\{x \in X \mid g(x)<+\infty\}$ is a lattice if $g$ is submodular.
\end{claim}
\begin{proof}
$\forall x,y\in \operatorname{dom}(g)$, prove $g(x\vee y)<+\infty$, $g(x\wedge y)<+\infty$.
\end{proof}

\subsubsection{Lemma: Supermodular $\Leftrightarrow$ $
\frac{\partial^{2} g(x)}{\partial x_{i} \partial x_{j}} \geq 0, \forall i \neq j
$}
\begin{lemma}
    Suppose $g$ is twice partially differentiable in $\mathfrak{R}^{n}$. Then $g$ is supermodular if and only if it has nonnegative cross partial derivatives, i.e.,
    $$
    \frac{\partial^{2} g(x)}{\partial x_{i} \partial x_{j}} \geq 0, \forall i \neq j
    $$
\end{lemma}
\begin{proof}
\quad

\begin{center}
        \begin{tikzpicture}[domain=0:3.25]
    \draw[-](0,0)--(2,0) node[right] {$y=(y_1,y_2)$};
    \draw[-](2,0)--(2,1) node[right] {$x\vee y=(y_1,x_2)$};
    \draw[-](2,1)--(0,1) node[left] {$x=(x_1,x_2)$};
    \draw[-](0,1)--(0,0) node[left] {$x\wedge y=(x_1,y_2)$};
    \end{tikzpicture}
\end{center}
$$x_1\leq y_1;\ y_2\leq x_2$$
\begin{equation}
    \begin{aligned}
        &g\text{ is supermodular}\\
        \Leftrightarrow	& g(x \vee y)-g(x)\geq g(y)-g(x \wedge y), \forall x, y \in X\\
        & g(y_1,x_2)-g(x_1,x_2)\geq g(y_1,y_2)-g(x_1,y_2), \forall x, y \in X\\
        &(\text{if }y_1 \rightarrow	x_1,\text{ $y_2$ kept unchanged})\\
        &\frac{\partial g(x_1,x_2)}{\partial x_1}\geq \frac{\partial g(x_1,y_2)}{\partial x_1}\\
        &(\text{if }y_2 \rightarrow	x_2,\ y_2\leq x_2)\\
        &\frac{\partial^2 g(x)}{\partial x_i\partial x_j}\geq 0,\ \forall i\neq j\\
    \end{aligned}
    \nonumber
\end{equation}
\end{proof}
\textbf{Note}: Supermodularity $\approx$ Economic Complementarity

$g$ is the profit function of selling products $x_1$ and $x_2$, $\frac{\partial}{\partial x_2}(\frac{\partial g(x_1,x_2)}{\partial x_1})\geq 0$

\begin{example}[Examples of Supermodular Functions]
    \quad

\begin{enumerate}
    \item $f(x)=x_{1}^{\alpha_{1}} x_{2}^{\alpha_{2}} \ldots x_{n}^{\alpha_{n}}\left(\alpha_{i} \geq 0\right)$ for $x \geq 0$
    \item $f(x, z)=\sum_{i=1}^{n} g_{i}\left(\alpha_{i} x_{i}-\beta_{i} z_{i}\right)$ for any univariate concave function $g_{i}: \Re \rightarrow \bar{\Re}\left(\alpha_{i} \beta_{i} \geq\right.$ $0)$
    \item $f(x)=\sum_{i, j=1}^{n} a_{i j} x_{i} x_{j}=x^TAx$ with $a_{i j}=a_{j i}$ is supermodular if and only if $a_{i j} \geq 0\ \forall i \neq j$
\end{enumerate}
\end{example}

\subsubsection{Lemma: Preservation of Supermodularity}
\begin{lemma}[Preservation of Supermodularity]
\quad

\begin{enumerate}[a)]
    \item If $f_{i}$ is supermodular, then $\lim _{i \rightarrow \infty} f_{i}(x), \sum_{i} \alpha_{i} f_{i}\left(\alpha_{i} \geq 0\right)$ are supermodular
    \item If $f: \Re \rightarrow \Re$ is convex and nondecreasing (nonincreasing) and $g: \Re^{n} \rightarrow \Re$ is increasing and supermodular (submodular), then $f(g(x))$ is supermodular
    \item Given $f: \Re^{n} \times \Re^{m} \rightarrow \Re$, if $f(\cdot, y)$ is supermodular for all $y$, then $E_{\xi}[f(x, \xi)]$ is supermodular in $x$
\end{enumerate}
\end{lemma}

\begin{lemma}[Supermodularity of composite functions]
    \quad

    If $X=\Pi_{i=1}^{n} X_{i}$ and $X_{i} \subseteq \Re, f_{i}\left(x_{i}\right): X_{i} \rightarrow \Re$ is increasing (decreasing) on $X_{i}$ for $i=1, \ldots, n$, and $g\left(z_{1}, \ldots, z_{n}\right): \Re^{n} \rightarrow \bar{\Re}$ is supermodular in $\left(z_{1}, \ldots, z_{n}\right)$, then
$$
g\left(f_{1}\left(x_{1}\right), \ldots, f_{n}\left(x_{n}\right)\right)
$$
is supermodular on $X$
\end{lemma}

\begin{lemma}[Topkis 1998]
    If $X$ is a lattice, $f_{i}(x)$ is increasing and supermodular (submodular) on $X$ for $i=$ $1, \ldots, k, Z_{i}$ is a convex subset of $R^{1}$ containing the range of $f_{i}(x)$ on $X$ or $i=1, \ldots, k$, and $g\left(z_{1}, \ldots, z_{k}, x\right)$ is supermodular in $\left(z_{1}, \ldots, z_{k}, x\right)$ and is increasing (decreasing) and convex in $z_{i}$ for fixed $z_{-i}$ and $x$, then $g\left(f_{1}(x), \ldots, f_{k}(x), x\right)$
is supermodular on $X$
\end{lemma}

\subsection{Parametric Optimization Problems}
\begin{definition}
    \begin{align*}
        f(s)=&\max g(s,a)\\
        &\text{s.t. }a\in A(s)
    \end{align*}
\end{definition}
$S$: subset of $\mathfrak{R}^{m}$
    
$A(s):$ finite dimensional

$C:=\{(s, a) \mid s \in S, a \in A(s)\}$ (the graph of the constraint operator)

$A^{*}(s)$, the optimal solution set, is nonempty for every $s \in S$

\begin{definition}
A set $A(s)$ is \textbf{ascending on $S$} if for $s\leq s'$, $a\in A(s)$, $a'\in A(s')$, we have $a\wedge a'\in A(s)$ and $a\vee a'\in A(s')$.
\end{definition}

\begin{example}
$A(s)=[s,+\infty)$ is ascending on $S$.
\end{example}




\subsubsection{Theorem: Maximizer of supermodular func is ascending, the maximum value is also supermodular}

\begin{theorem}[Ascending Optimal Solutions and Preservation]
    \quad

    If
    \begin{enumerate}
        \item $S$: sublattice of $\Re^{m}$
        \item $C:=\{(s, a) \mid s \in S, a \in A(s)\}$ is a sublattice
        \item $g$ is supermodular on $C$
    \end{enumerate}
    Then

    \begin{enumerate}
        \item $A^{*}(s)$ is \textbf{ascending} on $S$. Under some conditions, the largest/smallest element of $A^{*}(s)$ exists, and is increasing in $\mathrm{s}$.
        \item $f(s)$ is supermodular.
    \end{enumerate}
\end{theorem}

\begin{proof}
Take $s\leq s'$, $a^*\in A^*(s),\ {a'}^*\in A^*(s')$, i.e.
\begin{center}
    $g(s,a^*)=\max g(s,a)$ s.t. $a\in A(s)$\\
    $g(s',{a'}^*)=\max g(s',a)$ s.t. $a\in A(s')$
\end{center}
\begin{center}
    $(s,a^*)\vee(s',{a'}^*)=(s',a^*\vee{a'}^*)$\\
    $(s,a^*)\wedge(s',{a'}^*)=(s,a^*\wedge{a'}^*)$
\end{center}
As we know $C$ is a sublattice, we have
\begin{equation}
    \begin{aligned}
        (s',a^*\vee{a'}^*)\in C \Rightarrow	a^*\vee{a'}^*\in A(s')\\
        (s,a^*\wedge{a'}^*)\in C \Rightarrow	a^*\wedge{a'}^*\in A(s)
    \end{aligned}
    \nonumber
\end{equation}
Hence, $$g(s',a^*\vee{a'}^*)\leq g(s',{a'}^*);\ g(s,a^*\wedge{a'}^*)\leq g(s,a^*)$$
Since $g$ is supermodular on $C$,
\begin{equation}
    \begin{aligned}
        g(s',a^*\vee{a'}^*)+g(s,a^*\wedge{a'}^*)&\geq g(s,a^*)+g(s',{a'}^*)\\
        0\geq g(s',a^*\vee{a'}^*)-g(s',{a'}^*)&\geq g(s,a^*)-g(s,a^*\wedge{a'}^*)\leq 0
    \end{aligned}
    \nonumber
\end{equation}
Hence, $$g(s',a^*\vee{a'}^*)=g(s',{a'}^*);\ g(s,a^*)=g(s,a^*\wedge{a'}^*)$$
which means,
$$a^*\vee{a'}^*\in A^*(s'),\ a^*\wedge{a'}^*\in A^*(s)$$
Then, " $A^*(s)$ is ascending on $S$ " is proved.

What's more, the largest elements of $A(s')$ and $A(s)$ are $a^*\vee{a'}^*$ and $a^*$, the smallest elements of $A(s')$ and $A(s)$ are ${a'}^*$ and $a^*\wedge{a'}^*$, which are both increased as $s$ increases to $s'$.
\end{proof}

\begin{proof}
$\forall s,s'\in S$, $a\in A^*(s),a'\in A^*(s')$.
\begin{equation}
    \begin{aligned}
        f(s)+f(s')&=g(s,a)+g(s,a')\\
        &\text{(Since $g$ is supermodular on $C$)}\\
        &\leq g(s\wedge s',a\wedge a')+g(s\vee s',a\vee a')\\
        &\leq f(s\wedge s')+f(s\vee s')
    \end{aligned}
    \nonumber
\end{equation}
" $f(s)$ is supermodular " is proved.
\end{proof}

\begin{example}
    Pricing: 
$p^*(c)=\argmax_{p\geq c'}(p-c)D(p)$, $(c'>c)$
\end{example}
1. $C=\{(p,c)|c<c',p\geq c'\}$ is a sublattice of $\mathbb{R}^2$.

2. $g(p,c)=(p-c)D(p)$, $\frac{\partial^2 g(p,c)}{\partial p\partial c}=-D'(p)\geq 0$ $\Rightarrow$ $g$ is supermodular on $C$.

Hence, $p^*(c)$ is increasing in $c$.

\begin{example}
Newsvendor model: $\min_{x\geq 0}f(x)=cx+h_+E[(x-\xi)^+]+h_-E[(\xi-x)^+]$
\end{example}



\section{$L^\natural$-Convexity}
\subsection{Discrete Midpoint Convexity}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.5]{DMC.png}
    \caption{Discrete Midpoint Convexity}
    \label{}
\end{figure}\end{center}
\begin{definition}
A function $f$ is \textbf{discrete midpoint convexity} if $$f(\lceil\frac{p+q}{2}\rceil)+f(\lfloor\frac{p+q}{2}\rfloor)\leq f(p)+f(q)$$
\end{definition}

\subsection{$L^\natural$-Convexity on $\mathbb{Z}^n$}
\begin{definition}
    A function $f: Z^{n} \rightarrow \bar{\Re}$ is called $L^{\natural}$ convex if $f$ satisfies the discrete midpoint convexity.
\end{definition}
\textbf{An equivalent definition}: A function $f: Z^{n} \rightarrow \bar{\Re}$ is $L^\natural$-convex if and only if
$$
g(x, \alpha):=f(x-\alpha e)=f([x_1-\alpha,x_2-\alpha,...,x_n-\alpha]^T)
$$
is submodular in $(x, \alpha)$ on $Z^{n+1}(e:$ all-ones vector $)$.





\subsection{$L^\natural$-Convexity on $\mathcal{F}^{n}(\mathcal{F}=\mathbb{Z}$ or $\Re)$}
\begin{definition}[Murota 2003]
    \quad

    A function $f: \mathcal{F}^{\mathrm{n}} \rightarrow \Re$ is \underline{$L^{\natural}$-convex if and only if $g(x, \xi):=f(x-\xi e)$ is submodular in $(x, \xi) \in \mathcal{F}^{\mathrm{n}} \times S$}, where $e$ is a vector with all components equal to 1 and $S$ is the intersection of $\mathcal{F}$ with any unbounded interval in $\mathfrak{R}$. $(f$ is required to be convex if $\mathcal{F}=\mathfrak{R})$
\end{definition}

\begin{definition}
    A set $V$ is \underline{$L^{\natural}$-convex if and only if its indicator function $\delta_V(x)$ is $L^{\natural}$.} $$\delta_V(x)=\left\{\begin{matrix}
        +\infty &,x\notin V\\
        0&, x\in V
    \end{matrix}\right.$$
    $\Leftrightarrow g(x,\xi)=\delta_V(x-\xi e)$ is subnormal, i.e.
    \begin{equation}
        \begin{aligned}
            g(x\vee y,\max\{\xi_x,\xi_y\})+g(x\wedge y,\min\{\xi_x,\xi_y\})\leq g(x,\xi_x)+g(y,\xi_y),\ \forall (x,\xi_x),(y,\xi_y)
        \end{aligned}
        \nonumber
    \end{equation}
    If $x-\xi_xe,y-\xi_y e$ in $V$, $x\vee y-\max\{\xi_x,\xi_y\}e\text{ and } x\wedge y-\min\{\xi_x,\xi_y\}$ must in $V$.
\end{definition}

Note: $f$ is $L^{\natural}$-concave if $-f$ is $L^{\natural}$-convex.


\subsection{Properties of $L^\natural$-Convexity}
\subsubsection{Proposition: $L^\natural$-convex $\Leftrightarrow$ $
a_{i j} \leq 0, \forall i \neq j, a_{i i} \geq 0, \sum_{j=1}^{n} a_{i j} \geq 0, \forall i
$}
\begin{proposition}
    A quadratic function $f(x)=\sum_{i, j=1}^{n} a_{i j} x_{i} x_{j}$ with $a_{i j}=a_{j i}$ is \underline{$L^\natural$-convex on $\mathcal{F}$ if and only if} its Hessian is a diagonally dominated M-matrix
    $$
    a_{i j} \leq 0\ \forall i \neq j,\quad a_{ii}\geq 0, \quad \sum_{j=1}^{n} a_{i j} \geq 0\ \forall i
    $$
\end{proposition}
\begin{proof}
\quad

$f(x)$ is $L^\natural$-convex $\Leftrightarrow$ $g(x,\xi)=f(x-\xi e)=\sum_{i,j=1}^na_{ij}(x_i-\xi)(x_j-\xi)$ is submodular in $(x,\xi)$ i.e.
\begin{equation}
    \begin{aligned}
        \frac{\partial^2 g}{\partial \xi\partial x_i}&=\frac{\partial}{\partial \xi}(\sum_{j=1}^na_{ij}(x_j-\xi)+\sum_{j=1}^na_{ji}(x_j-\xi))=-2\sum_{j=1}^na_{ij}\leq 0\\
        \frac{\partial^2 g}{\partial x_j\partial x_i}&=\frac{\partial}{\partial x_k}(\sum_{k=1}^na_{ik}(x_k-\xi)+\sum_{k=1}^na_{ki}(x_k-\xi))=2a_{ij}\leq 0
    \end{aligned}
    \nonumber
\end{equation}

\end{proof}



\begin{proposition}
    A twice continuous differentiable function $f: \Re^{n} \rightarrow \Re$ is \underline{$L^\natural$-convex if and only if} its Hessian is a diagonally dominated M-matrix, that is
    $$
    a_{i j} \leq 0, \forall i \neq j, a_{i i} \geq 0, \sum_{j=1}^{n} a_{i j} \geq 0, \forall i
    $$
\end{proposition}
\begin{proof}
\quad\\

$L^\natural$-convex $\Leftrightarrow$ $g(x,\xi)=f(x-\xi e)$ is subnormal

(if twice differentiable)

$\Leftrightarrow \frac{\partial^2 g(x,\xi)}{\partial x_i\partial x_j}=\frac{\partial^2 f(x-\xi e)}{\partial x_i\partial x_j}\leq 0, i\neq j$, $\frac{\partial^2 g(x,\xi)}{\partial x_i\partial \xi}=-\sum_{j=1}^n \frac{\partial^2 f(x-\xi e)}{\partial x_i\partial x_j}\leq 0$, $\forall (x,\xi)\in \Re^{n+1}$

$\Leftrightarrow \frac{\partial^2 f(x)}{\partial x_i\partial x_j}\leq 0, i\neq j;\ \sum_{j=1}^n \frac{\partial^2 f(x)}{\partial x_i\partial x_j}\geq 0$ $\forall x\in \Re^n$
\end{proof}

\subsubsection{Corollary: $L^{\natural}$-convex $\longrightarrow$ convex $+$ submodular}
\begin{corollary}
    If a twice differentiable function $f$ is $L^{\natural}$-convex, then the function is convex and submodular.
\end{corollary}
\begin{proof}
\quad\\
\underline{$a_{i j}=\frac{\partial^2 f(x)}{\partial x_i\partial x_j} \leq 0, i\neq j$} means the cross partial derivatives are nonpositive, which equals to \underline{$f$ is submodular}.
\begin{equation}
    \begin{aligned}
        x^T\nabla^2 f(x)x&=\sum_{i, j=1}^{n} a_{i j} x_{i} x_{j}\\
        &=\sum_k^na_{kk}x_k^2+\sum_{j=1}^n\sum_{i< j}a_{ij}2x_ix_j\\
        &\geq \sum_k^na_{kk}x_k^2+\sum_{j=1}^n\sum_{i< j}a_{ij}(x_i^2+x_j^2)\\
        &\geq \sum_k^{n-1}a_{kk}x_k^2+\sum_{j=1}^{n-1}\sum_{i< j}a_{ij}(x_i^2+x_j^2)\\
        &\cdots\\
        &\geq 0,\quad \forall x\in \Re^n
    \end{aligned}
    \nonumber
\end{equation}
Then $f$ is convex.
\end{proof}

\begin{example}
\end{example}
    - Given any univariate (discrete) convex function $g_{i}: \mathcal{F} \rightarrow \bar{\Re}$ and $h_{i j}: \mathcal{F} \rightarrow \Re$, the function $f: \mathcal{F}^{n} \rightarrow \bar{\Re}$ defined by
    $$
    f(x):=\sum_{i} g_{i}\left(x_{i}\right)+\sum_{i \neq j} h_{i j}\left(x_{i}-x_{j}\right)
    $$
    is $L^{\natural}$-convex.

\begin{example}
\end{example}
    - A set with a representation
    $$
    \left\{x \in \mathcal{F}^{n}: l \leq x \leq u, x_{i}-x_{j} \leq v_{i j}, i \neq j\right\}
    $$
    is $L^{\natural}$-convex, where $l, u \in \mathcal{F}^{n}, v_{i j} \in \mathcal{F}$.

\subsubsection{Theorem: Minimizer of $L^\natural$-convex func is nondecreasing with bounded sensitivity, the minimum value is also $L^\natural$-convex}
\begin{theorem}
Assume $g:\mathcal{F}^n\times \mathcal{F}^m \rightarrow	\overline{\Re}$ and set $C\subset \mathcal{F}^n\times \mathcal{F}^m$ are $L^\natural$-convex, define $$f(s)=\inf_{a:(s,a)\in C}g(s,a)$$
Then,
\begin{enumerate}
    \item The optimal solution set $A^*(s)$ is nondecreasing in $s$ with bounded sensitivity i.e., $$A^*(s+\omega e)\leq A^*(s)+\omega e,\ \forall \omega\in F_+$$ (Zipkin 2008, Chen et al. 2018)
    \item $f$ is $L^\natural-convex$. (Zipkin 2008)
\end{enumerate}
\end{theorem}

\subsection{Relationship with Multimodularity}
\begin{definition}
A function $f(x_1,x_2,...,x_n)$ is multimodular if $f(x_1-x_0,x_2-x_1,...,x_n-x_{n-1})$ submodular in $(x_0,x_1,...,x_n)$.
\end{definition}
Multimodularity and $L^\natural$-convexity are equivalent subject to an unimodular linear transformation.

\section{Optimization with decisions truncated by random variables}
$$\min_{u\in \mathcal{U}} E[f(u\wedge \xi)]$$
\textbf{Question 1 (Supply uncertainty in SCM):} $u$: ordering quantities; $\xi$: random capacities.

\textbf{Question 2 (Demand uncertainty in RM):} $u$: booking limits; $\xi$: random demands.

\textbf{Difficulty:} the object function is not convex (even if $f$ is).

\subsection{Unconstrained Problem}
Consider $$\tau^*=\min_{u\in \mathcal{F}^n} E[f(u\wedge \xi)]$$
$\mathcal{F}$ is either the real space or the set with all integers.

Random vector $\xi\in \mathcal{X}\subseteq \mathcal{F}^n$

\subsubsection{Reformulation}
\textbf{Reformulation:}
\begin{align*}
        &\min\quad E[f(v(\xi))]\\
        &\begin{array}{r@{\quad}r@{}l@{\quad}l}
        s.t.
        &v(\xi)&=(v_1(\xi_1),...,v_n(\xi_n))&, \forall \xi\in \mathcal{X} \\
        &v(\xi)&=u\wedge \xi&,\forall \xi\in \mathcal{X}\\
    \end{array} .
\end{align*}
Turn finding $u^*$ into finding $v^*()$

$v()$ is not convex.

\begin{theorem}[Equivalent Transformation, Chen, Gao and Pang 2018]
    Suppose that (Assumption I)

    (a) the function $f$ is lower semi-continuous with $f(u) \rightarrow+\infty$ for $|u| \rightarrow+\infty$;

    (b) the function $f$ is componentwise (discrete) convex;

    (c) the random vector $\xi$ has \underline{independent} components.

    Then $\tau^{*}$ is also the optimal objective value of the following optimization problem:
\begin{align*}
        &\min\quad E[f(v(\xi))]\\
        &\begin{array}{r@{\quad}r@{}l@{\quad}l}
        s.t.
        &v(\xi)&\leq \xi&,\forall \xi\in \mathcal{X}\\
        &v(\xi)&=(v_1(\xi_1),...,v_n(\xi_n))&, \forall \xi\in \mathcal{X} \\
    \end{array} .
\end{align*}
\end{theorem}

\subsubsection{$n=1$}
$\hat{u}$: minimizer of $f(u)$

Need to show $$\min_u E[f(u\wedge\xi)]=\min_{v(\xi)\leq \xi} E[f(v(\xi))]$$
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.4]{leq.png}
    \caption{Easy to show $\forall \xi,\ f(\hat{u}\wedge\xi)\leq f(v(\xi))$ when $v(\xi)\leq \xi$}
    \label{}
\end{figure}\end{center}

Easy to show $\forall \xi,\ f(\hat{u}\wedge\xi)\leq f(v(\xi))$ when $v(\xi)\leq \xi$. Then $$\argmin E[f(u\wedge \xi)]=\hat{u}=\argmin f(u)$$
\begin{equation}
    \begin{aligned}
        E[f(\hat{u}\wedge\xi)]&\geq \min_uE[f(u\wedge\xi)]\\
        &\geq \min_{v(\xi)\leq \xi}E[f(v(\xi))]\text{ (Consider $v^*(\xi)\geq u$)}\\
        &\geq E[f(\hat{u}\wedge\xi)]\text{ (See the figure)}\\
        \Rightarrow	\quad &\min_u E[f(u\wedge\xi)]=\min_{v(\xi)\leq \xi} E[f(v(\xi))]
    \end{aligned}
    \nonumber
\end{equation}

\subsubsection{$n\geq 2$}
$$\argmin E[f(u\wedge \xi)]\neq \hat{u}$$
\begin{example}
$$f(u_1,u_2)=(u_1+u_2-2)^2+(u_1-1)^2+(u_2-1)^2$$
$\xi_1,\xi_2$ can take values $0$ and $2$ with equal probability.

$\hat{u}=(1,1)$

$\argmin E[f(u\wedge \xi)]=(1.2,1.2)$
\end{example}

\subsection{Transformation for Constrained Problem}
$$\min_{u\in \mathcal{U}} E[f(u\wedge \xi)]$$
$$\downarrow $$
\begin{align*}
    &\min\quad E[f(v(\xi))]\\
    &\begin{array}{r@{\quad}r@{}l@{\quad}l}
    s.t.
    &v(\xi)&\leq \xi&,\forall \xi\in \mathcal{X}\\
    &v(\xi)&=(v_1(\xi_1),...,v_n(\xi_n))\in \mathcal{V}&, \forall \xi\in \mathcal{X} \\
    &\mathcal{V}&=\{u\wedge\xi\ | u\in \mathcal{U},\xi\in \mathcal{X}\}&
\end{array} .
\end{align*}
\textbf{Sufficient Conditions for the Transformation}
\begin{enumerate}[(a)]
    \item $\mathcal{U}=\{u|Au\leq b,u\geq l\}$, where $A\geq 0$
    \item $\mathcal{X}_j\subseteq[l_j,+\infty)$
\end{enumerate}
(Example: some situations $l=(l_1,...,l_n)=(0,...,0)$)

\subsection{Generalization}
$$\min_{u\in \mathcal{F}^n} l(u)+E[f(u\wedge \xi)]$$
\begin{enumerate}[$\bullet$]
    \item $l:\mathcal{F}^n \rightarrow \bar{\Re}$, $f:\mathcal{F}^n \rightarrow \bar{\Re}$
    \item $\xi\in \mathcal{X}\subseteq \mathcal{F}^n$
    \item $\xi$ \textbf{dependent} (different from before !)
\end{enumerate}

\subsubsection{Positive Dependence}
Let $F_{\xi_{i}}$ be the joint CDF of $\xi_{1}, \ldots, \xi_{i-1}, \xi_{i+1}, \ldots, \xi_{n}$ conditioned on $\xi_{i}$

$\left\{\xi_{1}, \ldots, \xi_{i-1}, \xi_{i+1}, \ldots, \xi_{n} \mid \xi_{i}\right\}$ is \underline{stochastically increasing} if
$\int_{S} d F_{\xi_{i}}(w)$ is an increasing function of $\xi_{i}$ for each increasing set $S$

$\left\{\xi_{1}, \ldots, \xi_{i-1}, \xi_{i}, \xi_{i+1}, \ldots, \xi_{n}\right\}$ has \underline{positive dependence} if
$\left\{\xi_{1}, \ldots, \xi_{i-1}, \xi_{i+1}, \ldots, \xi_{n} \mid \xi_{i}\right\}$ is stochastically increasing for all $i$

\begin{proposition}
    The collection of random variables generated by nonnegative linear combination of \textbf{independent log-concave} random variables has positive dependence.
\end{proposition}

\subsubsection{Transformation}
\begin{theorem}[Equivalent Transformation, Chen and Gao 2018]
    Suppose that (Assumption II)
    \begin{enumerate}[(1)]
        \item the function $f$ is lower semi-continuous with $f(u) \rightarrow+\infty$ for $|u| \rightarrow+\infty$;
        \item the function $f$ is componentwise (discrete) convex and \underline{supermodular};
        \item the random vector $\xi$ is \underline{positive dependent};
        \item $l(u)$ is componentwise increasing.
    \end{enumerate}
    Then problem $\min _{u \in \mathcal{F}} l(u)+E[f(u \wedge \xi)]$ has the \textbf{same optimal objective value} of
    \begin{align*}
        &\min\quad l(u)+E[f(v(\xi))]\\
        &\begin{array}{r@{\quad}r@{}l@{\quad}l}
        s.t.
        &v(\xi)&\leq \xi&,\forall \xi\in \mathcal{X}\\
        &v(\xi)&\leq u&,\forall \xi\in \mathcal{X}\\
        &v(\xi)&=(v_1(\xi_1),...,v_n(\xi_n))&, \forall \xi\in \mathcal{X} \\
        &v_i(\xi_i)&\text{ is increasing for all }i&\\
    \end{array} .
\end{align*}
\end{theorem}

\section{Single-Leg Capacity Allocation}
(Seats reserved for future consumers)
\subsection{Two-Class Model}
Two periods: Period 1, random demand $D_2$ for price $p_2$; Period 2, random demand $D_1$ for price $p_1$. $p_1>p_2$

Provide $y$ in period 1 and the remaining will be provided in period 2.
\begin{align*}
    &\max\quad p_1E_{D_1,D_2}[D_1\wedge (c-(c-y)\wedge D_2)]+p_2E_{D_2}[(c-y)\wedge D_2]\\
    &\begin{array}{r@{\quad}r@{}l@{\quad}l}
    s.t.
    &0\leq y\leq c&,y\in \mathcal{F} &\\
\end{array} .
\end{align*}
Where $\mathcal{F}=\mathbb{R}$ or $\mathbb{Z}$ and $a\wedge b=\min(a,b)$

\subsubsection{Theorem: convex $f$, $\argmin E_{D}f(u\wedge D)=\argmin f(u)$}
When $D_2$ is sufficiently high. Let $b=c-y$, and the question transferred to
\begin{align*}
    &\max\quad v(b)=p_1E_{D_1}[D_1\wedge (c-b)]+p_2b\\
    &\begin{array}{r@{\quad}r@{}l@{\quad}l}
    s.t.
    &0\leq b\leq c&,b\in \mathcal{F} &\\
\end{array} .
\end{align*}
$v(b)$ is a concave function.

\begin{theorem}
Consider the following optimization problem
\begin{align*}
    &\min\quad E_{D}f(u\wedge D)\\
    &\begin{array}{r@{\quad}r@{}l@{\quad}l}
    s.t.
    &0\leq u\leq c&,u\in \mathcal{F} &\\
\end{array} .
\end{align*}
Assume $D$ is a nonnegative random variable.

If $f$ is \textbf{convex} and $\mathcal{F}=\mathbb{R}$ or $f$ is \textbf{discrete convex} and $\mathcal{F}=\mathbb{Z}$, then any optimal solution of
\begin{align*}
    &\min\quad f(u)\\
    &\begin{array}{r@{\quad}r@{}l@{\quad}l}
    s.t.
    &0\leq u\leq c&,u\in \mathcal{F} &\\
\end{array} .
\end{align*}
is also optimal for the former optimization problem.

\textbf{(Actually, quasi-convexity suffices)}
\end{theorem}
According to the $n=1$ discussion of section 3, the theorem is easy to be proved. Then, the global-max in $v(b)$ is global-max for objective function.

Then we consider the equivalent minimum problem,
\begin{align*}
    &\max\quad \phi(y)=p_2y-p_1E_{D_1}[D_1\wedge y]\\
    &\begin{array}{r@{\quad}r@{}l@{\quad}l}
    s.t.
    &0\leq y\leq c&,y\in \mathcal{F} &\\
\end{array} .
\end{align*}
We need to find the optimal $y^*$ minimize the $\phi(y)$. To simplify the analysis, we find the $y^\circ$ which is the optimal $y$ regardless constraints. $y^*=\left\{\begin{matrix}
    0& \text{ if }y^\circ<0\\
    y^\circ&\text{ if }y^\circ\in[0,c]\\
    c&\text{ if }y^\circ>c
\end{matrix}\right.$
\subsubsection{Discrete, $\mathcal{F}=\mathbb{Z}$}
\begin{equation}
    \begin{aligned}
        \phi(y)-\phi(y-1)=p_2-p_1P(D_1\geq y)
    \end{aligned}
    \nonumber
\end{equation}
Then, the $y^\circ$ is
\begin{equation}
    \begin{aligned}
        \overline{y}&=\min\{y\in \mathbb{Z}: P(D_1>y)<r\}\\
        \underline{y}&=\max\{y\in \mathbb{Z}: P(D_1\geq y)>r\} \textbf{ (Littlewood's rule)}\\
        y^\circ&=[\underline{y},\overline{y}]\cap \mathbb{Z}
    \end{aligned}
    \nonumber
\end{equation}
Where $r=\frac{p_2}{p_1}$, higher $r$ causes lower $y^\circ$.

\begin{example}
    Suppose that $D_1$ is a Poisson random variable with mean 80, the full fare is $p_1 = 100$ and the discounted fare is $p_2 = 60$
\end{example}
$r=60/100=0.6$, $y^*=\max\{y\in \mathbb{Z}: P(D_1\geq y)>r\}=78$







\subsubsection{Continuous, $\mathcal{F}=\mathbb{R}$}
$y^\circ$ is the $y$ s.t. $1-F_1(y)=r$, where $F_1(\cdot)$ is the CDF of $D_1$.
$$y^\circ=F_1^{-1}(1-r)$$
\textbf{Special Case:} $D_1\sim \mathcal{N}(\mu,\sigma^2)$
\begin{equation}
    \begin{aligned}
        F_1(y)=\varPhi(\frac{y-\mu}{\sigma})
    \end{aligned}
    \nonumber
\end{equation}
$\varPhi(\cdot)$ is the CDF of the standard normal $\mathcal{N}(0,1)$. Then,
\begin{equation}
    \begin{aligned}
        y^\circ=\mu+\sigma\varPhi^{-1}(1-r)
    \end{aligned}
    \nonumber
\end{equation}
If $\frac{p_2}{p_1}=r<\frac{1}{2}$, $y^\circ$ increases as variance $\sigma$ increases.

\subsection{Multi-Class Model}
\begin{enumerate}[$\bullet$]
    \item $p_1>p_2>\cdots>p_n$
    \item Lower class demand arrives earlier.
    \item Demand of different classes are independent.
    \item Control: demand to accept or reject.
\end{enumerate}
\subsubsection{Sequence of Events}
At stage $j$ with remaining capacity $x$,
\begin{enumerate}
    \item Select booking limit $b$ for class $j$, equivalently, protection level $y=x-b$ for classes $l$, $l<j$.
    \item Demand $D_j$ is realized.
    \item Accept $b\wedge D_j$ of class $j$ and collect revenue $p_j(b\wedge D_j)$.
    \item Move on to stage $j-1$ with remaining capability $x-b\wedge D_j$.
\end{enumerate}

\subsubsection{Dynamic Programming}
Set $f_j(x,b)=p_j b+V_{j-1}(x-b)$, $V_0(x)=0, V_j(0)=0$, $x=0,1,...,c\text{ (discrete)}$, $x\in[0,c]\text{ (continuous)}$
\begin{equation}
    \begin{aligned}
        V_j(x)=\max_{b\in[0,x],b\in \mathcal{F}} \mathbb{E}[f_j(x,b\wedge D_j)]
        =\mathbb{E}[p_j (b\wedge D_j)]+\mathbb{E}[V_{j-1}(x-b\wedge D_j)]
    \end{aligned}
    \nonumber
\end{equation}
\begin{proposition}
(1). $\forall j$, $f_j$ is $L^\natural-$concave, $V_j$ is (discrete) convex; (2). The optimal solution of the dynamic programming $b_j^*$ is the same as $$\max_{b\in[0,x],b\in \mathcal{F}} f_j(x,b)=p_j b+V_{j-1}(x-b)$$
\end{proposition}
Define $y^*_{j-1}$ be the optimal solution of
$$\max_{y\geq 0,y\in \mathcal{F}} -p_j y+V_{j-1}(y)$$
Then $$b_j^*=(x-y_{j-1}^*)^+$$

\begin{equation}
    \begin{aligned}
        V_j(x)&=\mathbb{E}[f_j(x,(x-y_{j-1}^*)^+\wedge D_j)]\\
        &=\mathbb{E}[p_j(x-y_{j-1}^*)^+\wedge D_j+V_{j-1}(x-(x-y_{j-1}^*)^+\wedge D_j)]\\
        &=\left\{\begin{matrix}
            V_{j-1}(x)&\text{ if }x\leq y_{j-1}^*\\
            \mathbb{E}[p_j(x-y_{j-1}^*)\wedge D_j+V_{j-1}(x-(x-y_{j-1}^*)\wedge D_j)]&\text{ if }x> y_{j-1}^*
        \end{matrix}\right.
    \end{aligned}
    \nonumber
\end{equation}

\subsection{Discrete Case}
Define
\begin{equation}
    \begin{aligned}
        \Delta V_j(x)&=V_j(x)-V_j(x-1)
    \end{aligned}
    \nonumber
\end{equation}



\begin{lemma}
    If $x> y^*_{j-1}$, $\Delta V_j(x)=\mathbb{E}[\min\{p_j,\Delta V_{j-1}(x-D_j)\}]$
\end{lemma}
\begin{proof}
\quad\\
\begin{equation}
    \begin{aligned}
        \Delta V_j(x)&=p_j(\mathbb{E}[(x-y_{j-1}^*)\wedge D_j]-\mathbb{E}[(x-1-y_{j-1}^*)\wedge D_j])\\&+\mathbb{E}[V_{j-1}(x-(x-y_{j-1}^*)\wedge D_j)]-\mathbb{E}[V_{j-1}(x-1-(x-1-y_{j-1}^*)\wedge D_j)]\\
        &=\left\{\begin{matrix}
        p_j    &\text{ if }x-y^*_{j-1}\leq D_j\\
        \Delta V_{j-1}(x-D_j)    &\text{ if }x-y^*_{j-1}> D_j
        \end{matrix}\right.\\
        &=\mathbb{E}[p_j \mathbb{I}(x-D_j\leq y_{j-1}^*)+\Delta V_{j-1}(x-D_j)\mathbb{I}(x-D_j> y_{j-1}^*)]\\
        &(\text{ Since $y_{j-1}^*$ maximizes $-p_j y+V_{j-1}(y)$,}\\&\text{ $\Delta V_{j-1}(y)> p_j$ if $y\leq y_{j-1}^*$ and $\Delta V_{j-1}(y)\leq p_j$ if $y> y_{j-1}^*$})\\
        &=\mathbb{E}[\min\{p_j,\Delta V_{j-1}(x-D_j)\}]\\
    \end{aligned}
    \nonumber
\end{equation}
\end{proof}



\begin{proposition}[1.5 of GT 19]
    \quad

    \begin{enumerate}[(i)]
        \item $\Delta V_j(x+1)\leq \Delta V_j(x)$ (proved by $V_j$ is discrete concave)
        \item $\Delta V_{j+1}(x)\geq \Delta V_{j}(x)$
    \end{enumerate}
\end{proposition}
\begin{proof}
\quad\\
If $x\leq y^*_{j-1}$,
\begin{equation}
    \begin{aligned}
        \Delta V_j(x)&=V_{j-1}(x)-V_{j-1}(x-1)=\Delta V_{j-1}(x)
    \end{aligned}
    \nonumber
\end{equation}
If $x> y^*_{j-1}$ (i.e. $x-1\geq y^*_{j-1}$),
\begin{equation}
    \begin{aligned}
        \Delta V_j(x)
        &=\mathbb{E}[\min\{p_j,\Delta V_{j-1}(x-D_j)\}]\\
        &(\text{ $V_{j-1}(x)$ is discrete concave})\\
        &\geq \mathbb{E}[\min\{p_j,\Delta V_{j-1}(x)\}]\\
        &(\text{ Since $x>y_{j-1}^*$, $V_{j-1}(x)< p_j$})\\
        &=\Delta V_{j-1}(x)
    \end{aligned}
    \nonumber
\end{equation}
\end{proof}

\begin{theorem}[part of 1.6 of GT 19]
    \quad

    The optimal protection level at stage $j$ is $$y_{i-1}^*=\max\{y\in \mathbb{N}_+: \Delta V_{j-1}(y)>p_j \}$$
    Moreover, $y_{n-1}^*\geq y_{n-2}^*\geq \cdots \geq y_1^*=y_0^*=0$
\end{theorem}
(Easy to prove: Since $y_{j-1}^*$ maximizes $-p_j y+V_{j-1}(y)$, $\Delta V_{j-1}(y)> p_j$ if $y\leq y_{j-1}^*$ and $\Delta V_{j-1}(y)\leq p_j$ if $y> y_{j-1}^*$)

\textbf{Note:} Littlewood's rule is a special case for $n=2$.

\subsubsection{Discrete Case: Reformulation}
\begin{equation}
    \begin{aligned}
        V_j(x)&=\mathbb{E}[p_j(x-y_{j-1}^*)^+\wedge D_j+V_{j-1}(x-(x-y_{j-1}^*)^+\wedge D_j)]\\
        &=V_{j-1}(x)+\mathbb{E}[p_j(x-y_{j-1}^*)^+\wedge D_j+(V_{j-1}(x-(x-y_{j-1}^*)^+\wedge D_j)-V_{j-1}(x))]\\
        &=V_{j-1}(x)+\mathbb{E}[p_j(x-y_{j-1}^*)^+\wedge D_j-\sum_{z=1}^{(x-y_{j-1}^*)^+\wedge D_j}\Delta V_{j-1}(x+1-z)]\\
        &=V_{j-1}(x)+\mathbb{E}[\sum_{z=1}^{(x-y_{j-1}^*)^+\wedge D_j}(p_j-\Delta V_{j-1}(x+1-z))]\\
        V_j(x)&=V_{j-1}(x)+\mathbb{E}[\sum_{z=1}^{u^*}(p_j-\Delta V_{j-1}(x+1-z))]\\
        &u^*=\min\{(x-y_{j-1}^*)^+,D_j\}\\
        &y_{i-1}^*=\max\{y\in \mathbb{N}_+: p_j<\Delta V_{j-1}(y) \}
    \end{aligned}
    \nonumber
\end{equation}
\begin{enumerate}[$\bullet$]
    \item $y_1^*\leq y_2^*\leq \cdots\leq y_n^*$
    \item The "nested" booking limit $b_j^*=C-y_{j-1}^*$, $j=2,...,n$
    
    (nested booking limit is the total amount can be booked in $j,j+1,...,n$)
    $$b_j^*=y_j$$
    \item The marginal utility at $j$ of choosing to reserve one more item in the next stage $j-1$: $$\pi_j(x)=\Delta V_{j-1}(x)$$
    \item The amount of selling at stage $j$ $$u^*=\left\{\begin{matrix}
        0&\text{ if }p_j<\pi_j(x)\\
        \min\{\max\{z:p_j\geq \pi_j(x-z)\},D_j\}&\text{ if }p_j\geq \pi_j(x)
    \end{matrix}\right.$$
     $p_j<\pi_j(x)$ means the marginal utility of reserving is larger than selling it now.
\end{enumerate}

We can further compute, if $x>y_{j-1}^*$,
\begin{equation}
    \begin{aligned}
        \Delta V_j(x)=
            p_jPr(D_j\geq x-y_{j-1}^*)+\sum_{k=0}^{x-y_{j-1}^*-1}\Delta V_{j-1}(x-k)Pr(D_j=k)
    \end{aligned}
    \nonumber
\end{equation}
If $x\leq y_{j-1}^*$, $\Delta V_j(x)=\Delta V_{j-1}(x)$.

Which will simplify the computation.



\subsubsection{Discrete Case: Computation}

The policy is implemented as follows:
\begin{enumerate}
    \item At stage $n$, we start with $x_{n}=c$ units of inventory and we protect $y_{n-1}\left(x_{n}\right)=\min \left\{y_{n-1}^{*}, x_{n}\right\}$ units of capacity for fares $n-1, n-2, \ldots, 1$.
    \item Therefore, we allow up to $\left[x_{n}-y_{n-1}^{*}\right]^{+}$units of capacity to be sold to fare class $n$.
    \item We sell $\min \left\{\left[x_{n}-y_{n-1}^{*}\right]^{+}, D_{n}\right\}$ units of capacity to fare class $n$ and we have a remaining capacity of $x_{n-1}=x_{n}-\min \left\{\left[x_{n}-y_{n-1}^{*}\right]^{+}, D_{n}\right\}$ at stage $n-1$.
    \item We protect $y_{n-2}\left(x_{n-1}\right)=\min \left\{y_{n-2}^{*}, x_{n-1}\right\}$ units of capacity for fares $n-2, n-1, \ldots, 1$.
    \item Therefore, we allow up to $\left[x_{n-1}-y_{n-2}^{*}\right]^{+}$units of capacity to be sold to fare class $n-1$.
    \item We continue until we reach stage 1 with $x_{1}$ units of capacity, allowing $\left(x_{1}-y_{0}\right)^{+}=\left(x_{1}-0\right)^{+}=x_{1}$ to be sold to fare class $1$.
\end{enumerate}

\begin{equation}
    \begin{aligned}
        V_j(x)=\mathbb{E}[p_j\min\{(x-y_{j-1}^*)^+, D_j\}+V_{j-1}(x-\min\{(x-y_{j-1}^*)^+, D_j\})]
    \end{aligned}
    \nonumber
\end{equation}
$y_0^*=0, V_0(x)=0$, then we can compute $y_1^*,V_1(x)$,...

\textbf{Backward:}
Use
\begin{equation}
    \begin{aligned}
        \Delta V_j(x)&=
            p_jPr(D_j\geq x-y_{j-1}^*)+\sum_{k=0}^{x-y_{j-1}^*-1}\Delta V_{j-1}(x-k)Pr(D_j=k)\\
        y_{j-1}^*&=\max\{y\in \mathbb{N}_+: p_j<\Delta V_{j-1}(y) \}
    \end{aligned}
    \nonumber
\end{equation}
\begin{enumerate}
    \item $V_1(x_1)=\mathbb{E}[p_1\min\{x_1, D_1\}]$, then $\Delta V_{1}(x)=p_1 Pr(D_1\geq x)$
    \item $y_1^*=\max\{y\in \mathbb{N}_+: p_2<\Delta V_{1}(y) \}=\max\{y:Pr(D_1\geq y)>\frac{p_2}{p_1}\}$
    \begin{equation}
        \begin{aligned}
            \Delta V_2(x)&=p_2Pr(D_2\geq x-y_{1}^*)+\sum_{k=0}^{x-y_{1}^*-1}p_1 Pr(D_1\geq x-k)Pr(D_2=k)
        \end{aligned}
        \nonumber
    \end{equation}
    \item $y_2^*=\max\{y\in \mathbb{N}_+: p_3<\Delta V_{2}(y)\}=\max\{y: Pr(\Delta V_{1}(y-D_2)>p_3)\}$
    \item $\dots$
\end{enumerate}

The complexity is $O(nC^2)$

\begin{example}
    Suppose that there are five fare classes. The demand for all fare classes is a Poisson random variable. The fares and the expected demand for the
    five fare classes are given by $(p_5, p_4, p_3, p_2, p_1) = (15, 35, 40, 60, 100)$ and $(\mathbb{E}{D_5}, \mathbb{E}{D_4}, \mathbb{E}{D_3}, \mathbb{E}{D_2}, \mathbb{E}{D_1}) = (120, 55, 50, 40, 15)$. For this problem instance, the optimal protection levels are
\end{example}
\begin{enumerate}
    \item $V_1(x_1)=\mathbb{E}[100\min\{x_1, D_1\}]$, then $\Delta V_{1}(x)=100 Pr(D_1\geq x)$
    \item $y_1^*=\max\{y:Pr(D_1\geq y)>\frac{3}{5}\}=14$
    \begin{equation}
        \begin{aligned}
            \Delta V_2(x)&=60Pr(D_2\geq x-14)+\sum_{k=0}^{x-15}100 Pr(D_1\geq x-k)Pr(D_2=k)
        \end{aligned}
        \nonumber
    \end{equation}
    \item $y_2^*=\max\{y\in \mathbb{N}_+: p_j<$
\end{enumerate}



\subsection{Continuous Case}
Skip

\subsection{ Generalized Newsvendor Problem: High-before-low arrival pattern}
Consider the problem of selecting $c$ to maximize $$\Pi_n(c)=V_n(c)-kc$$
Where $V_n(c)$ is the expected revenue to the multi-fare RM problem.

Assume high-before-low arrival pattern. Then
$$V_n(c)=\sum_{j=1}^n p_j \mathbb{E}[D_j\wedge (c-D_{1:j-1})^+]$$
and $$\Delta V_n(c)=\sum_{j=1}^n(p_j-p_{j+1})Pr(D_{1:j}>c)$$
Where $D_{1:j}=\sum_{l=1}^j D_l, p_{n+1}=0$

\subsection{Heuristics}
When there are two classes, we find $y^*$: $\max\{y\in \mathbb{Z}: P(D_1\geq y)>r\}$

We try to use this form to simplify our computation,

\textbf{EMSR (expected marginal seat revenue)}
\begin{enumerate}[$\bullet$]
    \item \textbf{EMSR - a}
    \begin{equation}
        \begin{aligned}
            y_k^{j+1}&=\max\{y:P(D_k\geq y)>\frac{p_{j+1}}{p_k}\}, k=j,j-1,...,1\\
            y_j&=\sum_{k=1}^jy_k^{j+1}
        \end{aligned}
        \nonumber
    \end{equation}
    \item \textbf{EMSR - b}
    \begin{equation}
        \begin{aligned}
            \overline{p}_j&=\frac{\sum_{k=1}^jp_k \mathbb{E} [D_k]}{\sum_{k=1}^j \mathbb{E} [D_k]}\\
            y_j&=\max\{y:P(\sum_{k=1}^j D_k\geq y)>\frac{p_{j+1}}{\overline{p}_j}\}
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}

\subsection{ Bounds on Optimal Expected Revenue}
\subsubsection{Upper Bound}
\begin{equation}
    \begin{aligned}
        \overline{V}(c|D):&=\max\{\sum_{j=1}^np_jx_j|\sum_{j=1}^nx_j\leq c,0\leq x_j\leq D_j,j=1,...,n\}\\
        V_n^U(c):&=\mathbb{E}[\overline{V}(c|D)]\\
        &=\sum_{j=1}^n(p_j-p_{j+1})\sum_{k=1}^cPr(D_{1:j}\geq k),\quad (\text{Set }p_{n+1}=0)\\
        \mathbb{E}[\overline{V}(c|D)]&\leq \overline{V}(c|D)=\sum_{j=1}^n(p_j-p_{j+1})\min\{\overline{D}_{1:j},c\}
    \end{aligned}
    \nonumber
\end{equation}
\subsubsection{Lower Bound}
Using zero protection level
\begin{equation}
    \begin{aligned}
        V_n^L(c)&=\sum_{j=1}^n p_j \mathbb{E}[\min\{D_k,(c-D_{j+1:n})^+\}]\\
        &=\sum_{j=1}^n(p_j-p_{j-1})\mathbb{E}[\min\{D_{j:n},c\}],\quad (\text{Set }p_{0}=0)
    \end{aligned}
    \nonumber
\end{equation}


\subsection{Dynamical Models}
\begin{enumerate}[$\bullet$]
    \item $p_1\geq p_2\geq \cdots \geq p_n$.
    \item $T$ periods.
    \item At most one arrival each period.
    \item $\lambda_{jt}$: probability of an arrival of class $j$ in period $t$.
    \item $M_t$: set of offered classes.
\end{enumerate}

\subsubsection{Discrete Time}
\begin{equation}
    \begin{aligned}
        V_t(x)&=\sum_{j\in M_t}\lambda_{jt}\max\{p_j+V_{t-1}(x-1),V_{t-1}(x)\}+(1-\sum_{j\in M_t}\lambda_{jt})V_{t-1}(x)\\
        &=V_{t-1}(x)+\sum_{j\in M_t}\lambda_{jt}[p_j-\Delta V_{t-1}(x)]^+\\
        &=V_{t-1}(x)+R_t(\Delta V_{t-1}(x))
    \end{aligned}
    \nonumber
\end{equation}
Where we set $R_t(z)=\sum_{j\in M_t}\lambda_{jt}[p_j-z]^+$, $V_t(0)=0$, $V_0(x)=0,\forall x\geq 0$

\subsubsection{Continuous Time: Poisson arrival}
$$\frac{\partial V_t(x)}{\partial t}=R_t(\Delta V_t(x))$$

\subsubsection{Optimal Policy: discrete time}
Let $$a(t,x)=\max\{j:p_j\geq \Delta V(t-1,x)\}$$
Optimal to accept all fares in the active set
$$A(t,x)=\{j\in M_t: j\leq a(t,x)\}$$
and reject the remaining fare classes

\subsubsection{Structural Properties}
\begin{theorem}[1.18 of GT]
\quad

\begin{enumerate}[$\bullet$]
    \item $V_t(x)$ is increasing in $t,x$.
    \item $\Delta V_t(x)$ is increasing in $t$ and decreasing in $x$.
    \item $a(t,x),A(t,x)$ is increasing in $x$.
\end{enumerate}
If $\lambda_{jt}\equiv \lambda_j>0$, $M_t\equiv M=\{1,...,n\}$, then
\begin{enumerate}[$\bullet$]
    \item $V_t(x)$ is strictly increasing and concave in $t$.
    \item $a(t,x),A(t,x)$ is decreasing in $t$.
\end{enumerate}
\end{theorem}

\subsubsection{Discrete Case: Computation}
\begin{equation}
    \begin{aligned}
        V_t(x)=V_{t-1}(x)+\sum_{j\in M_t}\lambda_{jt}[p_j-\Delta V_{t-1}(x)]^+
    \end{aligned}
    \nonumber
\end{equation}
$V_0(x)=0$, then $V_1(x)$, then $\Delta V_1(x)$.

The complexity is $O(nC T)$ ($T \approx O(C)$)


\section{Network Revenue Management with Independent Demands}
\subsection{Settings}
\begin{enumerate}[$\bullet$]
    \item $m$ resources with initial capacities $c=\begin{pmatrix}
        c_1\\
        c_2\\
        \vdots\\
        c_m
    \end{pmatrix}\in \mathbb{Z}_+^m$
    \item Time from $T,T-1,T-2,...$ to $0$.
    \item ODF $kj$: Itineraries $k=1,...,K$; Possible fares for itinerary $k$, $p_{kj}$, $j\in \{1,...,n_k\}$. (Every itinerary may have $n_k$ kinds of prices).
    \item Demand arrives as compound Poisson arrival process with rate $\lambda_{tkj}$ at time $t$ for ODF $kj$.
    \item Resources utilized by itinerary $k$: $A_k=\begin{pmatrix}
        a_{1k}\\
        \vdots\\
        a_{mk}
    \end{pmatrix}$, $a_{ik}\in\{0,1\}$ with $a_{ik}=1$ if resource $i$ is consumed by itinerary $k$.
    \item $V(t,x)$: the maximum total expected revenue that can be extracted when the remaining capacities are $x\in \mathbb{Z}_+^m$ and the remaining time is $t\in \mathbb{R}_+$.
    \item Decision: $u=\{u_{kj}: j=1,...,n_k,k=1,...,K\}$, $u_{kj}=\left\{\begin{matrix}
        1& \text{accept a request for ODF }k_j\\
        0& \text{others}
    \end{matrix}\right.$
    \item Feasible set of decisions: $u(x)=\{u_{kj}\in\{0,1\}: A_ku_{kj}\leq x, j=1,...,n_k,k=1,...,K\}$
\end{enumerate}


\subsection{HJB Equation}
Assume now that the state is $(t, x)$ and consider a time increment $\delta t$ that is small enough so that we can approximate the probability of an arrival of a request for fare $j$ of itinerary $k$ by $\lambda_{tkj}\delta k$.

$$
\begin{aligned}
V(t, x)=& \sum_{k=1}^{K} \sum_{j=1}^{n_{k}} \lambda_{t k j} \delta t \max _{u_{k j} \in\{0,1\}}\left[p_{k j} u_{k j}+V\left(t-\delta t, x-A_{k} u_{k j}\right)\right] \\
&+\left\{1-\sum_{k=1}^{K} \sum_{j=1}^{n_{k}} \lambda_{t k j} \delta t\right\} V(t-\delta t, x)+o(\delta t)
\end{aligned}
$$
where $o(\delta t)$ is a quantity that goes to zero faster than $\delta t$. Subtracting $V(t-\delta t, x)$ from both side of the equation, dividing by $\delta t$, and using the notation $\Delta_{k} V(t, x)=$ $V(t, x)-V\left(t, x-A_{k}\right)$, we obtain the \underline{Hamilton-Jacobi-Bellman (HJB) equation}
$$
\frac{\partial V(t, x)}{\partial t}=\sum_{k=1}^{K} \sum_{j=1}^{n_{k}} \lambda_{t k j}\left[p_{k j}-\Delta_{k} V(t, x)\right]^{+}
$$
with boundary conditions $V(t, 0)=V(0, x)=0$ for all $t \geq 0$ and all $x \geq 0$. Notice that term $\left[p_{k j}-\Delta V_{k}(t, x)\right]^{+}$is equivalent to the maximum of $p_{k j} u_{k j}+V(t, x-$ $\left.A_{k} u_{k j}\right)-V(t-\delta t, x)$ over $u_{k j} \in\{0,1\} .$

For any vector $z \geq 0$, Define
$$
R_{t}(u, z):=\sum_{k=1}^{K} \sum_{j=1}^{n_{k}} \lambda_{t k j}\left[p_{k j}-z_{k}\right] u_{k j}
$$
and
$$
\begin{aligned}
\mathcal{R}_{t}(z):=\max _{u} R_{t}(u, z)=\sum_{k=1}^{K} \sum_{j=1}^{n_{k}} \lambda_{t k j} \max _{u_{j k} \in\{0,1\}}\left[p_{k j}-z_{k}\right] u_{k j}=\sum_{k=1}^{K} \sum_{j=1}^{n_{k}} \lambda_{t k j}\left[p_{k j}-z_{k}\right]^{+}
\end{aligned}
$$
Then $$
\frac{\partial V(t, x)}{\partial t}=\mathcal{R}_{t}(\Delta V(t, x)),\quad \Delta V(t, x)=\begin{pmatrix}
    \Delta_{1} V(t, x)\\
    \Delta_{2} V(t, x)\\
    \vdots\\
    \Delta_{K} V(t, x)\\
\end{pmatrix}
$$

\begin{center}
    \fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\begin{enumerate}
        \item Let's aggregate ODF's into a single index.
        \item $n=\sum_{k=1}^K n_k$
        \item HJB equation:
        $$\frac{\partial V(t, x)}{\partial t}=\mathcal{R}_{t}(\Delta V(t, x))=\sum_{j\in M_t} \lambda_{t j}\left[p_{j}-\Delta_jV(t,x)\right]^+$$
        \begin{enumerate}[$\bullet$]
            \item $V(t,0)=V(0,x)=0,\quad \forall t\geq 0,x\geq 0$
            \item $M_t\subset \{1,...,n\}$: offered set of fares at $t$
            \item $\Delta_jV(t,x)=V(t,x)-V(t,x-A_j)$
        \end{enumerate}
        \item Optimal Control:
        $$u_j^*(t,x)=\left\{\begin{matrix}
            1& \text{if $j\in M_t$, $A_j\leq x$ and \underline{$p_j\geq \Delta_j V(t,x)$}}\\
            0& \text{others}
        \end{matrix}\right.$$
    \end{enumerate}}}
\end{center}

Compute exact $\Delta_j V(t,x)$ can be expensive, we can use heuristics to approx it by $\Delta_j \widetilde{V}(t,x)$

\subsection{Upgrades}
Let $u_j$ be the set of products that can be used to fulfill a request for product $j$.

Customers are willing to take any products $k\in u_j$ at the price of product $p_j$.
\begin{equation}
    \begin{aligned}
        \frac{\partial V(t, x)}{\partial t}=\sum_{j\in M_t} \lambda_{j}\max_{k\in u_j}\left[p_{j}-\Delta_kV(t,x)\right]^+=\sum_{j\in M_t} \lambda_{t j}\left[p_{j}-\hat{\Delta}_jV(t,x)\right]^+
    \end{aligned}
    \nonumber
\end{equation}
where $\hat{\Delta}_jV(t,x)=\min_{k\in u_j}\Delta_k V(t,x)$ (Use the least valuable product to fulfill $p_j$'s request.)

\subsection{Upsells}
Selling $j$ instead of $k$ to get higher revenue, but may be rejected by customers.
\begin{enumerate}[$\bullet$]
    \item $\gamma_{jk}$: revenue obtained from selling product $j$ and fulfilling it with product $k\in u_j$.
    \item $\pi_{jk}$: probability a customer will accept the upgrade from product $j$ to product $k$.
\end{enumerate}
\begin{equation}
    \begin{aligned}
        \frac{\partial V(t, x)}{\partial t}=\sum_{j\in M_t} \lambda_{j}\max_{k\in u_j}\left[\pi_{jk}(r_{jk}-\hat{\Delta}_kV(t,x))+(1-\pi_{jk})(p_j-\hat{\Delta}_jV(t,x))\right]
    \end{aligned}
    \nonumber
\end{equation}

\subsection{Linear programming-based upper bound}
The discrete maximum problem is
\begin{equation}
    \begin{aligned}
        V(t,x)&=\max_{u\in U(x)}\sum_{j=1}^n\lambda_{tj}[p_ju_j+V(t-1,x-u_jA_j)]
    \end{aligned}
    \nonumber
\end{equation}
\textbf{\underline{Deterministic Linear Program}}

Let $D_j$ be the aggregate demand for ODF $j$ over $[0,T]$.

Then $D_j$ is Poisson with parameter $\Lambda_j=\int_0^T\lambda_{sj}ds$.

Define
$$\begin{array}{rlr}
    \bar{V}(T, c):=\max & \sum_{j \in N} p_{j} y_{j} & \\
    \text { s.t. } & \sum_{j \in N} a_{i j} y_{j} \leq c_{i} \quad \forall i \in M \\
    & 0 \leq y_{j} \leq \Lambda_{j} \quad \forall j \in N .
\end{array}$$

$$\begin{array}{rlr}
    \bar{V}(T, c|D):=\max & \sum_{j \in N} p_{j} y_{j} & \\
    \text { s.t. } & \sum_{j \in N} a_{i j} y_{j} \leq c_{i} \quad \forall i \in M \\
    & 0 \leq y_{j} \leq D_{j} \quad \forall j \in N .
\end{array}$$

\begin{theorem}[2.2 of GT]
$$V(T,C)\leq \mathbb{E}[\bar{V}(T, c|D)]\leq \bar{V}(T, c)$$
\end{theorem}
$\bar{V}(T, c)$ is the revenue of expected demand, $\mathbb{E}[\bar{V}(T, c|D)]$ is probability combination that is concave in $D$, so $\mathbb{E}[\bar{V}(T, c|D)]\leq \bar{V}(T, c)$. And $V(T,C)$'s decision is feasible in $\mathbb{E}[\bar{V}(T, c|D)]$, so $V(T,C)\leq \mathbb{E}[\bar{V}(T, c|D)]$.

\underline{Dual formulation} of $\bar{V}(T, c)$
$$\begin{array}{rlr}
    \bar{V}(T, c):=\min & \sum_{i \in M} c_{i} z_{i}+\sum_{j\in N}\Lambda_j\beta_j & \\
    \text { s.t. } & \sum_{i \in M} a_{i j} z_{i}+\beta_j \geq p_{j} \quad \forall j \in N \\
    & z_i, \beta_j\geq 0 \quad \forall i \in M, \forall j \in N.
\end{array}$$
We can simplify the formulation. Since $\beta_j\geq p_{j}-\sum_{i \in M} a_{i j} z_{i}$, $\beta_j\geq 0$ and dual is a minimization problem, we can rewrite $\beta_j= [p_{j}-\sum_{i \in M} a_{i j} z_{i}]^+$. Then,
\begin{equation}
    \begin{aligned}
        \sum_{j\in N}\Lambda_j\beta_j=\sum_{j\in N}\Lambda_j[p_{j}-\sum_{i \in M} a_{i j} z_{i}]^+=\int_0^T\mathcal{R}_{t}(A^Tz)dt
    \end{aligned}
    \nonumber
\end{equation}
so,
\begin{equation}
    \begin{aligned}
        \bar{V}(T, c)=\min_{z\geq 0} & \int_0^T\mathcal{R}_{t}(A^Tz)dt+c^Tz
    \end{aligned}
    \nonumber
\end{equation}

The optimal solution $z_i^*$ gives an estimation of the marginal value of the $i^{th}$ resource. The approximation of $\Delta_j V(T,c)$ is $\sum_{i\in M}a_{ij}z_i^*$


\begin{center}
    \fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\textbf{\underline{Bid-price Heuristic}}

    Accept ${ODF}_j$ if and only if $$p_j\geq \sum_{i\in M}a_{ij}z_i^*\text{ and }A_j\leq x$$
    
    \textbf{\underline{Probabilistic Admission Control (PAC) Heuristic}}
    
    Accept ${ODF}_j$ with probability $\frac{y_j^*}{\Lambda_j}$ whenever $A_j\leq x$.}}
\end{center}

Bid-price heuristic is not in general asymptotically optimal.

PAC heuristic is asymptotically optimal.
\begin{theorem}
Let $\Pi^b(T,c)$ be the total expected revenue from PAC heuristic and $V^b(T,c)$ be the optimal total expected revenue corresponding to circumstance $b\geq 1$ with capacity $bc$ and $b\lambda_{jt}$. Then $$\lim_{b \rightarrow \infty}\frac{\Pi^b(T,c)}{V^b(T,c)}=1$$
\end{theorem}

\subsection{Dynamic Programming Decomposition (DPD)}
In this section, we describe two possible approaches for approximating the value functions $V (t, \cdot)$ for the \underline{discrete-time} formulation
\begin{equation}
    \begin{aligned}
        V(t, x)=V(t-1, x)+\mathcal{R}_{t}(\Delta V(t-1, x))
    \end{aligned}
    \nonumber
\end{equation}


Consider the aggregated single index formulation
\begin{equation}
    \begin{aligned}
        V(t,x)=\max_{u\in U(x)}\sum_{j=1}^n\lambda_{tj}[p_ju_j+V(t-1,x-u_jA_j)]
    \end{aligned}
    \nonumber
\end{equation}
with $V(t,0)=V(0,x)=0$ and $\sum_{j=1}^n\lambda_{tj}=1,\lambda_{tj}\geq 0$. (scale can be standardized)

\subsubsection{Deterministic Linear Program}
The former DLP we use
$$\begin{array}{rlr}
    \bar{V}(T, c):=\max & \sum_{j \in N} p_{j} y_{j} & \\
    \text { s.t. } & \sum_{j \in N} a_{i j} y_{j} \leq c_{i} \quad \forall i \in M \\
    & 0 \leq y_{j} \leq \Lambda_{j} \quad \forall j \in N .
\end{array}$$
Its dual optimal value is $(z_1^*,z_2^*,...,z_m^*)$. We choose an arbitrary resource $i$ and relax the first set of constraints for all of the resources except for resource $i$ by associating the dual multipliers $(z_1^*,z_2^*,...,z_m^*)$ with them.

We relax the first constraints $\sum_{j \in N} a_{i j} y_{j} \leq c_{i}\quad \forall i\in M$, which won't change the objective value,
\begin{equation}
    \begin{aligned}
        \max\sum_{j \in N} p_{j} y_{j}&=\sum_{j \in N} p_{j} y_{j}-\sum_{k\neq i}[\sum_{j \in N} a_{k j} y_{j} - c_{k}]z_k\\
        &=\sum_{j\in N}[p_j-\sum_{k\neq i}a_{kj}z_k^*]y_j+\sum_{k\neq i}z_k^*c_k
    \end{aligned}
    \nonumber
\end{equation}

The new DLP is
$$\begin{array}{rlr}
    \bar{V}(T, c):=\max&\sum_{j\in N}[p_j-\sum_{k\neq i}a_{kj}z_k^*]y_j+\sum_{k\neq i}z_k^*c_k & \\
    \text { s.t. } & \sum_{j \in N} a_{i j} y_{j} \leq c_{i} \quad \forall i \in M \\
    & 0 \leq y_{j} \leq \Lambda_{j} \quad \forall j \in N .
\end{array}$$

We can prove the optimal $y^*$ and optimal objective values are the same.

\begin{claim}
The optimal values $y_j^*$ and optimal objective values of these two DLP are the same.
\end{claim}
(This claim can help prove the upperbound).

\begin{equation}
    \begin{aligned}
        V(t,x)=\max_{u\in U(x)}\sum_{j=1}^n\lambda_{tj}[p_ju_j+V(t-1,x-u_jA_j)]
    \end{aligned}
    \nonumber
\end{equation}

We consider the optimal total expected revenue in the single-resource revenue management problem for resource $i$, the corresponding price of ${ODF}_j$ should be $p_j-\sum_{k\neq i}a_{kj}z_k^*$.
Then the formulation is
\begin{equation}
    \begin{aligned}
        v_i(t,x_i)=\max_{u\in U_i(x_i)}\sum_{j\in N}\lambda_{tj}\left\{[p_j-\sum_{k\neq i}a_{kj}z_k^*]u_j+v_i(t-1,x_i-u_ja_{ij})\right\}
    \end{aligned}
    \nonumber
\end{equation}
We can prove that
\begin{enumerate}[$\bullet$]
    \item $v_i(T,c_i)\leq \bar{V}(T,c)-\sum_{k\neq i}z_k^*c_k$
    \item Theorem 2.11 of GT $$V(t,x)\leq \min_{i\in M}\{v_i(t,x_i)+\sum_{k\neq i}z_k^*x_k\}$$
\end{enumerate}

\subsubsection{Lagrangian Relaxation}
$$\begin{array}{cl}
    V(t, x)=\max & \sum_{j=1}^n\lambda_{tj}[p_ju_j+V(t-1,x-u_jA_j)] \\
    \text { s.t. } \quad & u_jA_j \leq x \\
    & u_{j} \in\{0,1\} \quad \forall \in N
\end{array}$$

To demonstrate the Lagrangian relaxation strategy, we use decision variables $\{w_{ij} : i \in M, j \in N\}$ in the dynamic programming formulation of the network revenue management problem, where $w_{ij} = 1$ if we make ${ODF}_j$ available for purchase on flight leg $i$, otherwise $w_{ij} = 0$.
$$\begin{array}{cl}
    V(t, x)=\max & \sum_{j \in N} \lambda_{t j}\left\{p_{j} w_{\psi j}+V\left(t-1, x-\sum_{i \in M} w_{i j} a_{i j} e_{i}\right)\right\} \\
    \text { s.t. } \quad & a_{i j} w_{i j} \leq x_{i} \\
    & w_{i j}=w_{\psi j} \\
    & w_{i j} \in\{0,1\}, w_{\psi j} \in\{0,1\} \quad \forall i \in M, j \in N
\end{array}$$

We can relex the second set of constraints by adding Lagrange mutipliers $\{\alpha_{tij} : i \in M, j \in N\}$.
\underline{Relaxed dynamic program}:
$$\begin{array}{cl}
    V^{\alpha}(t, x)=\max& \sum_{j \in N} \lambda_{t j}\left\{\sum_{i \in M} \alpha_{t i j} w_{i j}+\left[p_{j}-\sum_{i \in M} \alpha_{t i j}\right] w_{\psi j}\right.
    \left.+V^{\alpha}\left(t-1, x-\sum_{i \in M} w_{i j} a_{i j} e_{i}\right)\right\} \\
    \text { s.t. } \quad & a_{i j} w_{i j} \leq x_{i} \\
    & w_{i j} \in\{0,1\}, w_{\psi j} \in\{0,1\} \quad \forall i \in, j \in N
\end{array}$$

\begin{theorem}[2.13 of GT]
    Assume that the value functions $\left\{v_{i}^{\alpha}(t, \cdot): t=1, \ldots, T\right\}$ are computed through the dynamic program
$$
v_{i}^{\alpha}\left(t, x_{i}\right)=\max _{w_{i} \in \mathcal{U}_{i}\left(x_{i}\right)}\left\{\sum_{j \in N} \lambda_{t j}\left\{\alpha_{t i j} w_{i j}+v_{i}^{\alpha}\left(t-1, x_{i}-w_{i j} a_{i j}\right)\right\}\right\}
$$
Then
$$
V^{\alpha}(t, x)=\sum_{i \in M} v_{i}^{\alpha}\left(t, x_{i}\right)+\sum_{\tau=1}^{t} \sum_{j \in N} \lambda_{\tau j}\left[p_{j}-\sum_{i \in M} \alpha_{\tau i j}\right]^{+}
$$
\end{theorem}

\begin{theorem}[2.14 of GT]
    For any set of Lagrange multipliers $\alpha$, we have
    $$V(t,x)\leq V^{\alpha}(t,x)\quad \forall x\in \mathbb{Z}_+^m,t=1,...,t$$
\end{theorem}

The tightest possible upper bound, we can solve the problem$$\min_{\alpha\in \mathbb{R}^{Tmn}} V^{\alpha}(T,c)$$

\begin{lemma}[2.15 of GT]
    $V^{\alpha}(t,x)$ is a convex function of $\alpha$ for any $t=1,...,T$ and $x\in \mathbb{Z}_+^m$.
\end{lemma}
Then compute $\min V^{\alpha}(T,c)$ can be easier.
















\end{document}







