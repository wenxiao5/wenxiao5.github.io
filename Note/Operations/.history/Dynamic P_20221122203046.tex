\documentclass[11pt]{elegantbook}
\definecolor{structurecolor}{RGB}{40,58,129}
\linespread{1.6}
\setlength{\footskip}{20pt}
\setlength{\parindent}{0pt}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\elegantnewtheorem{proof}{Proof}{}{Proof}
\elegantnewtheorem{claim}{Claim}{prostyle}{Claim}
\DeclareMathOperator{\col}{col}
\title{\textbf{Dynamic Programming}}
\author{Wenxiao Yang}
\institute{Department of Mathematics, University of Illinois at Urbana-Champaign}
\date{2022}
\setcounter{tocdepth}{2}
\cover{cover.jpg}
\extrainfo{All models are wrong, but some are useful.}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}

\addbibresource[location=local]{reference.bib} % bib

\begin{document}

\maketitle
\frontmatter
\tableofcontents
\mainmatter


\chapter{Multi-Armed Bandits}
\section{Multi-Armed Bandit Problem}
\begin{definition}[Multi-Armed Bandit Problem]
    A decision-maker ("gambler") chooses one of $n$ actions ("arms") in each time step. Chosen arm produces random payoff from unknown distribution. Goal: Maximize expected total payoff.
\end{definition}

Every time, the DM pulls an arm $i\in\{1,...,n\}$, then he observes a reward $r_t$ which follows a distribution.
\begin{enumerate}[$\bullet$]
    \item Each arm has a \textbf{type} that \underline{determines its payoff distribution}.
    \item Gambler has a \underline{prior distribution} over \textbf{types} for each arm.
    \item \textbf{Types} are independent random variables.
    \item Objective: maximize expected discounted reward $\sum_{t=0}^\infty \beta^t r_t$
\end{enumerate}
\begin{definition}[State]
    After pulling arm $i$ some number of times, gambler has a \underline{posterior distribution} over types. Call this the \textbf{state} of arm $i$ (i.e., the posterior distribution over types after getting information).\\
    \underline{State can be expected reward} when pulled is governed by state.
\end{definition}
The player has two controls: either play the process or not. If the player chooses to play the bandit process $i$, $i = 1,...,n$, the state of the bandit process $i$ evolves in a Markovian manner while the state of all other processes remains frozen (i.e., it does not change). Such a bandit process is called a Markovian bandit process.

Let $\{X_t^i\}$ denote the bandit process. The state $X_t^i\in \mathcal{X}^i$ of the bandit process $i$ at time $t$, where $\mathcal{X}^i$ is a finite (or countable) space.

Let $\vec{u}_t=(u_t^1,...,u_t^n)$ denote the decision
made by the player at time $t$. The component $u_t^i\in\{0,1\}$ is binary valued and denotes
whether the player chooses to play the bandit
process $i$ ($u_t^i = 1$) or not ($u_t^i = 0$). Since
the player may only choose to play one bandit process at each time, $u_t^i = 1$ must have only one nonzero component:
\begin{equation}
    \begin{aligned}
        \sum_{i=1}^n u_t^i = 1,\quad \forall t
    \end{aligned}
    \nonumber
\end{equation}
Let $\mathcal{U}\subset \{0,1\}^n$ denotes all vectors with this property, $\mathcal{U}=\{\vec{u}:u^i\in\{0,1\},i=1,...,n; \sum_{i=1}^n u^i = 1\}$.

The collection $\{\vec{X}_t=(X_t^1,...,X_t^n)\}_{t=0}^\infty$ evolves as follows: $\forall i=1,...,n$
\begin{equation}
    \begin{aligned}
        X_{t+1}^i=\left\{\begin{matrix}
            f^i(X_t^i,W_t^i),&\textnormal{if }u_t^i=1\\
            X_t^i&\textnormal{if }u_t^i=0
        \end{matrix}\right.
    \end{aligned}
    \nonumber
\end{equation}

When bandit process $i$ is played, it yields a reward $r^i_t$ and all other processes yield no reward.
The objective of a player is to choose a decision strategy $g=\{g_t\}_t=0^\infty$, where $g_t:\prod_{i=1}^n \mathcal{X}^i \rightarrow \mathcal{U}$, to maximize the expected total discounted
reward
\begin{equation}
    \begin{aligned}
        \mathbb{E}_g\left[\sum_{t=0}^\infty \beta^t\sum_{i=1}^nr^i_t u_t^i\mid \vec{X}_0=\vec{x}_0\right]
    \end{aligned}
    \nonumber
\end{equation}
where $\vec{x}_0=(x_0^1,...,x_0^n)$ is the initial starting state of all bandit processes.

\section{Gittins Index}
One possible solution concept for the MAB
problem is to set it up as a Markov decision process (MDP) and use Markov decision theory to solve it. However, such an approach does not scale well with the number of bandit processes because of the \textit{curse of dimensionality}.

Instead of solving the $n$-dimensional MDP with state-space $\prod_{i=1}^n \mathcal{X}^i$, an optimal solution is obtained by solving $n$ $1$-dimensional
optimization problems:
\begin{definition}[Gittins Index]
    For each bandit $i$,
    $i = 1,..., n$, and for each state $x^i\in \mathcal{X}^i$, compute
    \begin{equation}
        \begin{aligned}
            \mathcal{V}^i(x^i)=\max_{\tau>0}\frac{\mathbb{E}\left[\sum_{t=0}^\tau\beta^tr^i_t\mid X^i_0=x^i\right]}{\mathbb{E}\left[\sum_{t=0}^\tau\beta^t\mid X^i_0=x^i\right]}
        \end{aligned}
        \nonumber
    \end{equation}
    where $\tau$ is a measurable stopping time. The function $\mathcal{V}^i(x^i)$ is called \textbf{Gittins index} of bandit process $i$ at state $x^i$. The optimal $\tau$ is called the \underline{optimal stopping time at $x^i$}.

    The optimal strategy is "\underline{At each time, play the arm with the highest Gittins index.}"
\end{definition}
To implement the optimal strategy, we need compute and store the Gittins index
$\mathcal{V}^i(x^i)$ of all states $x^i\in \mathcal{X}^i$ of all bandit processes $i$, $i = 1, ... , n$.

\begin{claim}
    An equivalent interpretation of the Gittins index strategy is \underline{"Pick the arm with the highest Gittins index and play it until its optimal stopping time ( or equivalently, until it enters the corresponding stopping set) and repeat."}
\end{claim}



\end{document}