\contentsline {chapter}{\numberline {1}Field and Vector Space}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Field $(\mathbb {F},+,\cdot )$}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Definition of Field \small {(@ Lec 02 of ECON 204)}}{1}{subsection.1.1.1}%
\contentsline {section}{\numberline {1.2}Vector Space}{1}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Definition of Vector Space \small {(@ Lec 02 of ECON 204)}}{1}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Theorem: A field is a vector space over its subfield}{2}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Vector Subspace}{2}{subsection.1.2.3}%
\contentsline {subsection}{\numberline {1.2.4}Linear Independent}{2}{subsection.1.2.4}%
\contentsline {subsection}{\numberline {1.2.5}span V, basis, dimension}{2}{subsection.1.2.5}%
\contentsline {subsection}{\numberline {1.2.6}Dimension}{3}{subsection.1.2.6}%
\contentsline {subsection}{\numberline {1.2.7}Theorem: $|V|>\dim X \Rightarrow $ linearly dependent}{3}{subsection.1.2.7}%
\contentsline {subsection}{\numberline {1.2.8}Theorem: $|V|=n$: Linear Indep $\Leftrightarrow $ Spans $\Rightarrow $ Basis}{4}{subsection.1.2.8}%
\contentsline {subsection}{\numberline {1.2.9}Standard basis vectors}{4}{subsection.1.2.9}%
\contentsline {section}{\numberline {1.3}Linear Transformation}{4}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Definition of Linear Transformation}{4}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Composition of Linear Transformations is also Linear}{4}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}Function from a Basis extends uniquely to a Linear Transformation}{4}{subsection.1.3.3}%
\contentsline {subsection}{\numberline {1.3.4}Image, Kernel, Rank}{5}{subsection.1.3.4}%
\contentsline {subsection}{\numberline {1.3.5}The Rank-Nullity Theorem: $\dim X=\dim \textnormal {ker }T+\textnormal {Rank }T$}{5}{subsection.1.3.5}%
\contentsline {subsection}{\numberline {1.3.6}Theorem: Linear Transformation $T$ is 1-to-1 $\Leftrightarrow $ $\textnormal {ker }T = \{0\}$}{5}{subsection.1.3.6}%
\contentsline {subsection}{\numberline {1.3.7}Definition of Invertible Linear Transformation}{5}{subsection.1.3.7}%
\contentsline {subsection}{\numberline {1.3.8}Theorem: Inverse of a Linear Transformation is also Linear}{5}{subsection.1.3.8}%
\contentsline {subsection}{\numberline {1.3.9}Theorem: Linear Transformation is completely determined by values on basis}{6}{subsection.1.3.9}%
\contentsline {subsection}{\numberline {1.3.10}GL(V): set of invertible linear transformations $V \rightarrow V$}{6}{subsection.1.3.10}%
\contentsline {section}{\numberline {1.4}Isomorphism}{6}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Isomorphic: $\exists $ invertible $T \in L(X, Y)$}{6}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Theorem: Isomorphic $\Leftrightarrow $ $\dim X = \dim Y$}{6}{subsection.1.4.2}%
\contentsline {section}{\numberline {1.5}Quotient Vector Spaces}{7}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}Quotient Vector Space: $X/W$}{7}{subsection.1.5.1}%
\contentsline {subsection}{\numberline {1.5.2}Theorem: $\dim (X/W)=\dim X - \dim W$}{7}{subsection.1.5.2}%
\contentsline {subsection}{\numberline {1.5.3}Theorem: $T$ is isomorphic to $X/\textnormal {ker }T$}{7}{subsection.1.5.3}%
\contentsline {section}{\numberline {1.6}Coordinate Representation}{7}{section.1.6}%
\contentsline {section}{\numberline {1.7}Matrix Representation of a Linear Transformation}{8}{section.1.7}%
\contentsline {subsection}{\numberline {1.7.1}Definition: matrix corresponds to a linear transformation}{8}{subsection.1.7.1}%
\contentsline {subsection}{\numberline {1.7.2}Theorem: Matrix Space $\cong $ Linear Transformation Space}{9}{subsection.1.7.2}%
\contentsline {subsection}{\numberline {1.7.3}Theorem: $\textnormal {Mtx}_{W,V}(T)\cdot \textnormal {Mtx}_{V,U}(S)=\textnormal {Mtx}_{W,U}(T\circ S)$}{9}{subsection.1.7.3}%
\contentsline {chapter}{\numberline {2}Matrix Basics}{10}{chapter.2}%
\contentsline {section}{\numberline {2.1}Square Matrix $A_{n\times n}$: $det(A)$, singular}{10}{section.2.1}%
\contentsline {section}{\numberline {2.2}Orthogonal Vectors}{10}{section.2.2}%
\contentsline {section}{\numberline {2.3}Orthonormal Vectors}{10}{section.2.3}%
\contentsline {chapter}{\numberline {3}Eigenvalues Related}{11}{chapter.3}%
\contentsline {section}{\numberline {3.1}Eigenvalues, Eigenvectors Definition}{11}{section.3.1}%
\contentsline {section}{\numberline {3.2}Diagonalizable Matrix}{11}{section.3.2}%
\contentsline {section}{\numberline {3.3}Eigen Decomposition of Symmetric Matrices Results}{12}{section.3.3}%
\contentsline {section}{\numberline {3.4}Diagonalization of Real Symmetric Matrices}{12}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Proposition: $\lambda _{\min }\|x\|^2\leq x^TAx\leq \lambda _{\max }\|x\|^2$}{13}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Proposition: $\lambda ^2$ is the eigenvalue of $A^2$ and $A^TA$}{13}{subsection.3.4.2}%
\contentsline {section}{\numberline {3.5}Trace}{13}{section.3.5}%
\contentsline {section}{\numberline {3.6}Jacobian matrix}{14}{section.3.6}%
\contentsline {section}{\numberline {3.7}Hessian matrix}{14}{section.3.7}%
\contentsline {section}{\numberline {3.8}Positive Definite Matrices}{14}{section.3.8}%
\contentsline {subsection}{\numberline {3.8.1}Definition}{14}{subsection.3.8.1}%
\contentsline {subsection}{\numberline {3.8.2}Condition number (for PD matrix)}{15}{subsection.3.8.2}%
\contentsline {subsection}{\numberline {3.8.3}Diagonal matrix situation}{15}{subsection.3.8.3}%
\contentsline {subsection}{\numberline {3.8.4}Using eigenvalues}{16}{subsection.3.8.4}%
\contentsline {subsection}{\numberline {3.8.5}Sylvesterâ€™s Criterion}{16}{subsection.3.8.5}%
\contentsline {section}{\numberline {3.9}Matrix Norm (Induced Norm) and Spectral Radius}{17}{section.3.9}%
\contentsline {chapter}{\numberline {4}Euclidean geometry basics}{19}{chapter.4}%
\contentsline {section}{\numberline {4.1}Norm}{19}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Vector's Norm}{19}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Matrix's Norm}{19}{subsection.4.1.2}%
\contentsline {subsection}{\numberline {4.1.3}Difference between Spectral Radius and Spectral Norm}{20}{subsection.4.1.3}%
\contentsline {section}{\numberline {4.2} Euclidean distance, inner product}{20}{section.4.2}%
\contentsline {section}{\numberline {4.3}General Inner Products}{21}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1} Inner Product}{21}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Theorem: $*$ is inner product iff $\vec {x}*\vec {y}=\vec {x}^T H \vec {y}$ for some symmetric $H$}{21}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}Isometry}{22}{section.4.4}%
\contentsline {section}{\numberline {4.5} Linear isometries i.e. orthogonal group}{23}{section.4.5}%
\contentsline {section}{\numberline {4.6}Special orthogonal group}{23}{section.4.6}%
\contentsline {section}{\numberline {4.7}Translation}{23}{section.4.7}%
\contentsline {section}{\numberline {4.8}All isometries can be represented by a composition of \textit {a translation} and \textit {an orthogonal transformation}}{24}{section.4.8}%
\contentsline {chapter}{\numberline {5}Algebra Computation}{25}{chapter.5}%
\contentsline {section}{\numberline {5.1}Hessian Matrix}{25}{section.5.1}%
\contentsline {section}{\numberline {5.2}Taylor's Expansion}{25}{section.5.2}%
\contentsline {section}{\numberline {5.3} Random Vectors and Random Matrices}{25}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Mean}{25}{subsection.5.3.1}%
\contentsline {subsection}{\numberline {5.3.2}Variance, Covariance}{26}{subsection.5.3.2}%
\contentsline {section}{\numberline {5.4} Matrix Multiplication}{27}{section.5.4}%
\contentsline {section}{\numberline {5.5}Matrix Derivation}{27}{section.5.5}%
\contentsline {section}{\numberline {5.6}Matrix Inversion}{29}{section.5.6}%
\contentsline {subsection}{\numberline {5.6.1}Woodbury matrix identity}{29}{subsection.5.6.1}%
\contentsline {section}{\numberline {5.7}Linear Regression: Least Square}{29}{section.5.7}%
\contentsline {subsection}{\numberline {5.7.1}Normal Equations}{29}{subsection.5.7.1}%
\contentsline {section}{\numberline {5.8}LU Decomposition (Restricted to Square)}{30}{section.5.8}%
\contentsline {section}{\numberline {5.9}SVD: Singular Value Decomposition}{31}{section.5.9}%
\contentsline {subsection}{\numberline {5.9.1}Pseudo-inverse}{31}{subsection.5.9.1}%
\contentsline {subsection}{\numberline {5.9.2}Analysis of $A^TA$ and $AA^T$}{32}{subsection.5.9.2}%
\contentsline {subsection}{\numberline {5.9.3}Solve Normal Equations}{33}{subsection.5.9.3}%
\contentsline {subsection}{\numberline {5.9.4}Low-Rank Approximation}{33}{subsection.5.9.4}%
