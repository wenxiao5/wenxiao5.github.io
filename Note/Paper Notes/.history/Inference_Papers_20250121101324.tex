\chapter{Identification of Prediction Errors}
\section{\cite{rambachan2024identifying}: Identifying Prediction Mistakes in Observational Data}
Uncovering systematic prediction mistakes in empirical settings is challenging because
\begin{enumerate}
    \item the decision maker's preferences and
    \item the information set
\end{enumerate}
are unknown to us.

\subsection{Expected Utility Maximization at Accurate Beliefs}
A decision maker (DM) makes a binary choice $c\in\{0,1\}$ for each individual, which is summarized by characteristics $x\in \mathcal{X}$ and an unknown outcome $y^*\in \mathcal{Y}$ (observable when $c=1$).

\begin{example}[ (Pretrial Release)]
    A judge decides whether to detain ore release defendants $C\in\{0,1\}$. The outcome $Y^*\in\{0,1\}$ is whether a defendant would fail to appear in court if released. $X$ is the recorded information of the defendant.
\end{example}

\begin{example}[ (Medical Testing and Diagnosis)]
    $C\in\{0,1\}$ is whether to conduct a test. $Y^*\in\{0,1\}$ is whether the patient had a heart attack. $X$ is the recorded information of the patient.
\end{example}

\begin{example}[ (Hiring)]
    $C\in\{0,1\}$ is whether to hire a candidate. $Y^*$  is a vector of on-the-job productivity measures. $X$ is the recorded information of the candidate.
\end{example}


These three variables are summarized by a joint distribution, $(X,C,Y^*)\sim P(\cdot)$. We assume finite full support of $x$, i.e. there is a $\delta>0$ such that $P(x):=P(X=x)\geq \delta,\forall x\in \mathcal{X}$. As the $Y^*$ is only observable when $C=1$. We define
\begin{equation}
    \begin{aligned}
        Y:=C\cdot Y^*
    \end{aligned}
    \nonumber
\end{equation}
The observable data is the joint distribution $(X,C,Y)\sim P(\cdot)$. The DM's conditional choice probabilities are
\begin{equation}
    \begin{aligned}
        \pi_c(x):=P(C=c|X=x),c\in\{0,1\},x\in \mathcal{X}
    \end{aligned}
    \nonumber
\end{equation}
The observable conditional outcome probabilities are
\begin{equation}
    \begin{aligned}
        P_1(y^*\mid x):=P(Y^*=y^*\mid C=1, X=x),y^*\in \mathcal{Y},x\in \mathcal{X}
    \end{aligned}
    \nonumber
\end{equation}
The $P_0(y^*\mid x)$ and the true outcome probabilities $P(y^*\mid x)$ are not identified due to the missing-data problem.

\subsubsection*{Assumptions}
\begin{note}
    In the main context of paper: (i). The decision maker makes a binary choice $c\in\{0,1\}$ for each individual; (ii). The decision maker's choice does not have a direct causal effect on the outcome.
\end{note}

\begin{assumption}[Bounds on the Unobserved Conditional Outcome Probabilities]
    For each $x\in \mathcal{X}$, there exists a known subset $\mathcal{B}\subseteq \Delta \mathcal{Y}$ such that $P_0(\cdot\mid x)\in \mathcal{B}_x$.
\end{assumption}
Given this assumption, the identified set for the true outcome probabilities given $x\in \mathcal{X}$, denoted by
\begin{equation}
    \begin{aligned}
        \mathcal{H}(P(\cdot\mid x);\mathcal{B}_x):=\big\{\tilde{P}(\cdot\mid x)\in\Delta \mathcal{Y}: \tilde{P}(y^*\mid x)=\tilde{P}_0(y^*\mid x)\pi_0(x)+P_1(y^*\mid x)\pi_1(x),\\
        \forall y^*\in \mathcal{Y} \textnormal{ and for some } \tilde{P}_0(\cdot\mid x)\in \mathcal{B}_x\big\}
    \end{aligned}
    \nonumber
\end{equation}