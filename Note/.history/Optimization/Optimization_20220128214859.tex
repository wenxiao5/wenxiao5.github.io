%!TEX program = xelatex
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{ctex}
\usepackage{authblk}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{verbatim}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{indentfirst}
\usepackage{amssymb}
\setlength{\parindent}{0pt}
\usetikzlibrary{shapes,snakes}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}

\DeclareMathOperator{\col}{col}
\usepackage{booktabs}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{note}{Note}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{hyperref}
\newcommand{\code}{	exttt}
\geometry{a4paper,scale=0.8}
\title{Optimization}
\author[*]{Wenxiao Yang}
\affil[*]{Department of Mathematics, University of Illinois at Urbana-Champaign}
\date{2022}





\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Unconstrained Optimization}
\subsection{Conditions for Optimality}
Function: $f:\mathbb{R}^n \rightarrow	\mathbb{R}^n$, $x\in \&,\ \&\subseteq \mathbb{R}^n$.

Terminology: $x^*$ will always be the optimal input at some function.

\subsection{Global minimizer, Local minimizer}
\begin{definition}
    \quad\\
    Say $x^*$ is a \underline{global minimizer(minimum)} of $f$ if $f(x^*)\leq f(x), \forall x\in \&$.

    Say $x^*$ is a \underline{unique global minimizer(minimum)} of $f$ if $f(x^*)< f(x), \forall x\neq x^*$.

    Say $x^*$ is a \underline{local minimizer(minimum)} of $f$ if $\exists r>0$ so that $f(x^*)\leq f(x)$ when $\|x-x^*\|<r$.
\end{definition}

A minimizer is \underline{strict} if $f(x^*)< f(x)$ for all relevant $x$.

\subsection{Optimization in $\mathbb{R}$}
\subsubsection{Theorem 1: differentiable $f$, $x^*$ is a local minimizer $\Rightarrow f'(x^*)=0$}

\begin{theorem}
If $f(x)$ is differentiable function and $x^*$ is a local minimizer, then $f'(x^*)=0$.
\end{theorem}

\begin{proof}
\quad\\
Def of $f'(x)=\lim_{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}$\\
Def of local minimizer: $f(x^*)-f(x)\geq 0, |x^*-x|<r$\\
when $0<h<r$, $\frac{f(x+h)-f(x)}{h}\geq 0$; when $-r<h<0$, $\frac{f(x+h)-f(x)}{h}\leq 0$. Then $f'(x)=0$.
\end{proof}

\subsubsection{Theorem 2: $f'(x^*)=0, f''(x^*)\geq 0,\ \forall x\in [a,b]\Rightarrow x^*$ is a global minimizer on $[a,b]$; $f'(x^*)=0, f''(x^*)\geq 0 \Rightarrow x^*$ is a local minimizer}
\begin{theorem}
    If $f:\mathbb{R} \rightarrow \mathbb{R}$ is a function with a continuous second derivative and $x^*$ is a critical point of $f$ (i.e. $f'(x)=0$), then:\\
    (1): If $f''(x)\geq 0,\ \forall x\in\mathbb{R}$, then $x^*$ is a global minimizer on $\mathbb{R}$.\\
    (2): If $f''(x)\geq 0,\ \forall x\in[a,b]$, then $x^*$ is a global minimizer on $[a,b]$.\\
    (3): If we only know $f''(x^*)\geq 0$, $x^*$ is a local minimizer.
\end{theorem}
\begin{proof}[proof of theorem 2]
\quad\\
(1)$f(x)=f(x^*)+f'(x^*)(x-x^*)+\frac{1}{2}f''(\xi)(x-x^*)^2=f(x^*)+0+\textit{something non negative}\geq f(x^*)\  \forall x$\\
(2) Similar to (1)\\
(3)$f''(x^*)\geq 0,\ f''$ continuous $\Rightarrow \exists r$ s.t. $f''(x)\geq 0$ $\forall x\in[x^*-\frac{r}{2},x^*+\frac{r}{2}]$, then $x$ is a local minimizer.
\end{proof}


\subsection{Optimization in $\mathbb{R}^n$}
\subsubsection{Necessary Conditions for Optimality: Local Extremum $\Rightarrow \nabla f(x^*)=0$}
A base point $x$, we consider an arbitrary direction $u$. $\{x+tu| t\in \mathbb{R}\}$

For $\alpha>0$ sufficiently small:
\begin{enumerate}
    \item $f(x^*)\leq f(x^*+\alpha u)$
    \item $g(\alpha)=f(x^*+tu)-f(x^*)\geq 0$
    \item $g(\beta)$ is continuously differentiable for $\beta\in[0,\alpha]$
\end{enumerate}

By chain rule, $$g'(\beta)=\sum_{i=1}^n \frac{\partial f}{\partial x_i}(x^*+\beta u)u_i$$

By Mean Value Theorem, $$g(\alpha)=g(0)+g'(\beta)\alpha\text{ for some }\beta\in[0,\alpha]$$
Thus $$g(\alpha)=\alpha\sum_{i=1}^n \frac{\partial f}{\partial x_i}(x^*+\beta u)u_i\geq 0$$
$$\Rightarrow \sum_{i=1}^n \frac{\partial f}{\partial x_i}(x^*+\beta u)u_i\geq 0$$
Letting $\alpha \rightarrow	0$ and hence $\beta \rightarrow	0$, we get $$\sum_{i=1}^n \frac{\partial f}{\partial x_i}(x^*)u_i\geq 0\text{ for all }u\in \mathbb{R}^n$$
By choosing $u=[1,0,...,0]^T$, $u=[-1,0,...,0]^T$, we get $$\frac{\partial f(x^*)}{\partial x_1}\geq 0,\ \frac{\partial f(x^*)}{\partial x_1}\leq 0 \Rightarrow	\frac{\partial f(x^*)}{\partial x_1}= 0$$
Similarly, we can get $$\nabla f(x^*)=[\frac{\partial f(x^*)}{\partial x_1},\frac{\partial f(x^*)}{\partial x_2},...,\frac{\partial f(x^*)}{\partial x_n}]^T=0$$

\begin{theorem}
If $f$ is continuously differentiable and $x^*$ is a local extremum. Then $\nabla f(x^*)=0$.
\end{theorem}

\subsubsection{Stationary Point}
All points $x^*$ s.t. $\nabla f(x^*)=0$ are called \underline{stationary points}.

Thus, all extrema are stationary points.

But not all stationary points have to be extrema.

\begin{example}
$f(x)=x^3$, $x=0$ is a stationary point but not extrema.
\end{example}














































\section{MATH 484}
A base point $x$, we consider an arbitrary direction $u$. $\{x+tu| t\in \mathbb{R}\}$

We define \underline{the restriction of $f$ to the line through $x$ in the direction of $u$} to be the function:
$$\phi_u (t)=f(x+tu)$$
\begin{lemma}
$x^*$ is a global minimizer of $f$ iff for all $u$, $t=0$ is the global minimizer of $\phi_u (t)$
\end{lemma}
\begin{proof}
\quad\\
($\Rightarrow$) $\phi_u (0)=f(x^*)\leq f(x^*+tu)=\phi_u (t)$\\
($\Leftarrow$) Let $X\in \mathbb{R}^n$, $u=X-x^*$. $\phi_u (0)\leq \phi_u (1) \Rightarrow f(x^*)\leq f(x^*+u)=f(x)$
\end{proof}


\subsubsection{The first-derivative test in $\mathbb{R}^n$: $\phi'_u (t)=\nabla f(x+tu)\cdot u$}
First derivative of $f:\mathbb{R}^n \rightarrow \mathbb{R}$,
Easier: $\phi'_u (t)$?\\
Suppose $f:\mathbb{R}^n \rightarrow \mathbb{R}$ and $\mathsf{g}:\mathbb{R} \rightarrow \mathbb{R}^n$:
$$\frac{\partial f(\mathsf{g}(t))}{\partial t}=\sum_{i=1}^n \frac{\partial f}{\partial x_i}(\mathsf{g}(t))\frac{d}{dt}g_i(t)$$
$$\frac{\partial \phi_u (t)}{\partial t}=\sum_{i=1}^n \frac{\partial f}{\partial x_i}(x+tu)u_i$$

The gradient of $f$: $\nabla f(x)=(\frac{\partial f}{\partial x_1},..., \frac{\partial f}{\partial x_d})^T \Rightarrow \phi'_u (t)=\nabla f(x+tu)\cdot u$

\underline{Fine print}: Chain rule only works when all $\frac{\partial f}{\partial x_k}$ exists and are continuous.\\

\begin{example}
$f(x,y)=x^2+3xy-1,\ x^*=(0,0),\ u=(3,2)$\\
$\phi_u(t)=f(x^*+tu)=f(3t,2t)=27t^2-1$\\
$\phi_u'(t)=54t$\\
$\nabla f(x,y)=(2x+3y, 3x)$\\
$\phi'_u (t)=\nabla f(x+tu)\cdot u= 54t$
\end{example}

\subsubsection{Theorem 4: $\nabla f$ is continuous, $x^*$ is a global minimizer of $f$ $\Rightarrow \nabla f(x^*)=0$}
\begin{theorem}[Theorem 2.1]
    Given a function $f:\mathbb{R}^n \rightarrow \mathbb{R}$, if $\nabla f$ is continuous and $x^*$ is a global minimizer of
    $f$, then $\nabla f(x^*)=0$. (When $\nabla f(x^*)=0$, we call $x^*$ a \underline{critical point} of $f$.)
\end{theorem}
$x^*$ is a global minimizer $\Rightarrow$ $x^*$ is a critical point, inverse may not true.

\subsubsection{The second-derivative test in $\mathbb{R}^n$}
$$\phi'_u (t)=\sum_{i=1}^n \frac{\partial f}{\partial x_i}(x+tu)u_i$$
$$\phi^{''}_u (t)=\sum_{i=1}^n \sum_{j=1}^n u_iu_j\frac{\partial^2 f}{\partial x_i\partial x_j}(x+tu)$$
\subsubsection{Hessian matrix}
Define Hessian matrix of $f$ and write $Hf$. That is,

$$\phi^{''}_u (t)=u^T Hf(x+tu) u$$
\underline{Fine print}: Chain rule only works when all $\frac{\partial^2 f}{\partial x_i\partial x_j}$ exists and are continuous. ($\Rightarrow$ $Hf$ is continuous)\\

\subsubsection{Theorem 5: $Hf$ is continuous, $\nabla f(x^*)=0$, $u^T Hf(x^*) u\geq 0,\forall u$ $\Rightarrow$ $x^*$ is a global minimizer of $f$}
\begin{theorem}
    Given a function $f:\mathbb{R}^n \rightarrow \mathbb{R}$, if $Hf$ is continuous and $x^*$ is a critical point of
    $f$. If for any $u$, that $u^T Hf(x^*) u\geq 0$. Then $x^*$ is a global minimizer of $f$.
\end{theorem}
proved by Taylor
\begin{theorem}[Taylor]
    Given a function $f:\mathbb{R}^n \rightarrow \mathbb{R}$, if $Hf$ is continuous and $x^*$ is a critical point of $f$, then
    $$f(x)=f(x^*)=\nabla f(x^*)(x-x^*)+\frac{1}{2}(x-x^*)^T Hf(z) (x-x^*)$$
for some $z$ on the line between $x$ and $x^*$
\end{theorem}





\subsection{Minimizing over other sets}
What if the domain of $f$: $D\subset \mathbb{R}^n$\\
(1): want $x^*$ to be in the interior of $D$, not on the boundary (want to be able to "look" from $x^*$ in any direction.)\\
(2): want $x^*$ to "see" all other points in $D$ using straight line $u$.\\
Convexity\\
good domain e.g. Ball: $B(x^*,r)=\{x| ||x-x^*||<r \}$

\subsubsection{Theorem 7: $\nabla f$ is continuous, $x^*$(interior of $D$) is a local minimizer of $f$ $\Rightarrow \nabla f(x^*)=0$}
\begin{theorem}[Theorem 4.1, 类似Theorem 2.1]
    Suppose $f: D \rightarrow\mathbb{R}$ has continuous $\nabla f$ and $x^*$ is not on the boundary of $D$. If $x^*$ is a local minimizer of $f$, then $x^*$ is a critical point of $f$: $\nabla f(x^*)=0$
\end{theorem}

\subsubsection{Theorem 8: $Hf$ is continuous, $x^*$(interior of $D$) $\nabla f(x^*)=0$, $\exists r$ s.t. $u^T Hf(x^*) u\geq 0, \forall x\in B(x^*,r),\forall u$ $\Rightarrow$ $x^*$ is a local minimizer of $f$}
\begin{theorem}
    Given a function $f: D \rightarrow\mathbb{R}$ , if $Hf$ is continuous and $x^*$ is a critical point of
    $f$ in the interior of $D$. Suppose $\exists r$ s.t. for any $u$, that $u^T Hf(x^*) u\geq 0$ whenever $x\in B(x^*,r)\subset D$. Then $x^*$ is a local minimizer of $f$.
\end{theorem}




































\end{document}