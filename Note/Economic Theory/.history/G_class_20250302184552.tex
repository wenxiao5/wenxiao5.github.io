\documentclass[11pt]{elegantbook}
\usepackage{graphicx}
%\usepackage{float}
\definecolor{structurecolor}{RGB}{40,58,129}
\linespread{1.6}
\setlength{\footskip}{20pt}
\setlength{\parindent}{0pt}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\elegantnewtheorem{proof}{Proof}{}{Proof}
\elegantnewtheorem{claim}{Claim}{prostyle}{Claim}
\DeclareMathOperator{\col}{col}
\title{Ganesh Class}
\author{Wenxiao Yang}
\institute{Haas School of Business, University of California Berkeley}
\date{2025}
\setcounter{tocdepth}{2}
\extrainfo{All models are wrong, but some are useful.}

\cover{cover.png}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}


\bibliographystyle{apalike_three}

\begin{document}
\maketitle

\frontmatter
\tableofcontents

\mainmatter



\chapter{Self-Control/Addiction}
\section{Hyperbolic Discounting}
The classical discounting model discounts as $1,\delta,\delta^2,...$, i.e., $f(t)=\delta^t$ which discounts at a constant rate. However, the hyperbolic discounting model discounts faster in short run and slower in long run (i.e., we care more about now than future, and the difference of two future dates is small.)
\begin{equation}
    \begin{aligned}
        u_\textnormal{day1}\succ u_\textnormal{day2}\succ \cdots u_\textnormal{day365}\sim u_\textnormal{day366}
    \end{aligned}
    \nonumber
\end{equation}
The functional form can be given by
\begin{equation}
    \begin{aligned}
        f(t)=\left(1+\alpha t\right)^{-\frac{\gamma}{\alpha}}
    \end{aligned}
    \nonumber
\end{equation}
The discount rate is declining with time, $-\frac{f'(t)}{f(t)}=\frac{\gamma}{1+\alpha t}$.

\paragraph*{Quasi Hyperbolic Discounting}
A good approximation of the hyperbolic discounting model is
\begin{equation}
    \begin{aligned}
        U_t=u_t+\beta\left[\delta u_{t+1}+\delta^2 u_{t+2}+\cdots+\right]
    \end{aligned}
    \nonumber
\end{equation}
where $\beta$ is the uniformly discount rate for future utility.

\paragraph*{Dynamic Inconsistency} Preference hold at $t$ do not agree with preference hold at $t+1$.


\paragraph*{Application to Procrastination}
Cost Minimization Model:

Each period $t$ is consisted of three parts, 1. Pay cost $L$ because task is not done, 2. Observe current cost to do the task, $c\sim \textnormal{Unif}[0,1]$, 3. Choose to do task in $t$ or not.

Agent's decision follows a quasi hyperbolic discounting model, where we have $\beta<1$ and $\delta=1$. Agent has a ``sophisticated'' belief about her future behavior if $\hat{\beta}=\beta$ and has a ``naive'' belief about her future behavior if $\hat{\beta}=1$.

The optimal strategy is given by a critical cost $c^*$ such that the agent do the task if and only if $c\leq c^*$. Then, the expected un-discounted cost $v$ follows
\begin{equation}
    \begin{aligned}
        v=L+c^*\underbrace{\frac{c^*}{2}}_\textnormal{average cost conditional on $c\leq c^*$}+(1-c^*)v
    \end{aligned}
    \nonumber
\end{equation}
The sophisticated agent should be indifferent between doing the task and delaying:
\begin{equation}
    \begin{aligned}
        c^*=\beta v = \beta[L+c^*\frac{c^*}{2}+(1-c^*)v]
    \end{aligned}
    \nonumber
\end{equation}
which gives us
\begin{equation}
    \begin{aligned}
        c^*=\sqrt{\frac{L}{1/\beta-1/2}}
    \end{aligned}
    \nonumber
\end{equation}
which is increasing in $L$ and $\beta$.

The naive agent with $\hat{\beta}=1$ now using $c^*=\sqrt{\frac{L}{1/\hat{\beta}-1/2}}=\sqrt{2L}$ in the future. We have the critical cost at current time follows
\begin{equation}
    \begin{aligned}
        \tilde{c}=\beta[L+c^*\frac{c^*}{2}+(1-c^*)v]=\beta\sqrt{2L}
    \end{aligned}
    \nonumber
\end{equation}


\paragraph*{Doing it Now or Later (Matthew Rabin Lecture)}
A 2 hours effort today can save 10 mins every day in the future. The expected profit from doing the task today:
\begin{equation}
    \begin{aligned}
        U_t(today)=-120+\beta \delta 10 + \beta \delta^2 10 + \cdots=-120+\beta\frac{\delta}{1-\delta}10
    \end{aligned}
    \nonumber
\end{equation}
The expected profit from doing the task tomorrow with belief $\hat{\beta}$:
\begin{equation}
    \begin{aligned}
        U_t(tomorrow)=\beta\delta\left(-120+\hat{\beta}\frac{\delta}{1-\delta}10\right)
    \end{aligned}
    \nonumber
\end{equation}
With biased belief $\hat{\beta}>\beta$, there can exist infinite delay.



\section{\cite{fudenberg2006dual}: Dual-Self Model}
\begin{enumerate}
    \item $A$: The set of actions for the short-run selves (the set of probability measures is $\mathbf{A}$).
    \item $R$: The set of self-control actions for the long-run selves (the set of probability measures is $\mathbf{R}$). $0\in R$ is taken to mean that no self-control is used.
    \item $Y$: The set of states that encode the effects of history on current and future payoff possibilities (the set of probability measures is $\mathbf{Y}$).
    \item A finite history of play, $h_t\in H$, can consist of the past states and actions $h_t=(y_1,r_1,a_1,\cdots,y_t,r_t,a_t)$ or the null history $0$.
\end{enumerate}
The probability distribution over states at time $t+1$ depends on the time-$t$ state $y_t$ and action $a_t$, $\mu(y_t,a_t)$.















\bibliography{ref}

\end{document}