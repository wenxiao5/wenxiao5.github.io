\documentclass[11pt]{elegantbook}
\usepackage{graphicx}
%\usepackage{float}
\definecolor{structurecolor}{RGB}{40,58,129}
\linespread{1.6}
\setlength{\footskip}{20pt}
\setlength{\parindent}{0pt}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\elegantnewtheorem{proof}{Proof}{}{Proof}
\elegantnewtheorem{claim}{Claim}{prostyle}{Claim}
\DeclareMathOperator{\col}{col}
\title{Dynamic Programming and Bargaining}
\author{Wenxiao Yang}
\institute{Haas School of Business, University of California Berkeley}
\date{2024}
\setcounter{tocdepth}{2}
\extrainfo{All models are wrong, but some are useful.}

\cover{cover.png}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}


\bibliographystyle{apalike_three}

\begin{document}
\maketitle

\frontmatter
\tableofcontents

\mainmatter

\chapter{Dynamic Programming in Continuous Time}
This chapter is based on the notes of Jesús Fernández-Villaverde and Galo Nuño, Wikipedia, and the notes of Benjamin Moll.

\section{Optimal Control and Hamiltonian}
\subsection{Hamiltonian}
Consider a problem
\begin{equation}
    \begin{aligned}
        \max_{u(t)}\ &J=\int_{t_0}^{t_1}I(x(t),u(t),t)dt\\
        \textnormal{s.t. }&\dot{x}(t)=f(x(t),u(t),t)\\
    \end{aligned}
    \nonumber
\end{equation}
The solution method involves defining an ancillary function known as the control Hamiltonian
\begin{definition}[Hamiltonian]
    $\mathcal{H}(x(t),u(t),\lambda(t),t)\equiv I(x(t),u(t),t)+\lambda^T(t)f(x(t),u(t),t)$
\end{definition}

\begin{proposition}[Conditions for Maximum]\label{prop:Hamiltonian_FOC}
    The \textbf{first-order necessary conditions} for a maximum are given by
    \begin{enumerate}
        \item Maximum principle:
        \begin{equation}
            \begin{aligned}
                \mathcal{H}_u(x(t),u(t),\lambda(t),t)=0
            \end{aligned}
            \nonumber
        \end{equation}
        \item State transition function:
        \begin{equation}
            \begin{aligned}
                \dot{x}(t)=\mathcal{H}_\lambda(x(t),u(t),\lambda(t),t)=f(x(t),u(t),t)
            \end{aligned}
            \nonumber
        \end{equation}
        \item Costate equations:
        \begin{equation}
            \begin{aligned}
                \dot{\lambda}(t)=-\mathcal{H}_x(x(t),u(t),\lambda(t),t)=-[I_x(x(t),u(t),t)+\lambda^T(t)f_x(x(t),u(t),t)]
            \end{aligned}
            \nonumber
        \end{equation}
        with boundary condition for co-state variable(s) $\lambda(t)$, called
        “transversality condition”
        \begin{equation}
            \begin{aligned}
                \lambda(t_1)=0 \textnormal{ or }\lim_{t_1 \rightarrow \infty}\lambda(t_1)=0 \textnormal{ for infinite time horizons}
            \end{aligned}
            \nonumber
        \end{equation}
    \end{enumerate}
    A \textbf{sufficient condition} for a maximum is the concavity of the Hamiltonian evaluated at the solution, i.e.
    \begin{equation}
        \begin{aligned}
            \mathcal{H}_{uu}\left(x^*(t),u^*(t),\lambda(t),t\right)\leq 0
        \end{aligned}
        \nonumber
    \end{equation}
    where $u^*(t)$ is the optimal control and $x^*(t)$ is the resulting optimal trajectory for the state variable. Therefore, the necessary conditions are sufficient if the functions $I(x(t),u(t),t)$ and $f(x(t),u(t),t)$ are concave in both $x(t)$ and $u(t)$.
\end{proposition}
\begin{proof}[Derivation from the Lagrangian]
    The Lagrangian is
    \begin{equation}
        \begin{aligned}
            \mathcal{L}=\int_{t_0}^{t_1}\left(I(x(t),u(t),t)+\lambda^T(t)\left[f(x(t),u(t),t)-\dot{x}(t)\right]\right)dt
        \end{aligned}
        \nonumber
    \end{equation}
    In order to eliminate $\dot{x}(t)$, we can use integration by parts on the last term:
    \begin{equation}
        \begin{aligned}
            \int_{t_0}^{t_1}\lambda^T(t)\dot{x}(t)dt&=\int_{t_0}^{t_1}\lambda^T(t)dx(t)\\
            &=\lambda^T(t)x(t)\big|_{t_0}^{t_1}-\int_{t_0}^{t_1}\dot{\lambda}^T(t)x(t)dt\\
            &=\lambda^T(t_1)x(t_1)-\lambda^T(t_0)x(t_0)-\int_{t_0}^{t_1}\dot{\lambda}^T(t)x(t)dt\\
        \end{aligned}
        \nonumber
    \end{equation}
    which can be substituted back into the Lagrangian expression to give
    \begin{equation}
        \begin{aligned}
            \mathcal{L}=\int_{t_0}^{t_1}\left(I(x(t),u(t),t)+\lambda^T(t)f(x(t),u(t),t)+\dot{\lambda}^T(t)x(t)\right)dt-\lambda^T(t_1)x(t_1)+\lambda^T(t_0)x(t_0)
        \end{aligned}
        \nonumber
    \end{equation}
    In the form of Lagrangian functional, the first-order necessary condition is, for $t\in(t_0,t_1)$
    \begin{equation}
        \begin{aligned}
            \underbrace{I_u(x(t),u(t),t)+\lambda^T(t)f_u(x(t),u(t),t)}_{=\mathcal{H}_u(x(t),u(t),\lambda(t),t)}=0\\
            \underbrace{I_x(x(t),u(t),t)+\lambda^T(t)f_x(x(t),u(t),t)}_{=\mathcal{H}_x(x(t),u(t),\lambda(t),t)}+\dot{\lambda}(t)=0
        \end{aligned}
        \nonumber
    \end{equation}
    If both the initial value $x(t_0)$ and terminal value $x(t_1)$ are fixed, no conditions on $\lambda(t_0)$ and $\lambda(t_1)$ are needed. If the terminal value is free, as is often the case, the additional condition $\lambda(t_1)=0$ is necessary for optimality. The latter is called a transversality condition for a fixed horizon problem.
\end{proof}

\subsection{Optimal Control Problem}
\begin{definition}[Generic Deterministic Optimal Control Problem]
    The generic deterministic optimal control problem is
    \begin{equation}
        \begin{aligned}
            v(x_0)=\max_{\{\alpha(t)\}_{t\geq 0}}&\int_0^\infty e^{-\rho t}r(x(t),\alpha(t))dt\\
            \textnormal{s.t. }&\dot{x}(t)=f(x(t),\alpha(t))\\
            &x(0)=x_0
        \end{aligned}
        \nonumber
    \end{equation}
    \begin{note}
        $\frac{dx(t)}{dt}$ can be written as $\dot{x}(t)$.
    \end{note}
    Here, $x\in\mathbb{X}\subset \mathbb{R}^N$ is the \textbf{state} vector, $\alpha\in \mathbb{A}\subset \mathbb{R}^M$ is the \textbf{control} vector, $\rho>0$ is the \textbf{discount factor}, $f(\cdot):\mathbb{A}\times \mathbb{X} \rightarrow \mathbb{R}^N$ the \textbf{drift}, and $r(\cdot):\mathbb{A}\times \mathbb{X} \rightarrow \mathbb{R}$ is the instantaneous \textbf{reward} (utility).
\end{definition}
%By Lagrangian functional: $\mathcal{L}(x(t),\alpha(t),\lambda(t))=e^{-\rho t}r(x(t),\alpha(t))+\lambda (f(x(t),\alpha(t))-\dot{x}(t))$.
We can obtain conditions for an optimum using the Hamiltonian.
The current-value Hamiltonian is
    \begin{equation}
        \begin{aligned}
            \mathcal{H}(x,\alpha,\lambda)=r(x,\alpha)+\lambda f(x,\alpha)
        \end{aligned}
        \nonumber
    \end{equation}
where $\lambda\in \mathbb{R}^N$ is the ``co-state'' vector.
\begin{proposition}
    The first-order necessary conditions for an optimum are
    \begin{equation}
        \begin{aligned}
            &\mathcal{H}_\alpha(x(t),\alpha(t),\lambda(t))=0\\
            &\dot{\lambda}(t)=\rho\lambda(t)-\mathcal{H}_x(x(t),\alpha(t),\lambda(t))\\
            &\dot{x}(t)=f(x(t),\alpha(t))
        \end{aligned}
        \nonumber
    \end{equation}
    for all $t\geq 0$.
    The boundary condition (``transversality condition'') is
    \begin{equation}
        \begin{aligned}
            \lim_{T \rightarrow \infty}e^{-\rho T}\lambda(T)x(T)=0
        \end{aligned}
        \nonumber
    \end{equation}
\end{proposition}
\begin{proof}
    Let $\tilde{H}(x(t),\alpha(t),L(t))=e^{-\rho t}r(x(t),\alpha(t))+L(t)f(x(t),\alpha(t))$. By the Proposition \ref{prop:Hamiltonian_FOC}, the first-order necessary for the optimal $\alpha(t)$ is
    \begin{enumerate}
        \item $\tilde{H}_\alpha(x(t),\alpha(t),L(t))=0$:
        \begin{equation}
            \begin{aligned}
                \tilde{H}_\alpha(x(t),\alpha(t),L(t))=e^{-\rho t}r_\alpha(x(t),\alpha(t))+L(t)f_\alpha(x(t),\alpha(t))&=0\\
                \Leftrightarrow\quad r_\alpha(x(t),\alpha(t))+\underbrace{e^{\rho t}L(t)}_{:=\lambda(t)}f_\alpha(x(t),\alpha(t))&=0\\
                \Leftrightarrow\quad \mathcal{H}_\alpha(x(t),\alpha(t),\lambda(t))&=0
            \end{aligned}
            \nonumber
        \end{equation}
        \item $\dot{x}(t)=f(x(t),\alpha(t))$
        \item $\dot{L}(t)=-[e^{-\rho t}r_x(x(t),\alpha(t))+L(t)f_x(x(t),\alpha(t))]$: By substituting $L(t)=e^{-\rho t}\lambda(t)$, we have
        \begin{equation}
            \begin{aligned}
                &-\rho e^{-\rho t}\lambda(t) + e^{-\rho t}\dot{\lambda}(t)=-[e^{-\rho t}r_x(x(t),\alpha(t))+e^{-\rho t}\lambda(t)f_x(x(t),\alpha(t))]\\
                \Leftrightarrow\quad&-\rho\lambda(t) + \dot{\lambda}(t)=-\underbrace{[r_x(x(t),\alpha(t))+\lambda(t)f_x(x(t),\alpha(t))]}_{:=\mathcal{H}_x(x(t),\alpha(t),\lambda(t))}\\
                \Leftrightarrow\quad&\dot{\lambda}(t)=\rho\lambda(t)-\mathcal{H}_x(x(t),\alpha(t),\lambda(t))
            \end{aligned}
            \nonumber
        \end{equation}
    \end{enumerate}
\end{proof}

\section{Hamilton-Jacobi-Bellman Equation}
Consider the generic deterministic optimal control problem is
\begin{equation}
    \begin{aligned}
        v(x_0)=\max_{\{\alpha(t)\}_{t\geq 0}}&\int_0^\infty e^{-\rho t}r(x(t),\alpha(t))dt\\
        \textnormal{s.t. }&\dot{x}(t)=f(x(t),\alpha(t))\\
        &x(0)=x_0
    \end{aligned}
    \nonumber
\end{equation}
More generally, define the problem begins at $t$ with $x(t)=x$ as
\begin{equation}
    \begin{aligned}
        v(t,x)=\max_{\{\alpha(s)\}_{s\geq t}}&\int_t^\infty e^{-\rho(s-t)}r(x(s),\alpha(s))ds\\
        \textnormal{s.t. }&\dot{x}(t)=f(x(t),\alpha(t))\\
        &x(t)=x
    \end{aligned}
    \nonumber
\end{equation}
which may be written as $v_t(x)$.
\begin{note}
    $v_t(x)$'s value only depends on $x$, i.e., $v_{t_1}(x)=v_{t_2}(x),\forall t_1\neq t_2$.
\end{note}

\begin{proposition}[Hamilton-Jacobi-Bellman Equation]
    The value function of the generic optimal control problem satisfies the Hamilton-Jacobi-Bellman (HJB) equation
    \begin{equation}
        \begin{aligned}
            \rho v_t(x) = \underbrace{\frac{\partial v}{\partial t}}_{=0} + \max_{\alpha\in A}\left\{r(x,\alpha) + v_t'(x)\cdot f(x,\alpha)\right\}
        \end{aligned}
        \nonumber
    \end{equation}
    with a transversality condition $\lim_{T \rightarrow \infty}e^{-\rho T}v_T(x)=0$.\\
    ($v'(x)\in \mathbb{R}^N$ is the gradient of $v$ if $N>1$.)
\end{proposition}
\begin{proof}[Derivation from Discrete-time Bellman]
    By the discrete-time Bellman equation,
    \begin{equation}
        \begin{aligned}
            v(x(t))=\max_{\alpha(t)}\ \Delta r(x(t),\alpha(t)) + e^{-\rho\Delta}v(x(t+\Delta))
        \end{aligned}
        \nonumber
    \end{equation}
    where $x(t+\Delta)=x(t)+\Delta [f(x(t),\alpha(t))]$.\\
    For small $\Delta$ (will take $\Delta \rightarrow 0$), $e^{-\rho\Delta}=1-\rho\Delta$.
    \begin{equation}
        \begin{aligned}
            v(x(t))=\max_{\alpha(t)}\ \Delta r(x(t),\alpha(t)) + (1-\rho\Delta)v(x(t+\Delta))
        \end{aligned}
        \nonumber
    \end{equation}
    Subtract $(1-\rho)\Delta v(x(t))$ from both sides:
    \begin{equation}
        \begin{aligned}
            \rho \Delta v(x(t))=\max_{\alpha(t)}\ \Delta r(x(t),\alpha(t)) + (1-\rho\Delta)[v(x(t+\Delta))-v(x(t))]
        \end{aligned}
        \nonumber
    \end{equation}
    Divide by $\Delta$ and manipulate last term
    \begin{equation}
        \begin{aligned}
            \rho v(x(t))=\max_{\alpha(t)}\ r(x(t),\alpha(t)) + (1-\rho\Delta)\frac{v(x(t+\Delta))-v(x(t))}{x(t+\Delta)-x(t)}\frac{x(t+\Delta)-x(t)}{\Delta}
        \end{aligned}
        \nonumber
    \end{equation}
    Taking $\Delta \rightarrow 0$,
    \begin{equation}
        \begin{aligned}
            \rho v(x(t))=\max_{\alpha(t)}\ r(x(t),\alpha(t)) + v'(x(t))\dot{x}(t)
        \end{aligned}
        \nonumber
    \end{equation}
\end{proof}
\begin{note}
    The HJB can be written in the terms of the Hamiltonian,
    \begin{equation}
        \begin{aligned}
            \rho v(x) &= \max_{\alpha\in A} r(x,\alpha) + v'(x)\cdot f(x,\alpha)\\
            & = H(x,\alpha,v'(x))
        \end{aligned}
        \nonumber
    \end{equation}
\end{note}

\section{Brownian Motion and Diffusion Process}
\begin{definition}[Standard Brownian Motion]
    A standard Brownian motion (Wiener Process) is a stochastic process $W$ which satisfies
    \begin{equation}
        \begin{aligned}
            W(t+\Delta t)-W(t)=\epsilon_t\sqrt{\Delta t},\ \epsilon_t\sim \mathcal{N}(0,1),\ W(0)=0
        \end{aligned}
        \nonumber
    \end{equation}
    We can see $W(t)\sim \mathcal{N}(0,t)$.
\end{definition}
A more generalized Brownian motion can be defined as
\begin{definition}[Brownian Motion]
    A Brownian motion with drift $\mu$ and variance $\sigma^2$ is given as
    \begin{equation}
        \begin{aligned}
            x(t)=x(0)+ \mu t + \sigma W(t)
        \end{aligned}
        \nonumber
    \end{equation}
    where $W(t)$ is a Wiener Process.\\
    Since $W(t)\sim \mathcal{N}(0,t)$, we have $x(t)-x(0)\sim \mathcal{N}(\mu t,\sigma^2 t)$.
\end{definition}
Let $dW(t):=W(t+\Delta t)-W(t)=\epsilon_t\sqrt{\Delta t}$, where $\epsilon_t\sim \mathcal{N}(0,1)$. Then, we can write
\begin{equation}
    \begin{aligned}
        d x(t)= \mu dt + \sigma d W(t)
    \end{aligned}
    \nonumber
\end{equation}
which is called a \textbf{stochastic differential equation}.

\subsection{Diffusion Process}
\begin{definition}[Diffusion Process]
    The stochastic differential equation can be generalized further to the case that considering the dependence of $x$ on $\mu$ and $\sigma$:
    \begin{equation}
        \begin{aligned}
            dx=\mu(x)dt+\sigma(x)dW
        \end{aligned}
        \nonumber
    \end{equation}
    which is called a \textbf{diffusion process}. The $\mu(\cdot)$ is called the drift and the $\sigma(\cdot)$ is called the diffusion.
\end{definition}
(All results can be extended to the case where they depend on $t$,
$\mu(x, t), \sigma(x, t)$ but abstract from this for now.)
\begin{note}
    By choosing functions $\mu$ and $\sigma$, you can get pretty much any stochastic process you want (except jumps).
\end{note}

\section{Stochastic HJB Equations}
The generic problem of stochastic optimal control is
\begin{equation}
    \begin{aligned}
        v(x_0)=\max_{\{\alpha(t)\}_{t\geq 0}}&\mathbb{E}\int_0^{\infty}e^{-\rho t}r(x(t),\alpha(t))dt\\
        \textnormal{s.t. }& dx(t)=f(x(t),\alpha(t))dt+\sigma(x(t))dW(t)\\
        & \alpha(t)\in A,\forall t\geq 0,\ x(0)=x_0
    \end{aligned}
    \nonumber
\end{equation}
\begin{proposition}[HJB of Stochastic Optimal Control]
    The HJB equation is
    \begin{equation}
        \begin{aligned}
            \rho v(x) = \max_{\alpha\in A}\ r(x,\alpha) + v'(x)f(x,\alpha) + \frac{1}{2}v''(x)\sigma^2(x)
        \end{aligned}
        \nonumber
    \end{equation}
\end{proposition}
Multivariate Case:
\begin{equation}
    \begin{aligned}
        \rho v(x) = \max_{\alpha\in A}\ r(x,\alpha) + \sum_{i=1}^N \frac{\partial v(x)}{\partial x_i} f_i(x,\alpha) + \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \frac{\partial^2 v(x)}{\partial x_i \partial x_j} \sigma_{ij}^2(x)
    \end{aligned}
    \nonumber
\end{equation}
In vector notation:
\begin{equation}
    \begin{aligned}
        \rho v(x) = \max_{\alpha\in A}\ r(x,\alpha) + \nabla_x v(x) \cdot f(x,\alpha) + \frac{1}{2}\textnormal{tr}(\nabla^2v(x)\sigma^2(x))
    \end{aligned}
    \nonumber
\end{equation}
where $\nabla_x v(x)$ is the gradient of $v(x)$ and $\nabla^2v(x)$ is the Hessian of $v(x)$.

\subsection{Ito's Lemma}
Let $x$ be a scalar diffusion.
\begin{equation}
    \begin{aligned}
        dx=\mu(x)dt+\sigma(x)dW
    \end{aligned}
    \nonumber
\end{equation}
We are interested in the evolution of $y(t)=f(x(t))$ where $f$ is any twice differentiable function.
\begin{lemma}[Ito's Lemma]
    $y(t)=f(x(t))$ follows
    \begin{equation}
        \begin{aligned}
            d f(x)=\left(f'(x)\mu(x)+\frac{1}{2}f''(x)\sigma^2(x)\right)dt+f'(x)\sigma(x)dW
        \end{aligned}
        \nonumber
    \end{equation}
\end{lemma}
Multivariate Case:
\begin{equation}
    \begin{aligned}
        d f(x)=\left(\sum_{i=1}^N\frac{\partial f(x)}{\partial x_i}\mu_i(x)+\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \frac{\partial^2 f(x)}{\partial x_i \partial x_j}\sigma_{ij}^2(x)\right)dt+\sum_{i=1}^N\frac{\partial f(x)}{\partial x_i}\sigma_i(x)dW_i
    \end{aligned}
    \nonumber
\end{equation}
In vector notation:
\begin{equation}
    \begin{aligned}
        d f(x) = \left(\nabla_x f(x)\cdot\mu(x)+\frac{1}{2}\textnormal{tr}(\nabla^2f(x)\sigma^2(x))\right)dt+\nabla_x f(x)\cdot\sigma(x)dW
    \end{aligned}
    \nonumber
\end{equation}
where $\nabla_x f(x)$ is the gradient of $f(x)$ and $\nabla^2f(x)$ is the Hessian of $f(x)$.
\begin{note}
    It is extremely powerful because it says that any (twice differentiable) function of a diffusion is also a diffusion.
\end{note}

\subsection{Kolmogorov Forward Equations}
Let $x$ be a scalar diffusion.
\begin{equation}
    \begin{aligned}
        dx=\mu(x)dt+\sigma(x)dW,\quad x(0)=x_0
    \end{aligned}
    \nonumber
\end{equation}
We are interested in the evolution of the \textit{distribution} of $x$, $g(x,t)$, and in particular in the stationary distribution $g(x)$. Natural thing to care about especially in heterogenous agent models.
\begin{proposition}[Kolmogorov Forward Equation]
    Given an initial distribution $g(x,0) = g_0(x)$, $g(x,t)$ satisfies the PDE
    \begin{equation}
        \begin{aligned}
            \frac{\partial g(x,t)}{\partial t}=-\frac{\partial [\mu(x)g(x,t)]}{\partial x}+\frac{1}{2}\frac{\partial^2 [\sigma^2(x)g(x,t)]}{\partial x^2}
        \end{aligned}
        \nonumber
    \end{equation}
\end{proposition}

\begin{corollary}
    If a stationary distribution $g(x)$ exists, it satisfies the ODE
    \begin{equation}
        \begin{aligned}
            0=\frac{\partial g(x,t)}{\partial t}=-\frac{\partial [\mu(x)g(x,t)]}{\partial x}+\frac{1}{2}\frac{\partial^2 [\sigma^2(x)g(x,t)]}{\partial x^2}
        \end{aligned}
        \nonumber
    \end{equation}
\end{corollary}
Multivariate Case:
\begin{equation}
    \begin{aligned}
        \frac{\partial g(x,t)}{\partial t}=-\sum_{i=1}^N\frac{\partial [\mu_i(x)g(x,t)]}{\partial x_i}+\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \frac{\partial^2 [\sigma_{ij}^2(x)g(x,t)]}{\partial x^2}
    \end{aligned}
    \nonumber
\end{equation}





\chapter{\cite{fuchs2022dynamic}: Dynamic Bargaining with Private Information}

\section{``Classic'' Coase Conjecture}
Consider a seller facing a buyer has private value of the good $v\in[1,2]$ that is distributed according to an atomless distribution with full support, $F(v)$. The seller has cost $c\geq 0$ to serve the buyer. Every period of an infinite horizon game, the uninformed seller makes an offer $p_t$. If $p_t$ is accepted, the game ends with payoffs $v-p_t$ and $p_t-c$ for the buyer and the seller, respectively. If $p_t$ is rejected, the seller makes another offer at time $t+\Delta$. Both buyer and seller discount future with the rate $r$. Thus, $\Delta$ can be thought of as the commitment power of the seller.

Since higher-value buyers lose more from delay, the Skimming Property holds in any equilibrium.
\begin{lemma}[Skimming Property]
    If type $v$ is willing to accept an offer $p_t$, then all higher types strictly prefer to accept this offer.
\end{lemma}
Denote by $k_t$ the highest type remaining before the offer at time $t$ is made. Then, the seller beliefs at time $t$ are $v\in[1,k_t]$ with truncated distribution $F(v)/F(k_t)$, where $k_0=2$.


A distinction arises in this game: (1). $c<1$ (the gap case); (2). $c=1$ (the no-gap case).

As shown by \cite{fudenberg1985infinite} and \cite{gul1986foundations}, the game must end in a finite number of rounds (uniformly bounded for all $\Delta$). The intuition is: the difference between distinguishing $k_t$ and an immediate trading $1$ is $k_t-1$. If the $k_t-1$ is relative small to $1-c$. The seller prefers an immediate trade rather than keep bargaining. So, there is a critical $k_t$ that is close enough to $1$ and the seller offers $p_t=1$ to get an immediate trade.

By the backward induction, equilibrium must be unique (up to seller's randomization at time $0$). These equilibria have the property that the seller value depends only on the state of the game, $k_t$, a property called ``stationarity'' that is similar to the equilibria being Markov.

Stationary equilibria continue to exist when $c = 1$, but other nonstationary equilibrium can be constructed by \cite{ausubel1989reputation}.

Formally, the Coase conjecture (\cite{coase1972durability}) states that
\begin{proposition}[Coase Conjecture]
    As $\Delta \rightarrow 0$ (i.e., the seller loses all ability to commit to prices), prices fall to the lowest buyer valuation, $p_0 \rightarrow 1$, and there is no inefficient delay.
\end{proposition}


\section{Exogenously Interdependent Values}
Suppose the cost of selling to type $v$ is an increasing function of the buyer's type $c(v)$ with $c'(v)$. We assume that immediate trade is strictly efficient for all $v\in (1,2]$, $c(v)<v$.

The equilibrium dynamics depend on the degree of adverse selection.
\begin{enumerate}
    \item If there is little adverse selection, i.e., $\mathbb{E}[c(v)]\leq 1$, the continuous-time limit of the unique stationary equilibrium has immediate trade (\cite{deneckere2006bargaining}).
    \item If there is more adverse selection, $\mathbb{E}[c(v)]>1$, the seller would prefer not to trade at all rather than offering $p_0=1$ and trading with all types, so trade cannot be efficient in equilibrium.\\
    No trade cannot be an equilibrium either:
    \begin{enumerate}
        \item For no trade to be an equilibrium, prices would always have to be above $2$. If the price were ever lower, a mass of buyers $(2-\epsilon,2]$ would accept this price, contradicting to no trade.
        \item If the price were always above $2$, there would be a profitable deviation for the seller. The seller could offer $p_0 = c(2) < 2$ and then offer a sequence of prices that would lead to no losses.
    \end{enumerate}
    What then do equilibria look like?
\end{enumerate}
Suppose the continuous-time limit equilibrium strategy of the buyer is characterized by a continuous downward-sloping demand function, $P(v)$. Then, the seller's best response problem is to choose the speed to maximize her expected value given $k_t$:
\begin{equation}
    \begin{aligned}
        r V(k_t) = \max_{\dot{k}_t\in[0,\infty]}\left(P(k_t)-c(k_t)-V(k_t)\right)(-\dot{k}_t)\frac{f(k_t)}{F(k_t)}+V'(k_t)\dot{k}_t
    \end{aligned}
    \nonumber
\end{equation}
When trade happens, the seller collects $P(k_t)-c(k_t)$ and the game ends. Trade happens with a probability flow






















































































\bibliography{ref}




\end{document}