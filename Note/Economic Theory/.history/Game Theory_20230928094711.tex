\documentclass[11pt]{elegantbook}
\definecolor{structurecolor}{RGB}{40,58,129}
\linespread{1.6}
\setlength{\footskip}{20pt}
\setlength{\parindent}{0pt}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\elegantnewtheorem{proof}{Proof}{}{Proof}
\elegantnewtheorem{claim}{Claim}{prostyle}{Claim}
\DeclareMathOperator{\col}{col}
\title{\textbf{Economic Theory and Some Useful Math}}
\author{Wenxiao Yang}
\institute{Haas School of Business, University of California Berkeley}
\date{2023}
\setcounter{tocdepth}{2}
\cover{cover.png}
\extrainfo{All models are wrong, but some are useful.}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{9,119,119}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}

\addbibresource[location=local]{reference.bib} % bib

\begin{document}

\maketitle
\frontmatter
\tableofcontents
\mainmatter



\chapter{Economic Theory Foundation}
Based on
\begin{enumerate}[$\circ$]
    \item Mas-Colell, Whinston, and Green, Microeconomic Theory, Oxford University Press (1995).
    \item UIUC ECON 530 21Fall, Nolan H. Miller
    \item UC Berkeley ECON 201A 23Fall
    \item UC Berkeley MATH 272 23Fall, Alexander Teytelboym
    \item  Jehle, G., Reny, P.: Advanced Microeconomic Theory . Pearson, 3rd ed. (2011). Ch. 6.
    \item Notes on Social Choice and Welfare, Alejandro Saporiti
    \item Yu, N. N. (2012). A one-shot proof of Arrow's impossibility theorem. \textit{Economic Theory}, 523-525.
\end{enumerate}
\section{Preferences and Utility Functions}

\subsection{Preference Relation}
\begin{definition}[Weak, Strict, Indifference]
    \normalfont
    $\succeq$ referred to as the \textbf{weak preference relation}: "$x$ is at least as good as $y$". (ordinal);\\
    "\textbf{No better than}": $y \preceq x$ if and only if $x \succeq y$.\\
    "\textbf{Strict preference}": $x \succ y$ if and only if $x \succeq y$ and not $y \succeq x$.\\
    "\textbf{Indifference}": $x \sim y$ if and only if $x \succeq y$ and $y \succeq x$.
\end{definition}

\subsection{Rational Preference}
\begin{definition}[Complete Preference]
    \normalfont
    \textbf{Completeness}: $\succeq$ is complete iff $\forall x,y\in X$, $x \succeq y$ or $y \succeq x$.
\end{definition}
The completeness means
\begin{enumerate}[-]
    \item Any two bundles can be compared
    \item Indifference is allowed
\end{enumerate}

\begin{definition}[Transitive Preference]
    \normalfont
    \textbf{Transitivity}: $\succeq$ is transitive iff $\forall x, y, z \in X$, if $x \succeq y$ and $y \succeq z$, then $x \succeq z$
\end{definition}
The transitivity
\begin{enumerate}[-]
    \item like transitivity of the real numbers
    \item extends pairwise preferences to longer chains in the logical way.
\end{enumerate}

\begin{definition}[Rantional Preference]
    \normalfont
    {Rationality}: $\succeq$ is \textbf{rational} \underline{if and only if} it is \textbf{complete} and \textbf{transitive}.
\end{definition}

\subsection{Utility Function $\Leftrightarrow$ Rational Preference}
\begin{definition}[Unitility Function]
    \normalfont
    We can say a function $u: X \rightarrow \mathbb{R}$ represents $\succeq$ if $\forall x,y\in X$, $$x\succeq y \Leftrightarrow u(x)\geq u(y)$$
\end{definition}

\begin{proposition}[Rational $\succeq$ $\Rightarrow$ $\exists u(\cdot)$]
    If $\exists$ a function $u: X \rightarrow \mathbb{R}$ represents $\succeq$, then $\succeq$ is rational (i.e., completeness and transitivity)
\end{proposition}
\begin{note}
    The reverse may not true.
\end{note}

Let $\mathcal{B}=2^X$ (all subsets of $X$) and $B\in \mathcal{B}$ be the all potential alternatives that can be chosen.

The choice of an agent can be represented by $C(B)\subseteq B, \forall B\in \mathcal{B}$.

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{Pref_prop.png}
    \caption{Properties of Preference and Utility Function}
    \label{}
\end{figure}\end{center}


\subsection{Monotone Preference}
\begin{definition}[Monotone $\succeq$]
    \normalfont
    $\succeq$ is \textbf{monotone} if $x,y\in X$ with $x\geq y\Rightarrow x\succeq y$ (and $x> y\Rightarrow x\succ y$).
\end{definition}

\begin{proposition}[Monotone $\succeq$ $\Rightarrow$ monotone $u(\cdot)$]
    If $\succeq$ is monotone, then $\exists$ a monotone $u(\cdot)$ that represents $\succeq$.
\end{proposition}

\begin{note}
    Complete, transitive, and monotone are three assumptions that made by all theories (either EU or non-EU).
\end{note}

\begin{definition}[Strongly Monotone $\succeq$]
    \normalfont
    $\succeq$ is \textbf{strongly monotone} if and only if for any $x=(x_1,...,x_n), y=(y_1,...,y_n)\in X$, if $\forall i: x_{i} \geq y_{i}$ and $\exists j$ such that $x_{j}>y_{j}$, then $x \succ y$.
\end{definition}
(When we compare elements that have more than one dimension, strongly monotone holds if at least one relation is not equal.)
$$
A=(1,1), B=(2,1), C(1,2), D=(2,2)
$$
Strongly monotone can infer that $D \succ B \succ A, D \succ C \succ A$.

Even weaker assumptions will ensure that the consumer's choice exhausts their budget.
\begin{definition}[Local Nonsatiation]
    \normalfont
    For any bundle $x$, there is a nearby bundle $y$ in the consumption set such that $y$ is preferred to $x$. That is, for all $x\in X$ and every $\varepsilon>0$,
    $$
    \exists y \in|x-y|<\varepsilon, \text { s.t. } y \succ x
    $$
\end{definition}

We have
\begin{center}
    Strong Monotonicity $\Rightarrow$ Monotonicity $\Rightarrow$ Local Nonsatiation
\end{center}

\subsection{Independence of Preference}
The 'standard' model of decisions under risk is based on von Neumann and Morgenstern Expected Utility (EU), which requires the independence assumption.
\begin{definition}[Independence of Preference]
    \normalfont
    \textbf{Independence}: For any $x,y,z\in X$ and $0<\alpha<1$, if $x\succeq y$ then $\alpha x+(1-\alpha)z \succeq \alpha y+(1-\alpha)z$.
\end{definition}



\subsection{Continuous Preference}
\begin{definition}[Continuous $\succeq$]
    \normalfont
    $\succeq$ is \textbf{continuous} on $X$ \underline{if and only if} for any sequence $\{x^n,y^n\}_n=1^\infty$ with $x^n\succeq y^n$ and we note $x=\lim_{n \rightarrow \infty}x^n$, $y=\lim_{n \rightarrow \infty}y^n$, we have $x\succeq y$ (i.e., the graph $\{(x,y)\mid x\succeq y\subseteq X\times X\}$ is closed).
\end{definition}
\begin{example}[ Lexicographic preferences (not continuous)]
    Under Lexicographic preference $\succ$, $x \succ y$ \underline{if and only if}
    \begin{enumerate}[$\circ$]
        \item $x_{1}>y_{1}$, or
        \item $x_{1}=y_{1}$, and $x_{2}>y_{2}$, or
        \item $x_{1}=y_{1}$ and $x_{2}=y_{2}$ and $x_{3}>y_{3}$, or
        \item etc.
    \end{enumerate}
    Under Lexicographic preferences, there is no indifference.
    
    We can find the Lexicographic preference violates continuity: $\left(1+\frac{1}{n}, 1\right) \succ(1,2)$ and $\lim \left(1+\frac{1}{n}, 1\right)=(1,1) \prec(1,2)$.
\end{example}

\begin{proposition}[Debreu's Theorem, Continuous $\succeq$ $\Rightarrow$ continuous $u(\cdot)$]
    If $\succeq$ is continuous, then $\exists$ a continuous $u(\cdot)$ that represents $\succeq$.
\end{proposition}

\subsection{Convex Preference}
\begin{definition}[Convex $\succeq$]
    \normalfont
    $\succeq$ is \textbf{convex} if for every $x\in X$ the $\{y\in X: y\succeq x\}$ is convex, i.e., $y_1\succeq x$ and $y_2\succeq x$ $\Rightarrow$ $\alpha y_1+ (1-\alpha) y_2\succeq x$ for all $\alpha\in[0,1]$.
\end{definition}
Convex relations imply \textit{averages are preferred to extremes}.

\begin{definition}[Strictly Convex]
    \normalfont
    $\succeq$ is \textbf{strictly convex} iff $\forall x, y, z \in X$, if $x \succeq z$ and $y \succeq z$, then $\alpha x+(1-\alpha) y \succ z$ for all $\alpha\in (0,1)$
\end{definition}

\begin{definition}[Quasi-Concave Function]
    \normalfont
    A function $u$ is \textbf{quasi-concave} if and only if for all $y\in X$, $\{x\in X: u(x)\geq u(y)\}$ is convex. Any function that is concave is also quasi-concave.
\end{definition}
\begin{proposition}[Concave Function $\Rightarrow$ Quasi-Concave Function]
    Any function that is concave is also quasi-concave.
\end{proposition}


\begin{proposition}[Convex $\succeq$ $\Leftrightarrow$ quasi-concave $u(\cdot)$]
    $\succeq$ is convex, $\Leftrightarrow$ $\exists$ a \underline{quasi-concave} $u(\cdot)$ that represents $\succeq$.
\end{proposition}


\subsection{Homothetic Preference}
\begin{definition}[Homotheticity]
    \normalfont
    $\succeq$ are homothetic if $x\succeq y \Rightarrow \alpha x\succeq \alpha y$ for all $\alpha>0$.
\end{definition}

\begin{proposition}[Homothetic preference $\Leftrightarrow$ homogeneous $u(\cdot)$]
    A continuous $\succeq$ is homothetic $\Leftrightarrow$ $\exists$ a continuous homogeneous $u(\cdot)$ that represents $\succeq$ such that $u(\alpha x)=\alpha u(x)$ for all $x>0$.
\end{proposition}


\subsection{Quasi-linearity}
\begin{definition}[Quasi-Linearity]
    \normalfont
    $\succeq$ on $X$ is \textbf{quasi-linear} on $x_1$ if $$x\succeq y \Rightarrow (x+\epsilon e_1)\succeq (y+\epsilon e_1)$$ where $e_1=(1,0,...,0)$ and $\epsilon>0$.
\end{definition}

\begin{theorem}[Quasi-Linearity $\Leftrightarrow$ $u(x)=x_1+v(x_{-1})$]
    A continuous $\succeq$ on $(-\infty,\infty)\times \mathbb{R}_+^{K-1}$ is quasi-linear in $x_1$ $\Leftrightarrow$ $\exists$ a $u(\cdot)$ that represents $\succeq$ such that $$u(x)=x_1+v(x_{-1})$$
    where $v(\cdot)$ satisfies $(v(x_{-1}),0,...,0)\sim(0,x_{-1})$.
\end{theorem}

\subsection{Separability}
\begin{definition}[Separability]
    \normalfont
    $\succeq$ satisfies \textbf{separability} if for any $x_i$
    \begin{equation}
        \begin{aligned}
            (x_i, x_{-i})\succeq (x'_i, x_{-i}) \Leftrightarrow (x_i, x'_{-i})\succeq (x'_i, x'_{-i})
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}

\begin{theorem}[Separability $\Rightarrow$ Additive $u(\cdot)$]
    $\succeq$ with \textbf{separability} admits additive $u$-representation
    \begin{equation}
        \begin{aligned}
            u(x)=v_1(x_1)+\cdots v_K(x_K)
        \end{aligned}
        \nonumber
    \end{equation}
\end{theorem}

\begin{note}
    Strong assumption, usually ignored in practice.
\end{note}


\subsection{Differentiable Preference}
Consider a vector of values $v(x)\in \mathbb{R}^K_+$ for the $K$ commodities and a feasible the direction $x+\varepsilon d\in X$ from $x$ for small enough $\varepsilon>0$.

$d$ is considered \underline{improvement} if and only if $$d\cdot v(x)>0$$

Given $v(x): X \rightarrow \mathbb{R}^K_+$, let $$D_v(x)=\{d:d\cdot v(x)>0\}$$
be the set of directions that are improvements relative to $x$.

$d\in \mathbb{R}^k$ is an \underline{improvement direction} at $x$ if there is $\lambda^*>0$ such that $\lambda d$ is an improvement $$x+\lambda d\succ x$$
for any $\lambda\leq \lambda^*$. Let $D_{\succeq}(x)$ be the set of all improvement directions at $x$.

Any improvement is an improvement direction if
\begin{enumerate}[-]
    \item $\succeq$ are strictly convex.
    \item $\succeq$ are convex, strongly monotonic, and continuous.
\end{enumerate}

\begin{definition}[Differentiable Preference]
    \normalfont
    $\succeq$ is \textbf{differentiable} if there exists a function $v(x): X \rightarrow \mathbb{R}_+^K$ such that $$D_{\succeq}(x)=D_v(x),\ \forall x\in X$$
\end{definition}

\begin{example}
    $\succeq$ represented by
    \begin{enumerate}[(1).]
        \item $\alpha x_1+\beta x_2$ for $\alpha,\beta>0$ are differentiable: $v(x)=(\alpha,\beta)$.
        \item $\min\{\alpha x_1,\beta x_2\}$ are differentiable where $\alpha x_1\neq \beta x_2$: $v(x)=\left\{\begin{matrix}
            (1,0)& \textnormal{ if }\alpha x_1<\beta x_2\\
            (0,1)& \textnormal{ otherwise}
        \end{matrix}\right.$
    \end{enumerate}
\end{example}

\begin{proposition}[Sufficient condition for differentiable $\succeq$]
    Any (monotonic and convex) $\succeq$ can be represented by a (strongly monotonic and quasi-concave) and differentiable $u$ is differentiable.
\end{proposition}

\section{Choice}
\subsection{Choice}
\begin{definition}
    \normalfont
    A \textbf{choice function} $C$ such that $C(A)\subseteq A$ which specifies for each nonempty subset $A\subseteq X$.
\end{definition}
A choice function $C$ can be rationalized if there is a preference relation $\succeq$ on $X$ such that $C=C_\succeq$, that is $$C(A)=C_\succeq(A)=\{x\in A:x\succeq y, \forall y\in A\}, \forall A\subseteq X$$

\begin{definition}[Rubinstein's Condition $\alpha$]
    \normalfont
    A choice function $C$ satisfies \textbf{condition $\alpha$} if for any two problems $A,B$, if $A\subseteq B$ and $C(B)\in A$, then $C(A)=C(B)$.
\end{definition}
%Condition $\alpha$ is sufficient for $C$ is being formulated by maximizing a preference $\succeq$.

\subsection{Choice Correspondence}
\begin{definition}[Choice Correspondence (More than one choice)]
    \normalfont
    A choice correspondence $C$ assigns a non-empty \underline{subset} for every non-empty set $A$
    $$\emptyset\neq C(A)\subseteq A$$
\end{definition}
\textbf{Properties:}
\begin{enumerate}
    \item[($\alpha$):] If $a\in A\subseteq B$, then $a\in C(B) \Rightarrow a\in C(A)$
    \item[($\beta$):] If $a,b\in A\subseteq B$, then $a,b\in C(A)$ and $b\in c(B) \Rightarrow a\in C(B)$.
\end{enumerate}
$\alpha$ and $\beta$ are equivalent to WARP.


\subsection{Weak Axiom of Revealed Preference (WARP)}
\begin{definition}[Weak Axiom of Revealed Preference]
    \normalfont
    Given a choice structure $(C,\mathcal{B})$ satisfies \textbf{WARP}. If $\exists B\in \mathcal{B}$ with $x,y\in B$, such that $x\in C(B)$. Then, $\forall B'\in \mathcal{B}$ with $x,y\in B'$, $y\in C(B') \Rightarrow x\in C(B')$.\\
    Or we can say, whenever $x,y\in B\cap B'$, $$x\in C(B) \textnormal{ and }y\in C(B)' \Rightarrow x\in C(B')$$
\end{definition}

\begin{proposition}[Rational $\Rightarrow$ WARP]
    Given $\succeq$ is rational, then $(C^*_{\succeq},\mathcal{B})$ satisfies WARP.\\
    ($C^*_{\succeq}$ is the choice rule that picks the maximal alternatives by $\succeq$)
\end{proposition}




\section{Social Choice}
Notations:
\begin{enumerate}
    \item We consider \underline{finite} set of alternatives $X$ and \underline{finite} set of agents $I$.
    \item We use $\mathcal{B}$ to denotes the set of all preference relations.
    \item We use $\mathcal{R}\subseteq \mathcal{B}$ to denotes the set of all rational preference relations.
    \item We use $\succeq\in \mathcal{R}$ to represents individual rational preference relation.
\end{enumerate}

\subsection{Social Welfare Function and Properties}
\begin{definition}[Social Welfare Function (SWF)]
    \normalfont
    A \textbf{social welfare function} (SWF) is a mapping $$f: \mathcal{A}\subseteq \mathcal{R}^I\rightarrow \mathcal{B}$$
    $\trianglerighteq=f(\succeq_1,...,\succeq_I)$ is interpreted as the \textbf{social preference relation}. It doesn't need to be rational (i.e., complete and transitive).
\end{definition}

\begin{definition}[SWF's Properties]\label{SWF_properties}
    \normalfont
    A social welfare function $f: \mathcal{A}\rightarrow \mathcal{B}$
    \begin{enumerate}[$\circ$]
        \item has \textbf{unrestricted domain} (UD) if $\mathcal{A}=\mathcal{R}^n$;
        \item is \textbf{transitive} (T) if $f(\succeq_1,...,\succeq_I)$ is transitive for all $(\succeq_1,...,\succeq_I)\in \mathcal{A}$;
        \item is \textbf{nondictatorial} (ND) if there is no agent $i\in I$ such that $\forall \{x,y\}\subseteq X$ $x\succeq_i y \Rightarrow x\trianglerighteq y$. (That is there is no distinguished voter who can choose the winner).
        \item is \textbf{weakly Paretian} (PA) if, $\forall \{x,y\}\subseteq X$ and any preference profile $(\succeq_1,...,\succeq_I)\in \mathcal{A}$, we have $x\succeq_i y,\forall i\in I \Rightarrow x\trianglerighteq y$.
        \item is \textbf{independent of irrelevant alternatives} (IIA) if, $\forall \{x,y\}\subseteq X$, and any $\succeq$ and $\succeq'$ with $\succeq_i\mid_{x,y}=\succeq'_i\mid_{x,y}, \forall i\in I$, if $x\trianglerighteq y$ then $x\trianglerighteq' y$.
    \end{enumerate}
\end{definition}


\subsection{Arrow's Theorem}
\begin{theorem}[Arrow's impossibility theorem]
    Suppose $|X|\geq 3$, $\mathcal{A}=\mathcal{R}^I$ (UD). Then if a SWF $f$ satisfies T, PA, and IIA, then it fails to be ND.
\end{theorem}
\begin{proof}
    \normalfont
    Yu, N. N. (2012). A one-shot proof of Arrow's impossibility theorem. \textit{Economic Theory}, 523-525.
\end{proof}


\section{Demand Theory and Equilibrium}
Budget set is given by $B=\{x\in X\subseteq \mathbb{R}_+^K: p\cdot x\leq w\}$, where $w$ is the DM's wealth and $p$ is the vector of prices. Without losing generality, we can assume $w=1$.

The DM's problem is finding the $\succeq$-best bundle $x\in B(p)$.

\begin{lemma}
    If $\succeq$ is continuous, then all such problems have a solution.
\end{lemma}

\begin{lemma}
    If $\succeq$ is convex, then the set $\succeq$-best bundle $x\in B(p)$ is convex.
\end{lemma}

\begin{lemma}
    If $\succeq$ is strictly convex, then the set $\succeq$-best bundle $x\in B(p)$ is (at most) a singleton.
\end{lemma}


Assume that $\succeq$ is differentiable and denote the vector of “subjective value
numbers” at $x^*$ (as defined above) by $v(x^*)=(v_1(x^*),...,v_K(x^*))$. If $x^*\in B()$


Consider a consumer's problem
\begin{equation}
    \begin{aligned}
        \max_{x\in X} u(x)\\
        s.t.\ p\cdot x\leq w
    \end{aligned}
    \tag{UMP}
    \label{UMP}
\end{equation}
The set of all optimal solutions are represented by $x(p,w)$.

\begin{proposition}
    If $p>>0$ and $u(\cdot)$ is continuous, then \ref{UMP} has a solution.
\end{proposition}
Solution: Marshallian (Uncompensated) Demand.

\begin{proposition}
    If $\succeq$ is monotone, then $p\cdot x=w$ for all $x\in x(p,w)$.
\end{proposition}

\begin{proposition}
    If $\succeq$ is convex, then the set of solutions $x(p,w)$ is convex.
\end{proposition}
\begin{proof}
    Suppose $x,x'\in X$. The optimal utility $u^*=u(x)=u(x')$. For any $\alpha\in[0,1]$, let $x''=\alpha x+(1-\alpha)x'$. Because $\succeq$ is convex, we have $u(\cdot)$ is quasi-concave, that is $u(x'')\geq u^*$. $x''$ is also feasible. So, $x''\in x(p,w)$.
\end{proof}

Consider the duality
\begin{equation}
    \begin{aligned}
        \min_{x\in X} p\cdot x\\
        s.t.\ u(x)\geq u
    \end{aligned}
    \tag{EMP}
    \label{EMP}
\end{equation}
The optimal solutions are represented by $h(p,u)$.

Solution: Hicksian (compensated) demand.

\begin{proposition}
    $u(\cdot)$ is monotone and $p>>0$.
    \begin{enumerate}[(i).]
        \item For $w>0$, if $x^*\in x(p,w)$, then $x^*\in h(p,u(x^*))$ and $px^*=w$.
        \item For $u>u(0)$, if $x^*\in h(p,u)$, then $x^*\in x(p,p\cdot x^*)$ and $u(x^*)=u$.
    \end{enumerate}
\end{proposition}

Slutsky: how change of price in good $k$ affects the demand of product $l$.
\begin{equation}
    \begin{aligned}
        \frac{\partial x_l(p,w)}{\partial p_k}=\underbrace{\frac{\partial h_l(p,u)}{\partial p_k}}_{\textnormal{substitution effect}}\underbrace{-\frac{\partial x_l(p,w)}{\partial w} x_k(p,w)}_{\textnormal{income effect}}
    \end{aligned}
    \nonumber
\end{equation}

\begin{definition}
    \normalfont
    Given endowment $\{w^i\}_{i\in I}$. A \textbf{competitive equilibrium} is a pair $p\in \mathbb{R}^L$ (price vector over $L$ goods) and an allocation $(x^i)_{i\in I}$ such that:
    \begin{enumerate}[(i).]
        \item $x^i\in \argmax u^i(x)$ s.t. $p\cdot x^i\leq p\cdot w^i, \forall i\in I$.
        \item $\sum_{i\in I}x^i_\rho (p,w)=\sum_{i\in I}w^i_\rho, \forall \rho\in L$.
    \end{enumerate}
\end{definition}

\begin{definition}
    \normalfont
    An allocation $x$ is \textbf{Pareto-effecient} if there doesn't exist an allocation $y$ s.t. $u_i(y)\geq u_i(x)$ for each $i\in I$ and $u_j(y)> u_j(x)$ for some $j\in I$.
\end{definition}

(Assume consumers' preferences are strict monotone)
\begin{theorem}[First-order fundamental welfare theorem]
    Suppose $(p^*,x^*)$ is a competitive equilibrium. Then $x^*$ is Pareto-efficient.
\end{theorem}

If CE exists we can prove a Pareto-effecient allocation is CE.
\begin{theorem}[Second-order fundamental welfare theorem]
    Suppose that $x^*$ is Pareto efficient and consumers receive endowment worth $w^i=p\cdot {x^i}^*$ for all $i\in I$. Then, if a competitive equilibrium exists for such $w$, then $x^*$ is a competitive equilibrium allocation.
\end{theorem}


\section{Basic Game Theory}
\subsection{von Neumann and Morgenstern Expected Utility (EU)}
\begin{definition}[von Neumann and Morgenstern Expected Utility (EU)]
\normalfont
    There a lottery $\mathcal{L}$ whose outcomes are denoted by $1,...,N$ with probabilities $p_1,...,p_N$ All lotteries have Bernoulli utility $u_1,...,u_N$ for an agent.

    With VNM (expected) utility, we can say two lotteries $\mathcal{L}\succeq \mathcal{L}'$ if and only if $\sum_{i=1}^N u_ip_i\geq \sum_{i=1}^N u_ip'_i$
\end{definition}

\subsection{Game, Dominant Strategy}
A game is denoted by $$\Gamma =\left(\underbrace{I}_{\textnormal{players}},\underbrace{\{S_i\}_{i\in I}}_{\textnormal{Strategy Set}},\underbrace{\{u_i(\cdot)\}_{i\in I}}_{\textnormal{VNM utility}}\right)$$

$u_i:\prod_{i\in I}S_i \rightarrow \mathbb{R}$ is the utility function that maps all players' strategies to a player's utilities.

\begin{definition}[Dominant Strategy]
    \normalfont
    A strategy $s_i\in S_i$ is \textbf{dominant} for $i$ in $\Gamma$ if for all $s'_i\neq s_i$, we have $u_i(s_i,S_{-i})\geq u_i(s'_i,S_{-i})$.
\end{definition}

\subsection{Nash Equilibrium and Existence}
\begin{definition}[Nash Equilibrium]
    \normalfont
    A strategy profile $\Sigma=(\sigma_1,...,\sigma_I)$ is a \textbf{Nash} equilibrium of the game $\Gamma$ if for every $i\in I$, we have: $u_i(\sigma^*_i,\sigma^*_{-i})\geq u_i(\sigma'_i,\sigma^*_{-i}), \forall \sigma'_i\in \Delta(S_i)$ (can't benefit from deviating).
\end{definition}

\begin{theorem}[Existence of Nash Equilibrium]
    A Nash equilibrium exists in $\Gamma$ if for all $i\in I$,
    \begin{enumerate}[(i).]
        \item $S_i$ is non-empty, convex, compact, subset of $\mathbb{R}^m$ (i.e., for some finite dimensions of real numbers).
        \item $u_i(s_i,...,s_I)$ is continuous in $(s_i,...,s_I)$ and quasi-concave in any $s_i$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    We prove a lemma for the best response correspondence $b_i(s_{-i})=\argmax_{s_i\in S_i}u(s_i,s_{-i})$ firstly.
    \begin{lemma}
        Suppose $\{S_i\}_{i\in I}$ are non-empty. Suppose that $S_i$ is compact and convex and $u_i$ is continuous in $(s_i,...,s_I)$ and quasi-concave in any $s_i$, then $b_i(s_{-i})$ is non-empty, convex-valued and uhc.
    \end{lemma}
    \begin{proof}
        This lemma is proved by Berge's Maximum Theorem (Theroem \ref{thm:Berge's Maximum Theorem}).
    \end{proof}
    Consider the function $b: S \rightarrow S$ with $b(s_i,...,s_I)=\{b_1(s_{-1}),...,b_I(s_{-I})\}$.

    As we proved $b$ is non-empty, convex-valued and uhc from $S$ to $S$ where $S$ is non-empty, compact, and convex. By the Kakutani's Fixed Point Theorem (Theorem \ref{thm:Kakutani's Fixed Point Theorem}), we have $b$ has a fixed point $s\in S$, which should be the Nash equilibrium.
\end{proof}

\subsection{Bayesian Game}
\begin{definition}[Bayesian Game]
    \normalfont
    A \textbf{Bayesian game} is defined by $\Gamma=(I, \{S_i\}_{i\in I}, \{u_i(\cdot)\}_{i\in I},\{\Theta_i\}_{i\in I}, \{F_i\}_{i\in I})$
    where $u_i(s_i,s_{-i},\theta_i)$ maps the strategies of players and player $i$'s type $\theta_i\in \Theta_i$ to player $i$'s utilities. $F_i$ is the distribution of the player $i$'s type.
\end{definition}


\subsection{Mechanism Design}
Design incentives for agents to reveal their types or achieve particular society outcomes.

Given the set of agents, alternatives (for the society), and types (of agents) are $I, X, \Theta$ and a social choice function $f:\Theta=(\Theta_1,...,\Theta_I) \rightarrow X$.

\begin{definition}[Mechanism]
    \normalfont
    A \textbf{mechanism} is represented as $$\Gamma=\left((S_1,...,S_I), g:S\triangleq(S_1,...,S_I) \rightarrow X\right)$$
    where $S_i$ represents the strategy set of agent $i$.
\end{definition}

\begin{definition}[Implement]
    \normalfont
    $\Gamma$ (indirectly) \textbf{implements} a social choice function $f$ if $\exists \left(s_1^*(\cdot),...,s_I^*(\cdot)\right)$ of a game induced by $\Gamma$ such that $g(s_1^*(\theta_1),...,s_I^*(\theta_I))=f(\theta_1,...,\theta_I)$ for all $(\theta_1,...,\theta_I)\in \Theta$
\end{definition}

\begin{definition}[Direct Mechanism]
    \normalfont
    A mechanism is \textbf{direct mechanism} if $S_i=\Theta_i$ for all $i\in I$ and $g(\theta)=f(\theta)$ for all $\theta=(\theta_1,...,\theta_I)\in\Theta$. So, a direct mechanism can be represented by $\Gamma=(\Theta,f(\cdot))$.
\end{definition}

\begin{definition}[Weak Dominance]
    \normalfont
    A strategy is weakly dominant if for all $\theta_i\in\Theta_i$ and all $s_{-i}(\cdot)\in S_{-i}$, we have:
    $$u_i(g(s_i(\theta_i),s_{-i}),\theta_i)\geq u_i(g(s'_i,s_{-i}),\theta_i)$$
    for all $s'_i\neq s_i$.
\end{definition}

\begin{definition}[Dominant Strategy Equilibrium]
    \normalfont
    Strategy profile $s^*=(s_1^*(\cdot),...,s_I^*(\cdot))$ is a \textbf{dominant strategy (D-S) equilibrium} of $\Gamma=(S,g(\cdot))$ if for all $i\in I$ and $\theta_i\in \Theta$, we have:
    \begin{equation}
        \begin{aligned}
            u_i(g(s_i^*(\theta_i),s_{-i}),\theta_i)&\geq u_i(g(s'_i,s_{-i}),\theta_i)
        \end{aligned}
        \nonumber
    \end{equation}
    for all $s'_i\in S_i$ and $s_{-i}\in S_{-i}$.
\end{definition}

\begin{definition}[Implement in dominant strategies]
    \normalfont
    $\Gamma$ \textbf{implements} $f$ in \textbf{dominant strategies} if $\exists$ a dominant strategy (D-S) equilibrium $s^*$ of $\Gamma$ such that $g(s^*(\theta))=f(\theta)$.
\end{definition}

\begin{definition}[Strategy-Proof, DSIC]
    \normalfont
    $f$ is \textbf{strategy-proof} (also called dominant-strategy-incentive-compatible, DSIC) if "$s^*_i(\theta_i)=\theta_i$ for all $\theta_i\in\Theta_i$ and all $i\in I$" is a dominant strategy (D-S) equilibrium of the direct mechanism $\Gamma=(\Theta,f(\cdot))$.
\end{definition}

\begin{theorem}[Revelation Principle]
    Suppose that $\exists \Gamma=(S,g(\cdot))$ that (indirectly) implements $f$ in dominant strategies. Then $f$ is strategy-proof (DSIC).
\end{theorem}

\begin{center}\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[domain=0:3.25]
        \node at (0,0) {Types: $\Theta$};
        \draw[->](1,0)--(7,0) node[right] {Alternatives: $X$};
        \draw[dashed,->](0.5,-0.5)--(3.5,-3);
        \node at (4,-3) {$s^*(\theta)$};
        \draw[dashed,->](4.5,-3)--(7.5,-0.5);
        \draw[->](0,-0.5)--(3.5,-3.5);
        \node at (4,-3.5) {$s^*=\theta$};
        \draw[->](4.6,-3.5)--(8.1,-0.5);
        \node at (0.5,-1.8) {Desirable};
        \node at (0.5,-2.2) {Situation};
        \node at (2.5,-1.3) {Mechanism};
        \node at (2.5,-1.7) {$\Gamma$};
        \node at (6.5,-2.5) {$f(\theta)$};
        \node at (5.5,-1.5) {$g(s^*(\theta))$};
    \end{tikzpicture}
    \caption{How Mechanism Design works}
    \label{}
\end{figure}\end{center}

\begin{theorem}[Gibbard-Satterthwaite theorem]
    Suppose that $|X|\geq 3$ and a social choice function $f$ is surjective. Then,
    \begin{center}
        $f$ is strategy-proof (DSIC) $\Leftrightarrow$ $f$ is dictatorial (\ref{SWF_properties})
    \end{center}
\end{theorem}
Some lemmas can help to prove the theorem.
\begin{lemma}
    If $f$ is strategy-proof (DSIC) and $f(\succeq)=x$ and $x\succeq_i y \Rightarrow x\succeq'_i y$ for all $i\in I$ and all $x\neq y\in X$, then $f(\succeq')=x$.
\end{lemma}

\begin{lemma}[Pareto Effeciency]
    If $f$ is strategy-proof (DSIC) and $x\succ_i y$ for all $i\in I$, then $f(\succeq')\neq y$.
\end{lemma}

\begin{example}
    Define $\succeq=\begin{pmatrix}
        x&y\\
        y&x\\
        z&z
    \end{pmatrix}$ and $\succeq'=\begin{pmatrix}
        x&y\\
        y&z\\
        z&x
    \end{pmatrix}$, each column 1/2 represents player 1/2's preferences.
\end{example}

\subsection{Cho-Kreps Intuitive Criterion}
\begin{definition}[Equilibrium Dominated Message]
    \normalfont
    A message is \textbf{equilibrium dominated} for a type if the type must do strictly worse by sending the message than it does in equilibrium (i.e., payoff in eq. is strictly better than maximum payoff from deviating).
\end{definition}

\begin{definition}[Cho-Kreps Intuitive Criterion]
    \normalfont
    If an information set is off the eq. path and a message is eq. dominated for a type, then beliefs should assign zero probability to the message coming from that type (if possible).
\end{definition}


\chapter{Market Design}
Based on
\begin{enumerate}[$\circ$]
    \item Two-Sided Matching: A Study in Game-Theoretic Modeling and Analysis, Roth, Alvin E.\& Sotomayor, Matilda, 1990.
    \item Fleiner, T. (2003). A fixed-point approach to stable matchings and some applications. \textit{Mathematics of Operations research}, 28(1), 103-126.
    \item Hatfield, J. W., \& Kominers, S. D. (2017). Contract design and stability in many-to-many matching. \textit{Games and Economic Behavior}, 101, 78-97.
\end{enumerate}
\section{Matching One-to-One}
Suppose there are doctors ($D$) and hospitals ($H$). For a doctor $d$, define a relation $\succeq_d$ over $H\cup\{d\}$; for a hospital $h$, define a relation $\succeq_h$ over $D\cup\{h\}$. A matching market is defined by $$\left(D,H,\{\succeq_i\}_{i\in D\cup H}\right)$$

\begin{note}
    Given a matching $\mu: D\cup H \rightarrow D\cup H$, we would call $\mu(d)$ be "$d$'s match".
\end{note}

\begin{definition}[Involution]
    \normalfont
    A matching $\mu: D\cup H \rightarrow D\cup H$ is \textbf{involution} such that $$\mu (d)\neq d \Rightarrow \mu(d)\in H, \forall d\in D$$ and $$\mu (h)\neq h \Rightarrow \mu(h)\in D, \forall h\in H$$
\end{definition}

\begin{definition}[Stable]
    \normalfont
    A matching $\mu: D\cup H \rightarrow D\cup H$ is \textbf{stable} if it is
    \begin{enumerate}[$\circ$]
        \item Individually Rational: $\nexists$ $i$ for whom $i>\mu(i)$.
        \item (Pairwise) Unblocked: $\nexists$ $(d,h)$ such that $d\succ_h \mu(h)$ and $h\succ_d \mu(d)$.
    \end{enumerate}
\end{definition}

\begin{theorem}[Gale-Shapley, 1962]
    For any matching market, a stable matching $\mu$ exists.
\end{theorem}
\begin{proof}
    We prove it by an algorithm:
    \begin{definition}[Deferred Acceptance Algorithm (DA)]
        \normalfont
        At each round, every doctor applies for his most preferred hospital among those haven't rejected him. Each hospital chooses its most preferred doctors among its applicants and the one on the previous waitlist, and then rejects others.
    \end{definition}
    Observation: DA terminates $\mu$. We want to prove
    \begin{enumerate}
        \item $\mu$ is IR (obviously);
        \item $\mu$ is unblocked.
        \subitem Suppose there is a block $(d,h)$ such that $d\succ_h \mu(h)$ and $h\succ_d \mu(d)$. That is impossible, because the $d\neq \mu(h)$, the $d$ must be rejected by $h$, which means $h\preceq_d \mu(d)$.
    \end{enumerate}
\end{proof}

\begin{note}
    We call "$h$ is \textbf{achievable} for $d$" if $\mu(d)=h$ for some stable matching $\mu$.
\end{note}


\subsection{Matching Markets: One-to-One}
\begin{definition}[$D$-Optimal Matching]
    \normalfont
    A matching $\mu: D\cup H \rightarrow D\cup H$ is \textbf{$D$-optimal}, denoted by $\mu^D$, if for any stable $\mu'$ we have that $\mu^D\succeq_D \mu'$ (the best stable matching for all doctors).
\end{definition}

\begin{theorem}[Deferred Acceptance Algorithm $\Rightarrow$ $D$-Optimal Matching]
    Deferred Acceptance Algorithm (with D proposing) terminates in $\mu^D$.
\end{theorem}
\begin{proof}
    %Suppose $d$ proposes to some $h$.
    %\begin{enumerate}
        %\item If $d$ is unacceptable ($d$ is below $\{h\}$) in $h$'s ranking, then $h$ is unachievable anyway.
        %\item Suppose $\exists d'\succ_h d$. If $h$ is achievable for $d'$, we have $h\succ_{d'} h'$
    %\end{enumerate}
    ...Theorem 2.12 (Gale and Shapley)
\end{proof}

\begin{theorem}[Lone-Wolf Theorem]
    The set of matched agent is identical in every stable $\mu$.
\end{theorem}
\begin{proof}
    $|\mu^D(H)|\geq |\mu(H)|\geq |\mu^H(H)|$; by symmetry, $|\mu^H(D)|\geq |\mu(D)|\geq |\mu^D(D)|$. Because $|\mu^D(H)|=|\mu^D(D)|$ and $|\mu^H(H)|=|\mu^H(D)|$ by one-to-one, so everything is equal.
\end{proof}

\subsection{Joint and Meet}
\begin{definition}[Joint and Meet]
    \normalfont
    \begin{enumerate}
        \item \textbf{Join $\mu \vee_D \mu'$} assign the more preferred match to every $d$ and the less preferred match to every $h$, that is,
        \begin{equation}
            \begin{aligned}
                \mu \vee_D \mu'(d)=\left\{\begin{matrix}
                    \mu(d),&\textnormal{ if }\mu(d)>_d\mu'(d)\\
                    \mu'(d),&\textnormal{ otherwise}
                \end{matrix}\right., \forall d\in D
            \end{aligned}
            \nonumber
        \end{equation}
        \begin{equation}
            \begin{aligned}
                \mu \vee_D \mu'(h)=\left\{\begin{matrix}
                    \mu(h),&\textnormal{ if }\mu(h)<_h\mu'(h)\\
                    \mu'(h),&\textnormal{ otherwise}
                \end{matrix}\right., \forall h\in H
            \end{aligned}
            \nonumber
        \end{equation}
        \item \textbf{Meet $\mu\wedge_D\mu'$} assign the less preferred match to every $d$ and the more preferred match to every $h$, that is,
        \begin{equation}
            \begin{aligned}
                \mu \wedge_D \mu'(d)=\left\{\begin{matrix}
                    \mu(d),&\textnormal{ if }\mu(d)<_d\mu'(d)\\
                    \mu'(d),&\textnormal{ otherwise}
                \end{matrix}\right., \forall d\in D
            \end{aligned}
            \nonumber
        \end{equation}
        \begin{equation}
            \begin{aligned}
                \mu \wedge_D \mu'(h)=\left\{\begin{matrix}
                    \mu(h),&\textnormal{ if }\mu(h)>_h\mu'(h)\\
                    \mu'(h),&\textnormal{ otherwise}
                \end{matrix}\right., \forall h\in H
            \end{aligned}
            \nonumber
        \end{equation}
    \end{enumerate}
\end{definition}

\begin{theorem}[Join and Meet of Stable Matchings are Stable]
    If $\mu$ and $\mu'$ are stable, then $\mu\vee_D\mu'$ and $\mu\wedge_D\mu'$ are stable.
\end{theorem}

\subsection{Strategic Incentives}
\begin{enumerate}[$\circ$]
    \item Type $=$ preference list.
    \item SCF: $f: \Theta \rightarrow \mathcal{M}$, where $\mathcal{M}$ is a set of stable matchings;
    \item Is $f$ strategy-proof?
    \item Does there exist a stable and strategy-proof (direct) mechanism?
\end{enumerate}

\begin{definition}
    \normalfont
    We say a mechanism $\varphi$ is strategy-proof (SP) if $\varphi(\succ_i,\succ_{-i})\geq \varphi (\succ'_i,\succ_{-i})$ for all $i\in I$ and $\succ'_i$ and $\succ_{-i}$.
\end{definition}

\begin{theorem}[Impossibility theorem (Roth)]
    There is no stable and strategy-proof (SP) mechanism.
\end{theorem}

The mechanism that yields the D-optimal stable matching (in terms of the stated preferences) makes it a dominant strategy for each doctor to state his true preferences. (Similarly, the mechanism that yields the H-optimal stable matching makes it a dominant strategy for every hospital to state its true preferences.)
\begin{theorem}[Dubins and Freedman; Roth]
    The doctor($D$)-optimal stable mechanism is strategy-proof for doctors.
\end{theorem}
\begin{proof}
    %Suppose under truthful $\succ$ (all doctors), a doctor $d$ has $\mu(d)=h$. $d$ changes his report to $\succ'_d$ such that $\mu'(d)=h'\succ_d h$.\\
    %Consider $\succ''_d$ which $\succ'_d$ truncated below $h'$.\\
    %Now, run a doctor-proposing DA (conside $\mu^D$) under $\succ''_d$. $d$ is unmatched.
\end{proof}


\section{Matching Many-to-Many}
Contracts are denoted by $x\in X$, $x_D\in D$, $x_H\in H$. $F\triangleq D\cup H$.

Consider a set of contracts $Y\subseteq X$,
\begin{enumerate}[$\circ$]
    \item $Y_D$ = doctors listed in $Y$;
    \item $Y_d$ = the contract in $Y$ that list the doctor $d$;
    \item $\succ_d$ over set of contracts that name the doctor $d$;
    \item The set of contracts $f\in F$ chooses from $Y$: $C_f(Y)=\max_{\succ_f}\{Z\subseteq X:Z\subseteq Y_f\}\subseteq Y_f$;
    \item The set of contracts doctors choose from $Y$: $C_D(Y)=\cup_{d\in D}C_d(Y)$.
    \item The set of contracts doctors reject from $Y$: $R_D(Y)=Y\backslash C_D(Y)$.
\end{enumerate}
The outcome of matching is $Y\subseteq X$.

\begin{definition}[Stable Contracts]
    \normalfont
    $A\subseteq X$ is \textbf{stable} if
    \begin{enumerate}[$\circ$]
        \item Individually Rational (IR): for all $f\in F$: $C_f(A)=A_f$;
        \item Unblocked: $\nexists$ non-empty $Z\subseteq X$ such that $Z\cap A=\emptyset $ and for all $f\in F$, $Z_f\subseteq C_f(A\cup Z)$.
    \end{enumerate}
\end{definition}

\begin{example}
    Preferences over doctor $d$: $\{x,y\}>\{x\}>\emptyset>\{y\}$; Preferences over hospital $h$: $\{y\}>\{x,y\}>\{x\}>\emptyset$.\\
    $\{x\}$ $\Rightarrow$ $\{x,y\}$ $\Rightarrow$ $\{y\}$ $\Rightarrow$ $\emptyset$ $\Rightarrow$ $\{x\}$.
\end{example}

\begin{definition}[Substitutability Condition]
    \normalfont
    Preference of $f$ satisfies the \textbf{substitutability condition} if for all $Y\subseteq X$ and $x,z\in X\backslash Y$:
    $$z\notin C_f(Y\cup\{z\}) \Rightarrow z\notin C_f(Y\cup\{z\}\cup\{x\})$$
    ($Y'\subseteq Y\subseteq X \Rightarrow R_f(Y')\subseteq R_f(X)$, where $R$ is the rejection choice.)
\end{definition}
If $z$ is rejected given a set, then it should also be rejected given a larger set.


\begin{theorem}
    If contracts are substitutes, then $Y\subseteq X$ is stable \underline{if and only if} pairwise stable.
\end{theorem}
\begin{proof}
    Prove $\Leftarrow$:
    (If not pairwise stable $\Rightarrow$ not stable)\\
    Suppose that $Z$ is a block. So, $Z\subseteq C_f(A\cup Z)$ for all $f$ listed in $Z$.\\
    We can pick a $z\in Z$ such that $z\in C_f(A\cup Z)$. By the substitutability condition, $z\in C_f(A\cup \{z\})$. So, it is stable.
\end{proof}

\begin{theorem}
    If contracts are substitutes, then a stable outcome exists.
\end{theorem}

\begin{definition}[Lattice]
    \normalfont
    On a \textbf{lattice}, $L=(X,<,\wedge,\vee)$ (or we just use $L=(X,<)$), $<$ is a partial order on $X$ in such a way that any two elements $x$ and $y$ of $X$ have a unique greatest lower bound (glb) $x \wedge y$ (meet) and a unique lowest upper bound (lub) $x \vee y$ (join).
\end{definition}



\begin{definition}[Complete Lattice]
    \normalfont
    A lattice $L=(X,<)$ is \textbf{complete} if there are both a meet (i.e. a greatest lower bound) and a join (i.e. a least upper bound) for any subset $Y\subseteq X$.\\
    These generalized meet and join operations on $Y$ are denoted by $\wedge Y$ and $\vee Y$.
\end{definition}

\begin{definition}[Monotone Function over Lattice]
    \normalfont
    A function from one lattice to another lattice $f:(X,<) \rightarrow (X',<')$ is \textbf{monotone} if $x\leq y \Rightarrow f(x)\leq' f(y)$ for any $x,y\in X$.
\end{definition}

\begin{theorem}[Tarski 1955]
    Let $L=(X,<)$ be a complete lattice and $f: L \rightarrow L$ be monotone ($\leq$) function on $L$. Then, the set $\{x\in L: f(x)=x\}$ of fixed points is a non-empty, complete lattice with order $\leq$.
\end{theorem}
\begin{proof}
    Fleiner, T. (2003). A fixed-point approach to stable matchings and some applications. \textit{Mathematics of Operations research}, 28(1), 103-126.
\end{proof}


%Given sets of contracts $X^D$ and $X^H$.

%Operator $f(X^D,X^H)$ produces $\left(X\backslash R_H(X^H), X\backslash R_D(X^D)\right)$. Fixed point: $\left\{\begin{matrix}
    %X^D=&X\backslash R_H(X^H)\\
    %X^H=&X\backslash R_D(X^D)
%\end{matrix}\right.$. $X^D\cap X^H=A$ is stable.\\
%(Find the intercection of contracts that are not rejected in both $X^D$ and $X^H$, which is stable.)




%Operator $g(X^D,X^H)$ produces $g_H(X^H)=\{x\in X: x\in C_H(X^H\cup\{x\})\}$ and $g_D(X^D)=\{x\in X: x\in C_D(X^D\cup\{x\})\}$. Fixed point: $\left\{\begin{matrix}
    %X^D=&g_H(X^H)\\
    %X^H=&g_D(X^D)
%\end{matrix}\right.$. $X^D\cap X^H=A$ is stable.\\
%(Find the intercection of contracts that are accepted both $X^D$ and $X^H$, which is stable.)


%Define partial order, $(X^D,X^H)\geq (\bar{X}^D,\bar{X}^H)$ if $X^D\subseteq \bar{X}^D$ and $X^H \supseteq  \bar{X}^H$.

%If $(X^D,X^H)\geq (\bar{X}^D,\bar{X}^H)$, then $g(X^D,X^H)\geq g(\bar{X}^D,\bar{X}^H)$


%Check if $X^D\subseteq \bar{X}^D$, then $g(X^D)\supseteq g(\bar{X}^D)$, by substitutability.


%Prove $X^D\cap X^H=A$ is stable:\\
%Given the claim $C_D(X^D)=A$ (prove later)
%\begin{enumerate}[1).]
    %\item IR: $C_D(X^D)=A$ by $C(A)=A$;
    %\item Unblocked: $z\in X\backslash A$ that blocks. Then $z\notin X^H \Rightarrow z\in C_D(A\cup\{z\})$ and $z\notin C_D(X^D\cup\{z\})$, but $C_D(X^D)=A \Rightarrow z\notin C_D(A\cup\{z\})$.
%\end{enumerate}


If some contracts are not substitute, there are no stable outcomes exist.

\section{Matching Many-to-One}
\underline{Settings}
\begin{enumerate}[$\circ$]
    \item Doctors, $D$; Hospitals, $H$; Contracts $X=D\times H\times \textnormal{terms}$;
    \item Hospitals preference $\succ_h$ over $2^X$;
    \item Doctors preference $\succ_d$ over $X$ (compare one contract with another one contract, not compare over sets of contracts);
    \item Outcome is $Y\subseteq X$ s.t. $|Y_d|\leq 1$ for all $d\in D$ (a doctor signs at most one contract).
\end{enumerate}

What restriction do we need to have a stable matching? Not as strong as substitute.

\begin{corollary}
    Doctor-proposing DA algorithm produces a doctor-optimal stable matching.
\end{corollary}

\begin{example} The preferences of agents are
    \begin{enumerate}[$\circ$]
        \item $d_1: h_1\succ h_2$; $d_2: h_1\succ h_2$; $d_3: h_2\succ h_1$;
        \item $h_1: d_3\succ d_1,d_2\succ d_1\succ d_2$; $h_2: d_1\succ d_2\succ d_3$.
    \end{enumerate}
    There are two stable outcomes
    \begin{enumerate}
        \item $(d_1,h_2)$, $(d_3,h_1)$;
        \item $(d_1,h_1), (d_2,h_1), (d_3,h_2)$.
    \end{enumerate}
    \begin{remark}
        Lone-Wolf Theorem doesn't hold.
    \end{remark}

    Assume the $d_2$'s true preference is $h_2\succ h_1$ and he reveals it, there is only one stable matching: $(d_1,h_2)$, $(d_3,h_1)$. So, the $d_2$ may benefit from lying.\\
    \begin{remark}
        Strategy-proof doesn't hold.
    \end{remark}
\end{example}

\begin{definition}[Law of Aggagate Demand/ Cardianlity Monotomicity (CM)]
    \normalfont
    For $h$, $Y\subseteq Y'\subseteq X \Rightarrow |C_h(Y)|\leq |C_h(Y')|$
\end{definition}

\begin{theorem}
    Under substitutes and CM, doctor-proposing DA is strategy-proof and LWT holds.
\end{theorem}

\begin{theorem}[Rural Hosptial Theorem]
    Under substitutes / CM, hospitals have same numbers of contracts in every stable outcome.
\end{theorem}

\subsubsection*{Cadets-branch matching}
Can be found in:
\begin{enumerate}[$\circ$]
    \item Jagadeesan, R. (2019). Cadet-branch matching in a Kelso-Crawford economy. \textit{American Economic Journal: Microeconomics}, 11(3), 191-224.
\end{enumerate}


\begin{remark}
    Contracts are not substitutes.
\end{remark}

\begin{definition}[Unilateral Substitute]
    \normalfont
    Contracts are \textbf{unilateral substitutes} if for all $z,x\in X$ and $Y\subseteq X$ \underline{such that $z_D\notin Y_D$} if $z\notin C_h(Y\cup\{z\}) \Rightarrow z\notin C_h(Y\cup\{z\}\cup\{x\})$
\end{definition}

\begin{remark}
    Preferences of branches satisfying unilateral substitute.
\end{remark}

\begin{remark}
    The outcome of doctor-proposing DA algorithm is doctor-optimal and stable.
\end{remark}

\section{Networks}
Based on
\begin{enumerate}[$\circ$]
    \item Fleiner, T., Jankó, Z., Tamura, A., \& Teytelboym, A. (2015). Trading networks with bilateral contracts. arXiv preprint arXiv:1510.01210.
    \item Fleiner, T., Jankó, Z., Schlotter, I., \& Teytelboym, A. (2023). Complexity of stability in trading networks. \textit{International Journal of Game Theory}, 1-20.
\end{enumerate}

Considering a trading network represented by a directed graph, where nodes are firms $F$ and edges $X$ are contracts (income arrow can be understood as buying products and outcome arrow can be understood as selling products).

The choice function of $f\in F$ is represented by $C^f$, the choice of $f$ over $Y_f\subseteq X_f$ is $C^f(Y_f)\subseteq Y_f$, where $X_f$ is the set of contracts involving $f$.

The choice sets of buyer side (B) and seller side (S) are defined as
\begin{equation}
    \begin{aligned}
        C_B^f(Y|Z)&\triangleq C^f(Y_f^B\cup Z_f^S)\cap X_f^B\\
        C_S^f(Z|Y)&\triangleq C^f(Z_f^S\cup Y_f^B)\cap X_f^S
    \end{aligned}
    \nonumber
\end{equation}
where $Y$ is the contracts from buyer side and $Z$ is the contratcts from seller side.


\begin{definition}[Irrelevance of Rejected Contracts]
    \normalfont
    Irrelevance of Rejected Contracts (IRC): $C(A)\subseteq B\subseteq A \Rightarrow C(A)=C(B)$
\end{definition}

\begin{definition}[Fully Substitute]
    \normalfont
    $C^f$ is \textbf{fully substitute} if for $Y'\subseteq Y\subseteq X$ and $Z'\subseteq Z\subseteq X$,
    \begin{equation}
        \begin{aligned}
            R_B^f(Y'|Z)\subseteq R_B^f(Y|Z)\\
            R_S^f(Z'|Y)\subseteq R_S^f(Z|Y)
        \end{aligned}
        \nonumber
    \end{equation}
    and
    \begin{equation}
        \begin{aligned}
            R_B^f(Y|Z)\subseteq R_B^f(Y|Z')\\
            R_S^f(Z|Y)\subseteq R_S^f(Z|Y')
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}
Define partial order, $(Y,Z)\geq (Y',Z')$ if $Y\subseteq Y'$ and $Z\supseteq  Z'$.


\begin{definition}[Stable Outcome, Hatfield and Kominers (2012)]
    \normalfont
    An outcome $A\subseteq X$ is stable if it is
    \begin{enumerate}
        \item Individual Rational: $\forall f\in F$, $C^f (A_f)=A_f$;
        \item Unblocked: there is no non-empty set $Z\subseteq X$ s.t. $Z\cap A=\emptyset$ and $\forall f\in F(Z)$, $Z_f\subseteq C^f(A\cup Z)$, where $F(Z)$ is the set of the firms are lined to $Z$.
    \end{enumerate}
\end{definition}

\begin{definition}[Trail]
    \normalfont
    \textbf{Trail} is the set of distinct edges $T=(X^1,X^2,...,X^M)$ such that the buyer side (the firm who is the buyer in the edge) of $X^i$ is exactly the seller side (the firm who is the seller in the edge) of $X^{i+1}$, which is denoted by $b(X^i)=s(X^{i+1})$, $i=1,...,M-1$.
\end{definition}


\begin{definition}[Trail-stable Outcome]
    \normalfont
    An outcome $A\subseteq X$ is \textbf{trail-stable} if its is
    \begin{enumerate}
        \item Individual Rational;
        \item There is no locally blocking trail $T=(X^1,X^2,...,X^M)$ such that
        \subitem $X^1\in C^{S(X^1)}(A\cup X^1)$;
        \subitem $\{X^i,X^{i+1}\}\in C^{b(X^{i})}(A\cup X^i\cup X^{i+1})$;
        \subitem $X^M\in C^{b(X^M)}(A\cup X^M)$.
    \end{enumerate}
\end{definition}

\begin{theorem}[Fleiner et al. 2016]
    If $C^f$ is fully substitute and IRC for all $f\in F$, then a trail-stable outcome exists.
\end{theorem}
\begin{proof}
    $Y\subseteq X$ and $Z\subseteq X$,
    \begin{equation}
        \begin{aligned}
            \Phi (Y,Z)=\left(X\backslash R_S(Z|Y), X\backslash R_B(Y|Z)\right)
        \end{aligned}
        \nonumber
    \end{equation}
    where $R_B(Y|Z)=\cup_{f\in F}R_B^f(Y|Z)$.
    \begin{claim}
        If $(Y,Z)$ is a fixed point of $\Phi$, then $A=Y\cap Z$ is trail-stable outcome.
    \end{claim}
    \begin{lemma}
        $C^f$ is fully substitute and IRC, and $(Y,Z)$ such that $Y \cap Z=A$, $C_S(Z|Y)=A$, $C_B(Y|Z)=A$. Then, for a contract $x\in X\backslash A$ and $A\subseteq A'\subseteq X$ if $C_S^{S(x)}(A\cup x|A')$ then $x\in C_S^{S(x)}(Z\cup x|A')$.
    \end{lemma}
    $\Phi$ will be monotone for the partial order $\geq$. As $(Y,Z)\geq (Y',Z')$, then $\Phi(Y,Z)\geq \Phi (Y',Z')$. Using Tarski fixed-point theorem, there is a $(Y,Z)$ fixed point.
    .....


    \textbf{Read} \textnormal{Fleiner, T., Jankó, Z., Tamura, A., \& Teytelboym, A. (2015). Trading networks with bilateral contracts. arXiv preprint arXiv:1510.01210.}
\end{proof}


\begin{proposition}
    $A$ is trail-stable $\Rightarrow$ $\exists$ $(Y,Z)$ such that $Y\cap Z=A$ and $(Y,Z)$ is a fixed point of $\Phi$.
\end{proposition}


\section{Corporate Game Theory}
There is a set of players $N=\{1,...,n\}$. The subset of players $S\subseteq N$ is called coalition.

There is a value function about coalition $v: 2^N \rightarrow \mathbb{R}$, which assumes $v(N)\geq \max_{S\subseteq N}v(S)$.

\begin{definition}[Cooperative Game]
    \normalfont
    A cooperative game is described by the pair $\left<N,v\right>$.
\end{definition}

\begin{definition}[Transferable Utility]
    \normalfont
    Utility is transferable if one player can losslessly transfer part of its utility to another player.
\end{definition}
Assume a TU (transferable utility) Economy. Consider a payoffs vector for all players, $x\in \mathbb{R}^n$. The efficiency requires $\sum_{i\in N}x_i=v(N)$. Individual Rational (IR) requires $x_i\geq v(\{i\})$.

\subsection{Core of Corporate Game and Farkas' lemma}
\begin{definition}[Core]
    \normalfont
    The \textbf{core} is the set of feasible allocations where no coalition of agents can benefit by breaking away from the grand coalition.
    $$C(v,N)=\left\{x\in \mathbb{R}^n: \sum_{i\in N}x_i=v(N), \sum_{i\in S}x_i\geq v(S), \forall S\subseteq N\right\}$$
\end{definition}

\begin{theorem}[Bondareva-Shapley Theorem]\ref{BST}
    The core of $\left<N,v\right>$ is non-empty ($C(v,N)\neq \emptyset$) \underline{if and only if} for every function $\alpha: 2^N\backslash\{\emptyset\} \rightarrow [0,1]$ where $\forall i\in N: \sum_{S:i\in S}\alpha(S)=1$, the following condition holds:
    \begin{equation}
        \begin{aligned}
            \sum_{S\in 2^N\backslash\{\emptyset\}}\alpha(S) v(S)\leq v(N)
        \end{aligned}
        \nonumber
    \end{equation}
\end{theorem}
Consider $B(N)$ be the solutions to: $\left\{\begin{matrix}
    \sum_{S:i\in S}y_S=1,&\forall i\in N\\
    y_S\geq 0,& \forall S\subseteq N
\end{matrix}\right.$

\begin{lemma}[Farkas' lemma]
    Let $A\in \mathbb{R}^{m\times n}$ and $b\in \mathbb{R}^m$. Then, \textbf{exactly one} of the following statement is true
    \begin{enumerate}[(1).]
        \item There exists $x\in \mathbb{R}^n$ such that $Ax=b$ and $x\geq 0$
        \item There exists $y\in \mathbb{R}^n$ such that $A^Ty\geq 0$ and $b^T y<0$.
    \end{enumerate}
\end{lemma}

\begin{lemma}

\end{lemma}

\begin{proof}
    \begin{lemma}[(Alternative) Farkas' lemma]
        Let $A$ be $m\times n$ matrix, $b\in \mathbb{R}^m$ and $F=\{x\in \mathbb{R}^n: Ax\geq b,x\geq 0\}$. Then, either $Cx=d$ or $\exists z$ such that for $y_S\geq 0$, $C^Tz-A^Ty_S=0$ and such that $d^Tz-b^Ty_S<0$, but not both.
    \end{lemma}
    By using this lemma, we can conclude $\left\{\begin{matrix}
        v(N)z-\sum_S v(S)y_S<0\\
        z-\sum{y_S}=0\\
        y_S\geq 0
    \end{matrix}\right.$ must hold at the same time, (let $z=1$, the last two lines are $B(N)$).

    Hence, $\forall y_S\in B(N)$, we have $v(N)\geq \sum_S v(S)y_S$.
\end{proof}


\subsection{Doubly stochastic matrix and Birkhoff-von Neumann Theorem}

Consider a matching game between sellers and buyers: $v(\{i,j\})=v_{ij}$, $v(\{i\})=0$ for buyer $i$ and $v(\{j\})\geq 0$ for seller $j$.

\underline{Core:}
\begin{equation}
    \begin{aligned}
        \max_{\alpha}\quad &\sum_i\sum_j v_{ij}\alpha_{ij}\\
        \textnormal{s.t.}\quad&\sum_{i}\alpha_{ij}=1,\forall j\\
        &\sum_{j}\alpha_{ij}=1,\forall i\\
        &\alpha_{ij}\geq 0
    \end{aligned}
    \nonumber
\end{equation}

\begin{definition}[Doubly Stochastic Matrix]
    \normalfont
    A \textbf{doubly stochastic matrix} is a square matrix $X=(x_{ij})$ of non-negative real numbers, each of whose rows and columns sums to $1$.\\
    The class of $n\times n$ doubly stochastic matrices is a convex polytope (convex set in euclidean space) known as the \textbf{Birkhoff polytope}.
\end{definition}

\begin{theorem}[Birkhoff-von Neumann Theorem]
    A matrix is doubly stochastic if and only if it is a convex combination of permutation matrices.
\end{theorem}
By this theorem, we can set efficient "integer" assignment.

Can the efficient allocation be competitive equilibrium (CE)?
\begin{theorem}
    The core of assignment of game is non-empty.
\end{theorem}
\begin{proof}
    The duality of core can be written as
    \begin{equation}
        \begin{aligned}
            \min \quad&  \sum_{j}u_j^S + \sum_{i}u_i^B\\
            \textnormal{s.t.}\quad& u_j^S+u_i^B\geq v_{ij}, \forall i,j
        \end{aligned}
        \nonumber
    \end{equation}
    By strong duality, the minimum value should be equal to $V(N)$.

    Hence, $\sum_{j\in T}u_j^S + \sum_{i\in T}u_i^B\geq V(T)$ for a subset $T\subseteq N$. That is, the core is non-empty.
\end{proof}

\begin{corollary}
    For an assignment game, outcome is in the core \underline{if and only if} the outcome is CE outcome.
\end{corollary}










\chapter{Stochastic Dominance}
Based on
\begin{enumerate}[$\circ$]
    \item MIT 14.123 S15 Stochastic Dominance Lecture Notes
    \item Princeton ECO317 Economics of Uncertainty Fall Term 2007 Notes for lectures 4. Stochastic Dominance
    \item Jensen, M. K. (2018). Distributional comparative statics. \textit{The Review of Economic Studies}, 85(1), 581-610.
\end{enumerate}

\section{General Definitions}
\begin{definition}[Jensen (2018), Defnition 1]\label{Defn SD}
    \normalfont
    Let $F$ and $G$ be two distributions on the same measurable space. Let $u$ be a function for which the following expression is well-defined,
    \begin{equation}
        \begin{aligned}
            \int u(x)dF\geq \int u(x)dG
        \end{aligned}
        \label{Eq 1}
    \end{equation}
    Then:
    \begin{enumerate}[$\bullet$]
        \item $F$ \textbf{first-order stochastically dominates} $G$ if \ref{Eq 1} holds for any \underline{increasing} function $u$.
        \item $F$ is a \textbf{mean-preserving spread} of $G$ if \ref{Eq 1} holds for any \underline{convex} function $u$.
        \item $F$ is a \textbf{mean-preserving contraction} of $G$ if \ref{Eq 1} holds for any \underline{concave} function $u$.
        \item \textbf{$F$ second-order stochastically dominates $G$} if \ref{Eq 1} holds for any \underline{concave and increasing} function $u$.
        \item \textbf{$F$ dominates $G$ in the convex-increasing order} if \ref{Eq 1} holds for any \underline{convex and increasing} function $u$.
    \end{enumerate}
\end{definition}
\begin{note}
    $F$ is a \textbf{mean-preserving contraction} of $G$ $\Leftrightarrow$ $G$ is a \textbf{mean-preserving spread} of $F$.
\end{note}

\begin{definition}[MPS and MPC]
    \normalfont
    We define the following notations of sets.
    \begin{enumerate}[$\circ$]
        \item $\textnormal{MPS}(f)$ is the set of all \textbf{mean-preserving spread} of $f$;
        \item $\textnormal{MPC}(f)$ is the set of all \textbf{mean-preserving contraction} of $f$;
    \end{enumerate}
\end{definition}

\section{First-order Stochastic Dominance}
\subsection{Two Equivalent Definitions}
\begin{definition}[First-order Stochastic Dominance]
    \normalfont
    For any lotteries $F$ and $G$, $F$ \textbf{first-order stochastically dominates} $G$ if and only if the decision maker weakly prefers $F$ to $G$ under every \underline{weakly increasing} utility function $u$, i.e.,
    $$\int u (x) dF \geq \int u(x) dG$$
\end{definition}

\begin{definition}[First-order Stochastic Dominance]
    \normalfont
    For any lotteries $F$ and $G$, $F$ \textbf{first-order stochastically dominates} $G$ if and only if
    $$F(x)\leq G(x),\forall x$$
\end{definition}

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{FOSD_1.png}
    \includegraphics[scale=0.2]{FOSD_2.png}
    \caption{$F_1$ is FOSD over $F_2$: CDF and density comparison}
    \label{}
\end{figure}\end{center}

\section{Second-order Stochastic Dominance}
\subsection{Definition in terms of final goals}
\begin{definition}[Second-order Stochastic Dominance]
    \normalfont
    For any lotteries $F$ and $G$, $F$ \textbf{second-order stochastically dominates} $G$ if and only if the decision maker weakly prefers $F$ to $G$ under every \underline{weakly \textbf{increasing concave}} utility function $u$, i.e.,
    $$\int u (x) dF \geq \int u(x) dG$$
\end{definition}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.25]{SOSD_1.png}
    \includegraphics[scale=0.25]{SOSD_2.png}
    \caption{$F_1$ is SOSD over $F_2$: CDF and density comparison}
    \label{}
\end{figure}\end{center}

\subsection{Mean-Preserving Spread/Contraction}
\begin{definition}[Mean-Preserving Spread]
    \normalfont
    Let $x_F$ and $x_G$ be the random variables associated with lotteries $F$ and $G$. Then $G$ is a \textbf{mean-preserving spread} of $F$ if and only if $$x_G \stackrel{d}{=} x_F+\varepsilon$$
    for some random variable $\varepsilon$ such that $\mathbb{E}(\varepsilon\mid x_F)=0\ \forall x_F$.
\end{definition}
The "$\stackrel{d}{=}$" means "is equal in distribution to" (that is, "has the same distribution as").

\begin{note}
    Given $G$ is a \textbf{mean-preserving spread} of $F$, $G$ has larger variance than $F$.
\end{note}

\begin{example}
    $F(198)=\frac{1}{2}, F(202)=\frac{1}{2}$ and $G(100)=\frac{1}{100}$, $G(200)=\frac{98}{100}$, $G(300)=\frac{1}{100}$. Then $$x_G \stackrel{d}{=} x_F+\varepsilon$$
    where the distribution of $\varepsilon$ can be solved by $$\left\{\begin{matrix}
        G(300)=F(198)P(\varepsilon=102|x_F=198)+F(202)P(\varepsilon=98|x_F=202)\\
        G(200)=F(198)P(\varepsilon=2|x_F=198)+F(202)P(\varepsilon=-2|x_F=202)\\
        G(100)=F(198)P(\varepsilon=-98|x_F=198)+F(202)P(\varepsilon=-102|x_F=202)
    \end{matrix}\right.$$
\end{example}


\subsection{For Same Mean Distributions, Second-order Stochastic Dominance is equivalent to Mean-Preserving Spread}
\begin{theorem}[Second-order Stochastic Dominance Equivalence]\label{SOSD_equiv}
    Given $\int x dF=\int x dG$ (same mean). The following are equivalent.
    \begin{enumerate}
        \item $F$ second-order stochastically dominates $G$: $\int u (x) dF \geq \int u(x) dG$  for every weakly increasing concave utility function $u$.
        \item $F$ is a mean-preserving contraction of $G$ ($G$ is a mean-preserving spread of $F$).
        \item For every $t\geq 0$, $\int_a^t G(x)dx\geq \int_a^t F(x)dx$.
    \end{enumerate}
\end{theorem}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.25]{SOSD_3.png}
    \caption{$F_1$ is SOSD over $F_2$, $S(t):\int_a^t F_2(x)dx\geq \int_a^t F_1(x)dx$}
    \label{}
\end{figure}\end{center}

\begin{corollary}[Euqivalent Definitions of MPC and MPS]
    \normalfont
    $F$ is a mean-preserving contraction of $G$ (or $G$ is a mean-preserving spread of $F$) \underline{if and only if}
    \begin{enumerate}[(1).]
        \item $\int x dF=\int x dG$
        \item $\int_a^t G(x)dx\geq \int_a^t F(x)dx, \forall t$
    \end{enumerate}
\end{corollary}

\begin{corollary}[$\textnormal{MPC}(f)$ and $\textnormal{MPS}(f)$ are convex and compact]
    $\textnormal{MPC}(f)$ and $\textnormal{MPS}(f)$ are \textbf{convex} and \textbf{compact}.
\end{corollary}




\chapter{Signalling Game}
Based on
\begin{enumerate}[$\circ$]
    \item "Kreps, D. M., \& Sobel, J. (1994). Signalling. \textit{Handbook of game theory with economic applications}, 2, 849-867."
    \item 
\end{enumerate}

\section{Canonical Game}
\begin{definition}[Canonical Game]
    \normalfont
    \begin{enumerate}
        \item There are two players: $\mathbf{S}$ (sender) and $\mathbf{R}$ (receiver).
        \item $\mathbf{S}$ holds more information than $\mathbf{R}$: the value of some random variable $t$ with support $\mathcal{T}$. (We say that $t$ is the \textbf{type} of $\mathbf{S}$)
        \item Prior belief of $\mathbf{R}$ concerning $t$ are given by a probability distribution $\rho$ over $\mathcal{T}$ (common knowledge)
        \item $\mathbf{S}$ sends a \textbf{signal $s\in \mathcal{S}$} to $\mathbf{R}$ drawn from a signal set $\mathcal{S}$.
        \item $\mathbf{R}$ receives this signal, and then takes an \textbf{action} $a\in \mathcal{A}$ drawn from a set $\mathcal{A}$ (which could depend on the signal $s$ that is sent).
        \item $\mathbf{S}$'s payoff is given by a function $u: \mathcal{T}\times \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ and $\mathbf{R}$'s payoff is given by a function $v: \mathcal{T}\times \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$.
    \end{enumerate}
\end{definition}

\section{Nash Equilibrium}
\begin{definition}[Strategy]
    \normalfont
    A \textbf{behavior strategy} for $\mathbf{S}$ is given by a function $\sigma: \mathcal{T}\times\mathcal{S} \rightarrow [0,1]$ such that $\sum_s \sigma(t,s)$ for each $t$.\\
    A \textbf{behavior strategy} for $\mathbf{R}$ is given by a function $\alpha: \mathcal{S}\times\mathcal{A} \rightarrow [0,1]$ such that $\sum_a \alpha(s,a)$ for each $t$.
\end{definition}

\begin{definition}[Nash Equilibrium]
    \normalfont
    Behavior strategies $\alpha$ and $\sigma$ form a \textbf{Nash equilibrium} if and only if
    \begin{enumerate}
        \item For all $t\in \mathcal{T}$,
        \begin{center}
            $\sigma(t,s)>0$ implies $\sum_a \alpha(s,a)u(t,s,a) = \max_{s'\in \mathcal{S}}\left(\sum_a \alpha(s',a)u(t,s',a)\right)$
        \end{center}
        \item For each $s\in \mathcal{S}$ such that $\sum_{t}\sigma(t,s)\rho(t)>0$,
        \begin{center}
            $\alpha(s,a)>0$ implies $\sum_{t}\mu(t;s)v(t,s,a) = \max_{a'}\sum_{t}\mu(t;s)v(t,s,a')$
        \end{center}
        where $\mu(t;s)$ is the $\mathbb{R}$'s posterior belief about $t$ given $s$, $\mu(t;s)=\frac{\sigma(t,s)\rho(t)}{\sum_{t'}\sigma(t',s)\rho(t')}$ if $\sum_t\sigma(t,s)\rho(t)>0$ and $\mu(t;s)=0$ otherwise.
    \end{enumerate}
\end{definition}

\begin{definition}[Separating \& Pooling Equilibrium]
    \normalfont
    An equilibrium $(\sigma,\alpha)$ is called a \textbf{separating} equilibrium if each type $t$ sends different signals; i.e., the set $\mathcal{S}$ can be partitioned into (disjoint) sets $\{\mathcal{S}_t; t\in \mathcal{S}\}$ such that $\sigma(t, \mathcal{S}_t) = 1$. An equilibrium $(\sigma,\alpha)$ is called a \textbf{pooling} equilibrium if there is a single signal $s^*$ that is sent by all types; i.e., $\sigma(t, s^*) = 1$ for all $t\in \mathcal{T}$.
\end{definition}


\section{Single-crossing}

\subsection{Situation over real line}
Consider the situation that $\mathcal{T},\mathcal{S},\mathcal{A}\subseteq \mathbb{R}$ and $\geq$ is the usual "greater than or equal to" relationship.

\begin{enumerate}
    \item We let $\Delta \mathcal{A}$ denote the set of probability distributions on $\mathcal{A}$.
    \item For each $s\in \mathcal{S}$ and $\mathcal{T}'\subseteq \mathcal{T}$, we let $\Delta\mathcal{A}(s,T')$ be the set of mixed strategies that are the best responses by $\mathbf{R}$ to $s\in \mathcal{S}$ for some probability distribution with support $\mathcal{T}'$.
    \item For $\alpha\in \Delta\mathcal{A}$, we write $u(t,s,\alpha)\triangleq \sum_{a\in \mathcal{A}}u(t,s,a)\alpha(a)$.
\end{enumerate}

\begin{definition}[Single-crossing]
    \normalfont
    The data of the game are said to satisfy the \textbf{single-crossing property} if the following holds: If $t\in \mathcal{T}$, $(s,\alpha)\in \mathcal{S}\times \Delta\mathcal{A}$ and $(s',\alpha')\in \mathcal{S}\times \Delta\mathcal{A}$ are such that $\alpha\in \Delta\mathcal{A}(s,\mathcal{T})$, $\alpha'\in \Delta\mathcal{A}(s',\mathcal{T})$, $s>s'$ and $u(t,s,\alpha)\geq u(t,s',\alpha')$, then for all $t'\in T$ such that $t'>t$, $u(t',s,\alpha)\geq u(t',s',\alpha')$.
\end{definition}




\chapter{Tools for Comparative Statics}
Consider the function $f:(0,2\pi) \times \mathbb{R} \rightarrow \mathbb{R}$ s.t. $$f(x,a)=\sin x+a$$
Let $X=(0,2\pi)$ and let $f_a(x) = f(x, a) = \sin x + a$ denote the perturbed function for fixed $a$.



\section{Regular and Critical Points and Values}
\subsection{Rank of Derivatives $\textnormal{Rank} df_x=\textnormal{Rank} Df(x)$}
Suppose $X \subseteq \mathbb{R}^n$ is open. Suppose $f : X \rightarrow \mathbb{R}^m$ is differentiable at $x \in X$, and let $W = \{e_1, . . . , e_n\}$ denote the standard basis of $\mathbb{R}^n$. Then $df_x \in L(\mathbb{R}^n, \mathbb{R}^m)$, and
\begin{equation}
    \begin{aligned}
        \textnormal{Rank} df_x &= \dim \textnormal{Im}(df_x)\\
        &= \dim \textnormal{span}\{df_x(e_1), . . . , df_x(e_n)\}\\
        &= \dim \textnormal{span}\{Df(x)e_1, . . . , Df(x)e_n\}\\
        &= \dim \textnormal{span}\{\textnormal{column 1 of }Df(x), . . . , \textnormal{column n of }Df(x)\}\\
        &=\textnormal{Rank} Df(x)
    \end{aligned}
    \nonumber
\end{equation}
Thus,
$$\textnormal{Rank} df_x\leq\min\{m,n\}$$
$df_x$ has \textbf{full rank} if $\textnormal{Rank} df_x=\min\{m,n\}$, that is, is $df_x$ has the maximum possible rank.

\subsection{Regular and Critical Points and Values}
\begin{definition}[Regular and Critical Points and Values]
    \normalfont
    Suppose $X \subseteq \mathbb{R}^n$ is open. Suppose $f : X \rightarrow \mathbb{R}^m$ is differentiable at $x \in X$.
    \begin{enumerate}
        \item $x$ is a \textbf{regular point} of $f$ if $\textnormal{Rank} df_x=\min\{m,n\}$.
        \item $x$ is a \textbf{critical point} of $f$ if $\textnormal{Rank} df_x<\min\{m,n\}$.
        \item $y$ is a \textbf{critical value} of $f$ if  there exists $x \in f^{-1}(y)$ such that $x$ is a critical point of $f$.
        \item $y$ is a \textbf{regular value} of $f$ if $y$ is not a critical value of $f$.
    \end{enumerate}
\end{definition}
\begin{note}
    Notice that if $y \notin f(X)$, so $f^{-1}(y) = \emptyset$, then $y$ is automatically a regular value of $f$.
\end{note}

\begin{example}
    Suppose $f(x,y)=(\sin x,\cos y)$, $Df(x,y)=\begin{bmatrix}
        \cos x&	0\\
        0&	-\sin y
    \end{bmatrix}$. Critical point: $\{(\frac{k\pi}{2}, \mathbb{R}): k\in 2\mathbb{Z}+1\}\cup\{(\mathbb{R},k\pi): k\in \mathbb{Z}\}$; Critical values: $\{(x,y): x=1\textnormal{ or }x=-1 \textnormal{ or }y=1\textnormal{ or }y=-1\}$
\end{example}


\section{Inverse and Implicit Function Theorem}
\subsection{Inverse Function Theorem}
Using Taylor's theorem to approximate
\begin{equation}
    \begin{aligned}
        f(x)=f(x_0)+Df(x_0)(x-x_0)+o(x-x_0)
    \end{aligned}
    \nonumber
\end{equation}
The requirement of "regular point" is necessary for the $Df(x_0)$ being invertible.
\begin{theorem}[Inverse Function Theorem]
    Suppose $X \subseteq \mathbb{R}^n$ is open. Suppose $f : X \rightarrow \mathbb{R}^n$ is $C^1$ on $X$, and $x_0\in X$. If $\det Df(x_0)\neq 0$ (i.e., $x_0$ is a regular point of $f$), then there are open neighborhoods $U$ of $x_0$ and $V$ of $f(x_0)$ s.t.
    \begin{equation}
        \begin{aligned}
            &f:U \rightarrow V \textnormal{ is bijective (on-to-on and onto)}\\
            &\exists\ f^{-1}:V \rightarrow U \textnormal{ is }C^1\\
            &Df^{-1}(f(x_0))=[Df(x_0)]^{-1}\\
            &\textnormal{(In $\mathbb{R}$, $(f^{-1})'(f(x_0))=(f'(x_0))^{-1}$)}
        \end{aligned}
        \nonumber
    \end{equation}
    If in addition $f \in C^k$, then $f^{-1} \in C^k$.
\end{theorem}

\subsection{Implicit Function Theorem}
Using Taylor's theorem to approximate
\begin{equation}
    \begin{aligned}
        f(x,a)=f(x_0,a_0)+Df(x_0,a_0)(x-x_0)+Df(x_0,a_0)(a-a_0)+\textnormal{remainder}
    \end{aligned}
    \nonumber
\end{equation}
The requirement of "regular point" is necessary for the $Df(x_0,a_0)$ being invertible.

We want to know how the function $x^*(a)$ changes with keeping $f(x^*,a)=0$.
\begin{theorem}[Implicit Function Theorem]
    Suppose $X \subseteq \mathbb{R}^n$ and $A \subseteq \mathbb{R}^p$ are open and $f : X \times A \rightarrow \mathbb{R}^n$ is $C^1$. Suppose $f(x_0, a_0) = 0$ and $\det(D_x f(x_0, a_0)) \neq 0$, i.e. $x_0$ is a regular point of $f(\cdot, a_0)$. Then there are open neighborhoods $U$ of $x_0$ ($U \subseteq X$) and $W$ of $a_0$ such that
    $$\forall a\in W,\ \exists ! x\in U \textnormal{ s.t. }f(x,a)=0$$
    For each $a \in W$ let $g(a)$ be that unique $x$. Then $g : W \rightarrow U$ is $C^1$ and $$Dg(a_0)=-[D_x f(x_0, a_0)]^{-1}[D_a f(x_0, a_0)]$$
    If in addition $f \in C^k$, then $g \in C^k$.
\end{theorem}
\subsection{Prove Implicit Function Theorem Given Inverse Function Theorem}
\begin{proof}
    \begin{enumerate}
        \item Firstly, we prove "$g$ is differentiable":
        The "change of $a$" incurs the value change:
        \begin{equation}
            \begin{aligned}
                f(x_0,a_0+h)&=f(x_0,a_0)+D_af(x_0,a_0)h+o(h)\\
                &=D_af(x_0,a_0)h+o(h)\\
            \end{aligned}
            \nonumber
        \end{equation}
        Find a $\Delta x$ such that the new $x$ can let the value go back to $0$, i.e., $f(x_0+\Delta x,a_0+h)=0$. That is,
        \begin{equation}
            \begin{aligned}
                g(a_0+h)=x_0+\Delta x
            \end{aligned}
            \nonumber
        \end{equation}
        To prove "$g$ is differentiable", we want to prove "$\exists T\in L(A,X)$ s.t. $\Delta x=T(h)+o(h)$"
        \begin{equation}
            \begin{aligned}
                0&=f(x_0+\Delta x,a_0+h)\\
                &=f(x_0,a_0)+D_xf(x_0,a_0+h)\Delta x+ D_a f(x_0,a_0)h+o(\Delta x)+o(h)\\
                &=D_xf(x_0,a_0+h)\Delta x+ D_a f(x_0,a_0)h+o(\Delta x)+o(h)\\
                D_x f(x_0,a_0+h)\Delta x&=-D_a f(x_0,a_0)h+o(\Delta x)+o(h)
            \end{aligned}
            \nonumber
        \end{equation}
        Because $f$ is $C^1$ and the determinant is a continuous function of the entries of the matrix, $\det D_xf(x_0, a_0 + h) \neq 0$ for $h$ sufficiently small, so
        \begin{equation}
            \begin{aligned}
                \Delta x&= -[D_x f(x_0,a_0+h)]^{-1}D_a f(x_0,a_0)h+o(\Delta x)+o(h)\\
                \textnormal{Since $f\in C^1$, }\Delta x&= -[D_x f(x_0,a_0)+o(1)]^{-1}D_a f(x_0,a_0)h+o(\Delta x)+o(h)\\
                \textnormal{Since $f\in C^1$, }\Delta x&= -[D_x f(x_0,a_0)]^{-1}D_a f(x_0,a_0)h+o(\Delta x)+o(h)
            \end{aligned}
            \nonumber
        \end{equation}
        Hence, "$g$ is differentiable" is proved and the derivative of $g$ is $Dg(a_0)=-[D_x f(x_0,a_0)]^{-1}[D_a f(x_0,a_0)]$.
        \item Secondly, given the "$g$ is differentiable", we can also compute the derivative by
        \begin{equation}
            \begin{aligned}
                Df(g(a),a)(a_0)&=0\\
                D_xf(x_0,a_0)Dg(a_0)+D_af(x_0,a_0)&=0\\
                Dg(a_0)&=-[D_x f(x_0,a_0)]^{-1}D_a f(x_0,a_0)
            \end{aligned}
            \nonumber
        \end{equation}
    \end{enumerate}
\end{proof}







\begin{example}
    $f: \mathbb{R}^3 \rightarrow \mathbb{R}^2$, $f((3,-1,2))=(0,0)$, $Df(3,-1,2)=\begin{bmatrix}
        1&2&1\\
        1&-1&1
    \end{bmatrix}$. Then,
    let $(x_0,a_0)=(3,-1,2)$, where $x_0=3$ and $a_0=(-1,2)$. Or, we can let $(x_0,a_0)=(3,-1,2)$, where $x_0=(3,-1)$ and $a_0=2$.
\end{example}

\subsection{Prove Inverse Function Theorem Given Implicit Function Theorem}
\begin{proof}[Prove Inverse Function Theorem Given Implicit Function Theorem]
    Define $F:X\times \mathbb{R}^n$ s.t. $F(x,y)=y-f(x)$. Let $y_0=f(x_0)$.
    \begin{equation}
        \begin{aligned}
            D_x F(x,y)=-Df(x),\ D_y F(x,y)=I_{n\times n}
        \end{aligned}
        \nonumber
    \end{equation}
    According to the implicit function theorem, there are open sets $U \subseteq X$ and $V \subseteq \mathbb{R}^n$ such that $x_0 \in U$, $y_0 \in V$ and a function $g : V \rightarrow U$ differentiable at $y_0$ such that $F(g(y), y) = 0$ for all $y \in V$. So, $0=F(g(y),y)=y-f(g(y))$, we have $f(g(y))=y$, that is $g=f^{-1}$.
    $f: U \rightarrow V$ is bijective because it has inverse $g : V \rightarrow U$.

    By the implicit function theorem, $g(y)$ is differentiable and
    \begin{equation}
        \begin{aligned}
            Df^{-1}(y_0)=Dg(y_0)=-[D_x F(x_0,y_0)]^{-1}[D_y F(x_0,y_0)]=[Df(x_0)]^{-1}
        \end{aligned}
        \nonumber
    \end{equation}
    where $y_0=f(x_0)$.
    
    By the implicit function theorem, the $g=f^{-1}$ is $C^k$ if $f$ is $C^k$.

    All in all, the inverse function theorem is proved.
\end{proof}

\subsection{Example: Using Implicit Function Theorem}
%Consider $u_1(a)=ba-ka^2$, where $a^*=\frac{b}{2k}$.

%More general, $U(a)=b(a)-c(a)$. $a^*$ solve $b'(a)-c'(a)=0$.

$x^2+y^2=c$. Define $g(x,y)=x^2+y^2-c$. The optimal solution of $y$ given $x$ is represented by $y^*(x)$. By the implicit function theorem,
\begin{equation}
    \begin{aligned}
        \frac{\partial y^*}{\partial x}=-\frac{\frac{\partial g}{\partial x}\big|_{x,y^*}}{\frac{\partial g}{\partial y}\big|_{x,y^*}}
    \end{aligned}
    \nonumber
\end{equation}


\begin{example}
    Let us consider a firm that produces a good $y$; it uses two inputs $x_1$ and $x_2$. The firm sells the output and acquires the inputs in competitive markets: The market price of $y$ is $p$, and the cost of each unit of $x_1$ and $x_2$ are $w_1$ and $w_2$ respectively. Its technology is given by $f : \mathbb{R}^2_+ \rightarrow \mathbb{R}_+$, where $f (x_1, x_2) = x_1^ax_2^b$, $a + b < 1$. Its profits take the form
    \begin{equation}
        \begin{aligned}
            \pi(x_1,x_2; p, w_1,w_2)=p x_1^ax_2^b-w_1x_1-w_2x_2
        \end{aligned}
        \nonumber
    \end{equation}
    The firm selects $x_1$ and $x_2$ in order to maximize profits. \textbf{We aim to know how its choice of $x_1$ and $x_2$ is affected by a change in $w_1$}.

    Assuming an interior solution, the first-order conditions of this optimization problem are
    \begin{equation}
        \begin{aligned}
            \frac{\partial \pi}{\partial x_1}(x_1^*,x_2^*;p,w_1,w_2)=pa(x_1^*)^{a-1}(x_2^*)^b-w_1=0\\
            \frac{\partial \pi}{\partial x_2}(x_1^*,x_2^*;p,w_1,w_2)=pb(x_1^*)^{a}(x_2^*)^{b-1}-w_2=0
        \end{aligned}
        \nonumber
    \end{equation}
    for some $(x_1, x_2) = (x_1^*,x_2^*)$.

    Let us define
    \begin{equation}
        \begin{aligned}
            F(x_1^*,x_2^*;p,w_1,w_2)=
            \begin{bmatrix}
                pa(x_1^*)^{a-1}(x_2^*)^b-w_1\\
                pb(x_1^*)^{a}(x_2^*)^{b-1}-w_1
            \end{bmatrix}
        \end{aligned}
        \nonumber
    \end{equation}
    Jacobian matrices are
    \begin{equation}
        \begin{aligned}
            D_{(x_1,x_2)}F(x_1^*,x_2^*;p,w_1,w_2)=\begin{bmatrix}
                pa(a-1)(x_1^*)^{a-2}(x_2^*)^b&pab(x_1^*)^{a-1}(x_2^*)^{b-1}\\
                pab(x_1^*)^{a-1}(x_2^*)^{b-1}&pb(b-1)(x_1^*)^{a}(x_2^*)^{b-2}
            \end{bmatrix}
        \end{aligned}
        \nonumber
    \end{equation}
    \begin{equation}
        \begin{aligned}
            D_{w_1}F(x_1^*,x_2^*;p,w_1,w_2)
            &=
            \begin{bmatrix}
                -1\\
                0
            \end{bmatrix}
        \end{aligned}
        \nonumber
    \end{equation}
    By the implicit function theorem, we can get
    \begin{equation}
        \begin{aligned}
            \begin{bmatrix}
                \frac{\partial x_1^*}{\partial w_1}\\
                \frac{\partial x_2^*}{\partial w_1}
            \end{bmatrix}&=-[D_{(x_1,x_2)}F(x_1^*,x_2^*;p,w_1,w_2)]^{-1}[D_{w_1}F(x_1^*,x_2^*;p,w_1,w_2)]\\
            &=[D_{(x_1,x_2)}F(x_1^*,x_2^*;p,w_1,w_2)]^{-1}\begin{bmatrix}
                1\\
                0
            \end{bmatrix}
        \end{aligned}
        \nonumber
    \end{equation}
\end{example}

\subsection{Corollary: $a \rightarrow \{x\in X: f(x,a)=0\}$ is lhc}
\begin{corollary}
    Suppose $X \subseteq \mathbb{R}^n$ and $A \subseteq \mathbb{R}^p$ are open and $f : X \times A \rightarrow \mathbb{R}^n$ is $C^1$. If $0$ is a regular value of $f(\cdot, a_0)$, then the correspondence $$a \rightarrow \{x\in X: f(x,a)=0\}$$
    is \textbf{lower hemicontinuous} at $a_0$.
\end{corollary}

\section{Transversality and Genericity}
\subsection{Lebesgue Measure Zero}
\begin{definition}[Lebesgue Measure Zero]
    \normalfont
    Suppose $A \subseteq \mathbb{R}^n$. $A$ has \textbf{Lebesgue measure zero} if for every $\varepsilon > 0$ there is a countable collection of rectangles $I_1, I_2, . . .$ such that
    \begin{equation}
        \begin{aligned}
            \sum_{k=1}^\infty\textnormal{Vol}(I_k)<\varepsilon \textnormal{ and }A\subseteq \cup_{k=1}^\infty I_k
        \end{aligned}
        \nonumber
    \end{equation}
    Here by a rectangle we mean $I_k = \times^n_{j=1}(a^k_j , b^k_j)=\{x\in \mathbb{R}^n: x_j\in (a^k_j , b^k_j), \forall j\}$ for some $a^k_j < b^k_j \in \mathbb{R}$, and
    \begin{equation}
        \begin{aligned}
            \textnormal{Vol}(I_k)=\prod_{j=1}^n|b^k_j-a^k_j|
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}

\begin{example}
    \begin{enumerate}
        \item “Lower-dimensional” sets have Lebesgue measure zero. For example, $A=\{x\in\mathbb{R}^2:x_2=0\}$
        \item Any \textbf{finite} set has Lebesgue measure zero in $\mathbb{R}^n$.
        \item \textbf{Finite Union} of sets that have Lebesgue measure zero has Lebesgue measure zero: If $A_n$ has Lebesgue measure zero $\forall n$ then $\cup_{n\in N}A_n$ has Lebesgue measure zero.
        \item Every \textbf{countable} set (e.g. $\mathbb{Q}$) has Lebesgue measure zero.
        \item No open set in $\mathbb{R}^n$ has Lebesgue measure zero.
    \end{enumerate}
\end{example}


\subsection{Sard's Theorem}
\begin{theorem}[Sard's Theorem]
    Let $X \subseteq \mathbb{R}^n$ be open, and $f : X \rightarrow \mathbb{R}^m$ be $C^r$ with $r \geq 1 + max\{0, n - m\}$. Then the set of all critical values of $f$ has Lebesgue measure zero.
\end{theorem}

\subsection{Transversality Theorem}
\begin{theorem}[Transversality Theorem]
    Let $X \subseteq \mathbb{R}^n$ and $A \subseteq \mathbb{R}^p$ be open, and $f : X \times A \rightarrow \mathbb{R}^m$ be $C^r$ with $r \geq 1 + max\{0, n - m\}$. Suppose that $0$ is a regular value of $f$ (that is all $(x,a)$ such that $f(x,a)=0$ are regular points). Then,
    \begin{enumerate}
        \item $\exists A_0 \subseteq A$ such that $A \backslash A_0$ has Lebesgue measure zero.
        \item $\forall a \in A_0$, $0$ is a regular value of $f_a = f(\cdot, a)$.
    \end{enumerate}
\end{theorem}
\begin{example}
    $f: \mathbb{R}^4 \rightarrow \mathbb{R}^3$ s.t. $f(x,y,z,w)=(g(x)+y,z^3+1,w+x+y^2)$
\end{example}


\chapter{Fixed Point Theorem}

\section{Contraction Mapping Theorem \small{(@ Lec 05 of ECON 204)}}
\subsection{Contraction: Lipschitz continuous with constant $<1$}
\begin{definition}
    \normalfont
    Let $(X, d)$ be a \underline{nonempty complete} metric space. An operator is a function $T : X \rightarrow X$. An operator $T$ is a \textbf{contraction of modulus $\beta$} if $\beta < 1$ and $$d(T(x), T(y)) \leq \beta d(x, y), \forall x,y\in X$$
\end{definition}
A contraction shrinks distances by a \textit{uniform} factor $\beta < 1$.

\subsection{Theorem: Contraction $\Rightarrow$ Uniformly Continuous}
\begin{theorem}[Contraction $\Rightarrow$ Uniformly Continuous]
    Every contraction is uniformly continuous.
\end{theorem}
\begin{proof}
    Let $\delta=\frac{\varepsilon}{\beta}$.
\end{proof}

\subsection{Blackwell's Sufficient Conditions for Contraction}
Let $X$ be a set, and let $B(X)$ be the set of all bounded functions from $X$ to $\mathbb{R}$. Then $(B(X), \|\cdot\|_\infty)$ is a normed vector space.

(Notice that below we use shorthand notation that identifies a constant function with its constant value in $\mathbb{R}$, that is, we write interchangeably $a \in \mathbb{R}$ and $a : X \rightarrow \mathbb{R}$ to denote the function such that $a(x) = a, \forall x \in X$.)

\begin{theorem}[Blackwell's Sufficient Conditions]
    Consider $B(X)$ with the sup norm $\|\cdot\|_\infty$. Let $T : B(X) \rightarrow B(X)$ be an operator satisfying
    \begin{enumerate}
        \item (monotonicity) $f(x) \leq g(x), \forall x\in X \Rightarrow (T f)(x) \leq (T g)(x), \forall x \in X$
        \item (discounting) $\exists \beta \in (0, 1)$ such that for every $a \geq 0$ and $x \in X$, $$(T (f + a)) (x) \leq (T f)(x) + \beta a$$
    \end{enumerate}
    Then $T$ is a contraction with modulus $\beta$.
\end{theorem}
\begin{proof}
    Fix $f, g \in B(X)$. By the definition of the sup norm,
    $$
    f(x) \leq g(x)+\|f-g\|_{\infty} \forall x \in X
    $$
    Then
    $$
    (T f)(x) \leq\left(T\left(g+\|f-g\|_{\infty}\right)\right)(x) \leq(T g)(x)+\beta\|f-g\|_{\infty} \quad \forall x \in X
    $$
    where the first inequality above follows from monotonicity, and the second from discounting. Thus
    $$
    (T f)(x)-(T g)(x) \leq \beta\|f-g\|_{\infty} \quad \forall x \in X
    $$
    Reversing the roles of $f$ and $g$ above gives
    $$
    (T g)(x)-(T f)(x) \leq \beta\|f-g\|_{\infty} \quad \forall x \in X
    $$
    Thus
    $$
    \|T(f)-T(g)\|_{\infty} \leq \beta\|f-g\|_{\infty}
    $$
    Thus $T$ is a contraction with modulus $\beta$
\end{proof}


\section{Fixed Point Theorem \small{(@ Lec 05 of ECON 204)}}
\subsection{Fixed Point}
\begin{definition}[Fixed Point]
    \normalfont
    A \textbf{fixed point} of an operator $T$ is element $x^*\in X$ such that $T(x^*)=x^*$.
\end{definition}

\begin{definition}[Fixed Point of Function]
    \normalfont
    Let $X$ be a nonempty set and $f : X \rightarrow X$. A point $x^* \in X$ is a \textbf{fixed point} of $f$ if $f(x^*) = x^*$.
\end{definition}

\begin{example}
    Let $X=\mathbb{R}$ and $f: \mathbb{R} \rightarrow \mathbb{R}$
    \begin{enumerate}
        \item $f(x)=2x$ has fixed point: $x=0$.
        \item $f(x)=x$ has fixed points: $x\in \mathbb{R}$.
        \item $f(x)=x+1$ doesn't have fixed points.
    \end{enumerate}
\end{example}

\subsection{$\bigstar$ Contraction Mapping Theorem: contraction $\Rightarrow$ exist unique fixed point}
\begin{theorem}[Contraction Mapping Theorem]
    Let $(X, d)$ be a nonempty complete metric space and $T : X \rightarrow X$ a contraction with modulus $\beta < 1$. Then
    \begin{enumerate}
        \item $T$ has a unique fixed point $x^*$.
        \item For every $x_0 \in X$, the sequence defined by
        \begin{equation}
            \begin{aligned}
                x_1&=T(x_0)\\
                x_2&=T(x_1)=T(T(x_0))=T^2(x_0)\\
                &\vdots\\
                x_{n+1}&=T(x_n)=T^{n+1}(x_0)
            \end{aligned}
            \nonumber
        \end{equation}
        converges to $x^*$.
    \end{enumerate}
\end{theorem}
Note that the theorem asserts both the \textbf{existence} and \textbf{uniqueness} of the fixed point, as well as giving an \textbf{algorithm} to find the fixed point of a contraction.
\begin{proof}
    Define the sequence $\{x_n\}$ as above. Then,
    \begin{equation}
        \begin{aligned}
            d(x_{n+1},x_n)&=d(T(x_n),T(x_{n-1}))\\
            &\leq \beta d(x_n,x_{n-1})\\
            &\leq \beta^n d(x_1,x_0)
        \end{aligned}
        \nonumber
    \end{equation}
    Then for any $n>m$,
    \begin{equation}
        \begin{aligned}
            d(x_n,x_m)&\leq d(x_1, x_0) \sum_{i=m}^{n-1}\beta^i\\
            &<d(x_1, x_0) \sum_{i=m}^{\infty}\beta^i\\
            &=\frac{\beta^m}{1-\beta}d(x_1, x_0) \rightarrow 0 \textnormal{ as }m \rightarrow \infty
        \end{aligned}
        \nonumber
    \end{equation}
    Fixed $\varepsilon>0$, we can choose $N(\varepsilon)$ such that $\forall n,m>N(\varepsilon)$, $$d(x_n,x_m)<\frac{\beta^m}{1-\beta}d(x_1, x_0) <\varepsilon$$
    Therefore, $\{x_n\}$ is Cauchy. Since $(X, d)$ is complete, $x_n \rightarrow x^*$ for some $x^*\in X$.

    Next we show that $x^*$ is a fixed point of $T$.
    $$
    \begin{aligned}
    T\left(x^*\right) & =T\left(\lim _{n \rightarrow \infty} x_n\right) \\
    & =\lim _{n \rightarrow \infty} T\left(x_n\right) \text { since } T \text { is continuous } \\
    & =\lim _{n \rightarrow \infty} x_{n+1} \\
    & =x^*
    \end{aligned}
    $$
    so $x^*$ is a fixed point of $T$.
    
    Finally, we show that there is at most one fixed point. Suppose $x^*$ and $y^*$ are both fixed points of $T$, so $T\left(x^*\right)=x^*$ and $T\left(y^*\right)=y^*$. Then
    $$
    \begin{aligned}
    d\left(x^*, y^*\right) & =d\left(T\left(x^*\right), T\left(y^*\right)\right) \\
    & \leq \beta d\left(x^*, y^*\right) \\
    \Rightarrow(1-\beta) d\left(x^*, y^*\right) & \leq 0 \\
    \Rightarrow d\left(x^*, y^*\right) & \leq 0
    \end{aligned}
    $$
    So $d\left(x^*, y^*\right)=0$, which implies $x^*=y^*$.
\end{proof}



\subsection{Conditions for Fixed Point's Continuous Dependence on Parameters}
\begin{theorem}[Continuous Dependence on Parameters]
    Let $(X, d)$ and $(\Omega, \rho)$ be two metric spaces and $T : X \times \Omega \rightarrow X$. For each parameter $\omega \in \Omega$ let $T_\omega : X \rightarrow X$ be defined by $T_\omega(x)=T(x,\omega)$.

    Suppose (1). $(X, d)$ is complete, (2). $T$ is continuous in $\omega$ (that is $T(x, \cdot) : \Omega \rightarrow X$ is continuous for each $x \in X$), and (3). $\exists \beta < 1$ such that $T_\omega$ is a contraction of modulus $\beta$ $\forall \omega \in \Omega$.
    
    Then the fixed point function (about parameter $\omega$) $x^*: \Omega \rightarrow X$ defined by $x^*(\omega)=T_\omega(x^*(\omega))$ is continuous.
\end{theorem}

\section{Brouwer's Fixed Point Theorem \small{(@ Lec 13 of ECON 204)}}
\subsection{Simple One: One-dimension}
\begin{theorem}
    Let $X = [a, b]$ for $a, b \in \mathbb{R}$ with $a < b$ and let $f : X \rightarrow X$ be continuous. Then $f$ has a fixed point.
\end{theorem}
\begin{proof}
    Easily proved by Intermediate Value Theorem.
\end{proof}

\subsection{$\bigstar$ Brouwer's Fixed Point Theorem: continuous function has fixed point over compact, convex set}
\begin{theorem}[Brouwer's Fixed Point Theorem]
    Let $X \subseteq \mathbb{R}^n$ be nonempty, \textbf{compact}, and \textbf{convex}, and let $f : X \rightarrow X$ be continuous. Then $f$ has a fixed point.
\end{theorem}
\begin{proof}
    \normalfont
    Consider the case when the set $X$ is the unit ball in $\mathbb{R}^n$.

    Using a fact that "Let $B$ be the unit ball in $\mathbb{R}^n$. Then there is no continuous function $h : B \rightarrow \partial B$ such that $h(x_0) = x_0$ for every $x_0 \in \partial B$", which is intuitive but hard to prove. (See \textit{J. Franklin, Methods of Mathematical Economics}, for an elementary (but long) proof.)

    Then prove by contradiction: suppose $f$ has no fixed points in $B$. That is, $\forall x\in B$, $x\neq f(x)$. Since x and its image $f(x)$ are distinct points in $B$ for every $x$, we can carry out the following construction. For each $x \in B$, construct the line segment originating at $f(x)$ and going through $x$. Let $g(x)$ denote the intersection of this line segment with $\partial B$. This construction gives a continuous function $g : B \rightarrow \partial B$. Furthermore, notice that if $x_0 \in \partial B$, then $x_0 = g(x_0)$. Then, $g$ gives $g(x)=x,\forall x\in \partial B$. Since there are no such functions by the fact above, we have a contradiction.
\end{proof}




\chapter{Correspondence: $\Psi : X \rightarrow 2^Y$ \small{(@ Lec 07 of ECON 204)}}
\begin{definition}[Correspondence]
    \normalfont
    A \textbf{correspondence} $\Psi : X \rightarrow 2^Y$ from $X$ to $Y$ is a function from $X$ to $2^Y$, that is, $\Psi(x) \subseteq Y$ for every $x \in X$. ($2^Y$ is the set of all subsets of $Y$)
\end{definition}
\begin{example}
Let $u : \mathbb{R}_+^n \rightarrow \mathbb{R}$ be a continuous utility function, $y > 0$ and $p \in \mathbb{R}_{++}^n$, that is, $p_i > 0$ for each $i$. Define $\Psi : \mathbb{R}_{++}^n \times \mathbb{R}_{++} \rightarrow 2^{\mathbb{R}_{+}^n}$ by
\begin{equation}
    \begin{aligned}
        \Psi(p,y)=&\argmax u(x)\\
        \textnormal{s.t. }&x\geq 0\\
        &p\cdot x\leq y
    \end{aligned}
    \nonumber
\end{equation}
$\Psi$ is the demand correspondence associated with the utility function $u$; typically $\Psi(p, y)$ is multi-valued.
\end{example}

\section{Continuity of Correspondences}
\subsection{Upper/Lower Hemicontinuous}
Let $X\subseteq \mathbb{E}^n$, $Y\subseteq \mathbb{E}^m$, and $\Psi: X \rightarrow 2^Y$.
\begin{definition}[Upper Hemicontinuous]
    \normalfont
    $\Psi$ is \textbf{upper hemicontinuous} (uhc) at $x_0 \in X$ if, for every \underline{open set} $V$ with $\Psi(x_0)\subseteq V$, there is an \underline{open set} $U$ with $x_0 \in U$ s.t.
    $$x\in U \Rightarrow \Psi(x)\subseteq V$$
\end{definition}
Upper hemicontinuity reflects the requirement that $\Psi$ doesn't “jump down/implode in the limit” at $x_0$. \textit{(A set to “jump down” at the limit $x_0$: It should mean the set suddenly gets smaller -- it “implodes in the limit” -- that is, there is a sequence $x_n \rightarrow x_0$ and points $y_n \in \Psi(x_n)$ that are far from every point of $\Psi(x_0)$ as $n \rightarrow \infty$.)}
\begin{definition}[Lower Hemicontinuous]
    \normalfont
    $\Psi$ is \textbf{lower hemicontinuous} (lhc) at $x_0 \in X$ if, for every \underline{open set} $V$ with $\Psi(x_0)\cap V \neq \emptyset$, there is an \underline{open set} $U$ with $x_0 \in U$ s.t.
    $$x\in U \Rightarrow \Psi(x)\cap V\neq \emptyset$$
\end{definition}
Lower hemicontinuity reflects the requirement that $\Psi$ doesn't “jump up/explode in the limit” at $x_0$. \textit{(A set to “jump up” at the limit $x_0$: It should mean that the set suddenly gets bigger -- it “explodes in the limit” -- that is, there is a sequence $x_n \rightarrow x_0$ and a point $y_0\in\Psi(x_0)$ that is far from every point of $\Psi(x_n)$ as $n \rightarrow \infty$.)}

\begin{definition}[Continuous Correspondence]
    \normalfont
    $\Psi$ is \textbf{continuous} at $x_0 \in X$ if it is both \textbf{uhc} and \textbf{lhc} at $x_0$.
\end{definition}

\begin{proposition}
    $\Psi$ is upper hemicontinuous (respectively lower hemicontinuous, continuous) if it is uhc (respectively lhc, continuous) at every $x \in X$.
\end{proposition}

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{uhc.png}
    \caption{The correspondence $\Psi$ “implodes in the limit” at $x_0$. $\Psi$ is not upper hemicontinuous at $x_0$.}
    \label{}
\end{figure}\end{center}

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{lhc.png}
    \caption{The correspondence $\Psi$ “explodes in the limit” at $x_0$. $\Psi$ is not lower hemicontinuous at $x_0$.}
    \label{}
\end{figure}\end{center}

\subsection{Theorem: $\Psi(x)=\{f(x)\}$ is uhc $\Leftrightarrow$ $f$ is continuous}
\begin{theorem}[$\Psi(x)=\{f(x)\}$ is uhc $\Leftrightarrow$ $f$ is continuous]
    Let $X \subseteq \mathbb{E}^n$, $Y \subseteq \mathbb{E}^m$ and $f : X \rightarrow Y$. Let $\Psi : X \rightarrow  2^Y$ be defined by $\Psi(x) = \{f(x)\}$ for all $x \in X$. Then $\Psi$ is \textbf{uhc} \underline{if and only if} $f$ is \textbf{continuous}.
\end{theorem}


\subsection{Berge's Maximum Theorem: the set of maximizers is uhc with non-empty compact values}
\begin{theorem}[Berge's Maximum Theorem]\label{thm:Berge's Maximum Theorem}
    Let $X \subseteq \mathbb{R}^n$ and $Y \subseteq \mathbb{R}^m$. Consider the function $f : X \times Y \rightarrow \mathbb{R}$ and the correspondence $\Gamma : Y \rightarrow 2^X$. Define $v(y) = \max_{x\in\Gamma(y)} f(x, y)$ and the set of maximizers $$\Omega(y) = \argmax_{x\in\Gamma(y)} f(x, y)=\{x:f(x,y)=v(y)\}$$
    Suppose $f$ and $\Gamma$ are continuous, and that $\Gamma$ has non-empty compact values. Then, $v$ is continuous and $\Omega$ is uhc with non-empty compact values.
\end{theorem}




\section{Graph of Correspondence}
An alternative notion of continuity looks instead at properties of the graph of the correspondence.
\begin{definition}[Graph of Correspondence]
    \normalfont
    The \textbf{graph} of a correspondence $\Psi : X \rightarrow 2^Y$ is the set
    $$\textnormal{graph}\Psi=\{(x,y)\in X\times Y:y\in\Psi(x)\}$$
\end{definition}

\subsection{Closed Graph}
By the definition of continuous function $f:\mathbb{R}^n \rightarrow \mathbb{R}$,  each convergent sequence $\{(x_n, y_n)\}$ in graph $f$ converges to a point $(x, y)$ in graph $f$, that is, graph $f$ is closed.

\begin{definition}[Closed Graph]
    \normalfont
    Let $X\subseteq \mathbb{E}^n$, $Y\subseteq \mathbb{E}^m$. A correspondence $\Psi: X \rightarrow 2^Y$ has closed graph if its graph is a closed subset of $X \times Y$, that is, if for any sequences $\{x_n\} \subseteq X$ and $\{y_n\} \subseteq Y$ such that $x_n \rightarrow x \in X$, $y_n \rightarrow y \in Y$ and $y_n \in \Psi(x_n)$ for each $n$, then $y \in \Psi(x)$.
\end{definition}
\begin{example}
    Consider the correspondence $\Psi(x)=\left\{\begin{matrix}
        \{\frac{1}{x}\},&\textnormal{ if }x\in(0,1]\\
        \{0\},&\textnormal{ if }x=0
    \end{matrix}\right.$ ("implode in the limit")\\
    Let $V = (-0.1, 0.1)$. Then $\Psi(0) = \{0\} \subseteq V$, but no matter how close $x$ is to $0$, $\Psi(x)=\{\frac{1}{x}\}\nsubseteq V$, so $\Psi$ is not uhc at $0$. However, note that $\Psi$ has closed graph.
\end{example}

\section{Closed-valued, Compact-valued, and Convex-valued Correspondences}
\begin{definition}[Closed-valued, Compact-valued, and Convex-valued Correspondences]
    \normalfont
    Given a correspondence $\Psi : X \rightarrow 2^Y$,
    \begin{enumerate}
        \item $\Psi$ is \textbf{closed-valued} if $\Psi(x)$ is a closed subset of $Y$ for all $x$;
        \item $\Psi$ is \textbf{compact-valued} if $\Psi(x)$ is compact for all $x$.
        \item $\Psi$ is \textbf{convex-valued} if $\Psi(x)$ is convex for all $x$.
    \end{enumerate}
\end{definition}

\subsection{Closed-valued, uhc and Closed Graph}
For closed-valued correspondences these concepts can be more tightly connected. A closed-valued and upper hemicontinuous correspondence must have closed graph. For a closed-valued correspondence with a compact range, upper hemicontinuity is equivalent to closed graph.

\begin{theorem}
    Let $X\subseteq \mathbb{E}^n$, $Y\subseteq \mathbb{E}^m$, and $\Psi: X \rightarrow 2^Y$.
    \begin{enumerate}
        \item $\Psi$ is \textbf{closed-valued} and \textbf{uhc} $\Rightarrow$ $\Psi$ has \textbf{closed graph}.
        \item $\Psi$ is \textbf{closed-valued} and \textbf{uhc} $\Leftarrow$ $\Psi$ has \textbf{closed graph}. (If $Y$ is \textbf{compact})
    \end{enumerate}
\end{theorem}

\begin{theorem}
    Let $X\subseteq \mathbb{E}^n$, $Y\subseteq \mathbb{E}^m$, and $\Psi: X \rightarrow 2^Y$. If $\Psi$ has \textbf{closed graph} and there is an \textbf{open set} $W$ with $x_0 \in W$ and a \textbf{compact set} $Z$ such that $x \in W \cap X \Rightarrow \Psi(x) \subseteq Z$, then $\Psi$ is \textbf{uhc} at $x_0$.
\end{theorem}



\subsection{Theorem: compact-valued, uhs correspondence of compact set is compact}
\begin{theorem}\label{thm:compact-valued, uhs correspondence of compact set is compact}
    Let $X$ be a compact set and $\Psi : X \rightarrow 2^X$ be a non-empty, compact-valued upper-hemicontinuous correspondence. If $C \subseteq X$ is compact, then $\Psi(C)$ is compact.
\end{theorem}
\begin{proof}
    Given the compact-valued $\Psi$, we can have an open cover of $\Psi(C)$, $\{U_\lambda:\lambda\in\Lambda\}$. So $\forall x\in C$, there exists $U_{l(x)},l(x)\in\Lambda$ such that $U_{l(x)}$ is an open cover of $\Psi(x)$.

    Consider a $c\in C$. Since $\Psi$ is uhs and $\Psi(c)\subseteq U_{l(c)}$, there exists open set $V_c$ s.t. $c\in V_c$ and $\Psi(x)\subseteq U_{l(c)}, \forall x\in V_c\cap C$.

    $\{V_c:c\in C\}$ is an open cover of $C$. Because $C$ is compact, there is a finite subcover $\{V_{c_i}: i=1,...,m\},m\in \mathbb{N}$, where $\{c_i:i=1,...,m\}\subseteq C$.

    Because $\Psi(x)\subseteq U_{l(c_i)}, \forall x\in V_{c_i}\cap C$ and $\{V_{c_i}: i=1,...,m\},m\in \mathbb{N}$ is a open cover for $C$, we can infer $\{U_{l(c_i)}:i=1,...,m\}$ is a finite subcover of $\{U_{l(c)}:c\in C\}$ for $\Psi(C)$. Hence, $\Psi(C)$ is compact.
\end{proof}

\section{Fixed Points for Correspondences \small{(@ Lec 13 of ECON 204)}}
\subsection{Definition}
\begin{definition}[Fixed Points for Correspondences]
    \normalfont
    Let $X$ be nonempty and $\psi : X \rightarrow 2^X$ be a correspondence. A point $x^* \in X$ is a fixed point of $\psi$ if $x^* \in \psi(x^*)$.
\end{definition}
\begin{note}
    We only need $x^*$ to be in $\psi(x^*)$, not $\{x^*\} = \psi(x^*)$. That is, $\psi$ need not be single-valued at $x^*$. So $x^*$ can be a fixed point of $\psi$ but there may be other elements of $\psi(x^*)$ different from $x^*$.
\end{note}



\subsection{Kakutani's Fixed Point Theorem: uhs, compact, convex values correspondence has a fixed point over compact convex set}
\begin{theorem}[Kakutani's Fixed Point Theorem]\label{thm:Kakutani's Fixed Point Theorem}
    Let $X \subseteq \mathbb{R}^n$ be a non-empty, \textbf{compact}, \textbf{convex} set and $\psi : X \rightarrow 2^X$ be an \textbf{upper hemi-continuous} correspondence with non-empty, \textbf{compact}, \textbf{convex} values. Then $\psi$ has a fixed point in $X$.
\end{theorem}


\subsection{Theorem: $\exists$ compact set $C = \cap_{i=0}^\infty \Psi^i(X)$ s.t. $\Psi(C)=C$}
\begin{theorem}
    Let $(X, d)$ be a compact metric space and let $\Psi(x) : X \rightarrow 2^X$ be a upper-hemicontinuous, compact-valued correspondence, such that $\Psi(x)$ is non-empty for every $x \in X$. There exists a compact non-empty subset $C\subseteq X$, such that $\Psi(C) \equiv \cup_{x\in C}\Psi(x) = C$.
\end{theorem}
\begin{proof}
    Let's construct a sequence $\{C_n\}$ such that $C_0=X$, $C_1=\Psi(C_0)$, ..., $C_n=\Psi(C_{n-1}),...$ We claim that $C=\cap_{i=0}^\infty C_i$ is a non-empty compact set and satisfies $\Psi(C)=C$.
    \begin{enumerate}
        \item Because we can infer $\Psi(X_1)\subseteq \Psi(X_2)$ if $X_1\subseteq X_2$, $X=C_0\supseteq C_1 \Rightarrow C_1=\Psi(C_0)\supseteq C_2=\Psi(C_1)$,...., so $C_0\supseteq C_1\supseteq \cdots C_n\supseteq \cdots$. Hence, $C$ is not empty.
        \item Because $X$ is compact, by the theorem \ref{thm:compact-valued, uhs correspondence of compact set is compact}, we can infer $C_n$ is compact for all $n$. Then, $C_n$ is closed for all $n$, so $C$ is closed. Because $C$ is a closed set of compact set $X$, $C$ is compact.
        \item $C\subseteq C_n,\forall n \Rightarrow \Psi(C)\subseteq \Psi(C_n),\forall n \Rightarrow \Psi(C)\subseteq C$
        \item Assume $C\subseteq \Psi(C)$ doesn't hold, that is $\exists y\in C$ s.t. $y\notin \Psi(C)$. Because $y\in C$ and $C_0\supseteq C_1\supseteq \cdots C_n\supseteq \cdots$, there exists $k\in C_n$ for all $n$ s.t. $y\in\Psi(k)$. $k\in \cap_{i=1}^\infty C_i=C$, so $\Psi(k)\subseteq \Psi(C)$, which contradicts to $y\notin \Psi(C)$. Hence, $C\subseteq \Psi(C)$.
    \end{enumerate}
    All in all the claim "$C=\cap_{i=0}^\infty C_i$ is a non-empty compact set and satisfies $\Psi(C)=C$" is proved.
\end{proof}










\chapter{Bayesian Persuasion: Extreme Points and Majorization}
Based on
\begin{enumerate}[$\circ$]
    \item Kleiner, A., Moldovanu, B., \& Strack, P. (2021). Extreme points and majorization: Economic applications. \textit{Econometrica}, 89(4), 1557-1593.
    \item 
\end{enumerate}


\section{Extreme Points}
\subsection{Extreme Points of Convex Set}
\begin{definition}[Extreme Points]
    \normalfont
    An \textbf{extreme point} of a convex set $A$ is a point $x\in A$ that cannot be represented as a convex combination of points in $A$.
\end{definition}

\subsection{Krein-Milman Theorem: Existence of Extreme Points}
\begin{theorem}[Krein-Milman Theorem]\label{KMT}
    Every non-empty \textbf{compact convex} subset of a Hausdorff locally convex topological vector space (for example, a normed space) is the closed, convex hull of its extreme points.\\
    In particular, this set has extreme points.
\end{theorem}


\subsection{Bauer's Maximum Principle: Usefulness of Extreme Points for Optimization}
\begin{theorem}[Bauer's Maximum Principle]\label{BMP}
    Any function that is \textbf{convex and continuous}, and defined on a set that is \textbf{convex and compact}, attains its maximum at some extreme point of that set.
\end{theorem}



\section{Majorization}
\subsection{Majorization and Weak Majorization}
\begin{definition}[Majorization of Non-decreasing Functions]
    \normalfont
    Consider right-continuous functions that map the unit interval $[0,1]$ into the real numbers. For two \underline{non-decreasing} functions $f,g \in L^1$, we say that $f$ \textbf{majorizes} $g$, denoted by $g \prec f$, if the following two conditions hold:
    \begin{equation}
        \begin{aligned}
            \int_x^1 g(s)ds\leq \int_x^1 f(s)ds,\forall x\in [0,1]
        \end{aligned}
        \tag{Condition 1}
        \label{C1}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            \int_0^1 g(s)ds=\int_0^1 f(s)ds
        \end{aligned}
        \tag{Condition 2}
        \label{C2}
    \end{equation}
\end{definition}

\begin{definition}[Weak Majorization]
    \normalfont
    $f$ \textbf{weakly majorizes} $g$, denoted by $g \prec_w f$, if \ref{C1} holds (not necessarily \ref{C2}).
\end{definition}

\subsection{How to work for non-monotonic functions? -- Non-Decreasing Rearrangement}
\begin{note}
    \normalfont
    \textbf{How this work with non-monotonic functions?}\\
    Suppose $f,g$ are non-monotonic, we compare their non-decreasing rearrangements $f^*$, $g^*$.
    \begin{definition}[Rearrangement]
        \normalfont
        Given a function $f$, let $m(x)$ denote the Lebesgue measure of the set $\{s \in[0, 1]: f(s)\leq x\}$, that is $m(x)=\int_{s\in\{s \in[0, 1]: f(s)\leq x\}}1ds$ (the "length" of the set). The non-decreasing rearrangement of $f$, $f^*$, is defined by $$f^*(t) = \inf\{ x \in \mathbb{R}: m(x) \geq t\},\ t\in[0,1]$$
    \end{definition}
\end{note}

\subsection{Theorem: $F$ majorizes $G$ $\Leftrightarrow$ $G$ is a mean-preserving spread of $F$}
Based on
\begin{enumerate}[$\circ$]
    \item Shaked, M., \& Shanthikumar, J. G. (2007). \textit{Stochastic orders}. New York, NY: Springer New York.
\end{enumerate}

\begin{definition}[Generalized Inverse]
    \normalfont
    Suppose $G$ is defined on the interval $[0,1]$, we can define the \textbf{generalized inverse}
    $$G^{-1}(x)=\sup\{s:G(s)\leq x\}, x\in [0,1]$$
\end{definition}

Let $X_F$ and $X_G$ be now random variables with distributions $F$ and $G$, defined on the interval $[0,1]$.
\begin{theorem}[Shaked \& Shanthikumar (2007), Section 3.A]
    $$G\prec F \Leftrightarrow F^{-1}\prec G^{-1} \Leftrightarrow X_{G}\leq_{ssd}X_F \textnormal{ and }\mathbb{E}[X_G]=\mathbb{E}[X_F]$$
    where $\leq_{ssd}$ denotes the standard \underline{second-order stochastic dominance}.
\end{theorem}
Based on Theorem \ref{SOSD_equiv} and the \ref{C2} of Majorization, we can conclude
\begin{corollary}[Majorization $\Leftrightarrow$ Mean-preserving Contraction]\label{Maj_Equiv}
    \begin{center}
        $F$ majorizes $G$ $\Leftrightarrow$ $F$ is a mean-preserving contraction of $G$ ($G$ is a mean-preserving spread of $F$)
    \end{center}
\end{corollary}
That is, we can construct random variables $X_F$, $X_G$,
jointly distributed on some probability space, such that $X_F \sim F$, $X_G \sim G$ and such that $X_F = \mathbb{E}[X_G \mid X_F]$.

\section{Capture Extreme Points in Economic Applications}
Let $L^1$ denote the real-valued and integrable functions defined on $[0,1]$.

In this section, we focus on \textbf{non-decreasing (weakly increasing) functions}, for example, a cumulative distribution function in Bayesian persuasion, or an incentive-compatible allocation in mechanism design.

\subsection{Definitions of $\mathcal{MPS}(f)$, $\mathcal{MPS}_w(f)$, $\mathcal{MPC}(f)$}
Based on Corollary \ref{Maj_Equiv}, we can define following sets
\begin{definition}
    \normalfont
    \begin{enumerate}
        \item The set of non-decreasing functions that are majorized by $f$ is denoted by
        \begin{equation}
            \begin{aligned}
                \mathcal{MPS}(f)&=\textnormal{MPS}(f)\cap\{g\in L^1\mid g \textnormal{ is non-decreasing}\}\\
                &=\{g\in L^1\mid g \textnormal{ is non-decreasing and }g\prec f\}
            \end{aligned}
            \nonumber
        \end{equation}
        \item The set of non-negative, non-decreasing functions that are weakly majorized by $f$ is denoted by
        \begin{equation}
            \begin{aligned}
                \mathcal{MPS}_w(f)=\{g\in L^1\mid g \textnormal{ is non-negative,  non-decreasing and }g\preceq f\}
            \end{aligned}
            \nonumber
        \end{equation}
        \item The set of non-decreasing functions that majorize $f$ and satisfy $f(0)\leq g\leq f(1)$ is denoted by
        \begin{equation}
            \begin{aligned}
                \mathcal{MPC}(f)&=\textnormal{MPC}(f)\cap\{g\in L^1\mid g \textnormal{ is non-decreasing and }f(0)\leq g\leq f(1)\}\\
                &=\{g\in L^1\mid g \textnormal{ is non-decreasing and }g\succ f \textnormal{ and }f(0)\leq g\leq f(1)\}
            \end{aligned}
            \nonumber
        \end{equation}
        where $f(0)\leq g\leq f(1)$ is used to ensure compactness.
    \end{enumerate}
\end{definition}

\subsection{Proposition: $\mathcal{MPS}(f)$, $\mathcal{MPS}_w(f)$, $\mathcal{MPC}(f)$ have extreme points and any element is a combination of extreme points}

Following two propositions are the Proposition 1 of the Kleiner et al. (2021).
\begin{proposition}[Non-decreasing $f$ $\Rightarrow$ $\mathcal{MPS}(f)$, $\mathcal{MPS}_w(f)$, and $\mathcal{MPC}(f)$ have extreme points]
    Suppose $f\in L^1$ is non-decreasing. Then $\mathcal{MPS}(f)$, $\mathcal{MPS}_w(f)$, and $\mathcal{MPC}(f)$ are convex and compact in the norm topology $\Rightarrow$ (by Krein-Milman Theorem \ref{KMT}) they all have non-empty set of extreme points.
\end{proposition}
\begin{note}
    We use $\textnormal{ext} A$ to denote the set of extreme points of set $A$.
\end{note}
\begin{proposition}[Non-decreasing $f$ $\Rightarrow$ any distribution is a combination of extreme points]
    Suppose $f\in L^1$ is non-decreasing. For any $g\in \mathcal{MPS}(f)$, $\exists$ a probability measure $\lambda_g$ over $\textnormal{ext} \mathcal{MPS}(f)$ such that $$g=\int_{\textnormal{ext} \mathcal{MPS}(f)}h\ d\lambda_g(h)$$
    (also hold for any $g\in \mathcal{MPS}_w(f)$ and $g\in \mathcal{MPC}(f)$).
\end{proposition}

\subsection{Extreme Points in $\mathcal{MPS}(f)$}\label{K_thm_1}
\begin{theorem}[Form of Extreme Points in $\textnormal{MPS}(f)$: Kleiner et al. (2021), Theorem 1]
    Let $f$ be non-decreasing. Then $g$ is an \textbf{extreme point} in $\mathcal{MPS}(f)$ \underline{if and only if} there exists a collection of disjoint intervals $\{[\underline{x}_i,\overline{x}_i)\}_{i\in I}$ such that
    \begin{equation}
        \begin{aligned}
            g(x)=\left\{\begin{matrix}
                f(x),&\textnormal{if } x\notin \cup_{i\in I}[\underline{x}_i,\overline{x}_i)\\
                \frac{\int_{\underline{x}_i}^{\overline{x}_i}f(s)ds}{\overline{x}_i-\underline{x}_i},&\textnormal{if } x\in [\underline{x}_i,\overline{x}_i)
            \end{matrix}\right.
        \end{aligned}
        \nonumber
    \end{equation}
\end{theorem}
$g$ is an extreme point of $\mathcal{MPS}(f)$ implies either that $g(x)=f(x)$ or that $g$ is constant at $x$.

\begin{definition}[Exposed Element]
    \normalfont
    An element $x$ of a convex set $A$ is \textbf{exposed} if there exists a continuous linear functional that attains its maximum on $A$ uniquely at $x$.
\end{definition}
\begin{note}
    Every exposed point is extreme, but the converse is not true in general.
\end{note}

\begin{corollary}[Kleiner et al. (2021), Corollary 1]
    Every extreme point of $\mathcal{MPS}(f)$ is exposed.
\end{corollary}

\subsection{Extreme Points in $\mathcal{MPS}_w(f)$}
For a set $A\subseteq [0,1]$, we use $\mathbf{1}_{A}(x)$ denote the indicator function of set $A$: it equals to $1$ if $x\in A$ and $0$ otherwise.
\begin{corollary}[Kleiner et al. (2021), Corollary 2]
    Suppose that $f$ is non-decreasing and non-negative. A function $g$ is an extreme point of $\mathcal{MPS}_w(f)$ \underline{if and only if} there is $\theta \in[0,1]$ such that $g$ is an extreme point of $\mathcal{MPS}(f)$ and $g(x) = 0,\forall x\in[0,\theta)$.
\end{corollary}

\subsection{Extreme Points in $\mathcal{MPC}(f)$}
\begin{theorem}[Kleiner et al. (2021), Theorem 2]
    Let $f$ be non-decreasing and continuous. Then $g \in \mathcal{MPC}(f)$ is an extreme point of $\mathcal{MPC}(f)$ \underline{if and only if} there exists a collection of intervals $\left[\underline{x}_i, \bar{x}_i\right)$, (potentially empty) sub-intervals $\left[\underline{y}_i, \bar{y}_i\right) \subseteq \left[\underline{x}_i, \bar{x}_i\right)$, and numbers $v_i$ indexed by $i \in I$ such that for all $x \in[0,1]$,
    \begin{equation}
        \begin{aligned}
            g(x)=
            \begin{cases}
                f(x) & \text { if } x \notin \bigcup_{i \in I}\left[\underline{x}_i, \bar{x}_i\right)\\
                f\left(\underline{x}_i\right) & \text { if } x \in\left[\underline{x}_i, \underline{y}_i\right)\\
                v_i & \text { if } x \in\left[\underline{y}_i, \bar{y}_i\right)\\
                f\left(\bar{x}_i\right) & \text { if } x \in\left[\bar{y}_i, \bar{x}_i\right)
            \end{cases}
        \end{aligned}
        \label{3}
    \end{equation}
    Moreover, a function $g$ as defined in (\ref{3}) is in $\mathcal{MPC}(f)$ if the following three conditions are satisfied:
    \begin{equation}
        \begin{aligned}
            \left(\bar{y}_i-\underline{y}_i\right) v_i=\int_{\underline{x}_i}^{\bar{x}_i} f(s) \mathrm{d} s-f\left(\underline{x}_i\right)\left(\underline{y}_i-\underline{x}_i\right)-f\left(\bar{x}_i\right)\left(\bar{x}_i-\bar{y}_i\right)
        \end{aligned}
        \label{4}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            f\left(\underline{x}_i\right)\left(\bar{y}_i-\underline{x}_i\right)+f\left(\bar{x}_i\right)\left(\bar{x}_i-\bar{y}_i\right) \leq \int_{\underline{x}_i}^{\bar{x}_i} f(s) \mathrm{d} s \leq f\left(\underline{x}_i\right)\left(\underline{y}_i-\underline{x}_i\right)+f\left(\bar{x}_i\right)\left(\bar{x}_i-\underline{y}_i\right)
        \end{aligned}
        \label{5}
    \end{equation}
    If $v_i \in\left(f\left(\underline{y}_i\right), f\left(\bar{y}_i\right)\right)$, then for an arbitrary point $m_i$ satisfying $f\left(m_i\right)=v_i$ it must hold that
    \begin{equation}
        \begin{aligned}
            \int_{m_i}^{\bar{x}_i} f(s) \mathrm{d} s \leq v_i\left(\bar{y}_i-m_i\right)+f\left(\bar{x}_i\right)\left(\bar{x}_i-\bar{y}_i\right)
        \end{aligned}
        \label{6}
    \end{equation}
\end{theorem}
Condition (\ref{4}) in the theorem ensures that $g$ and $f$ have the same integrals for each sub-interval $\left[\underline{x}_i, \bar{x}_i\right)$, analogously to the condition imposed in Theorem \ref{K_thm_1}. Condition (\ref{5}) ensures that $v_i \in\left(f\left(\underline{x}_i\right), f\left(\bar{x}_i\right)\right)$, ensuring that $g$ is non-decreasing. If $f$ crosses $g$ in the interval $\left[\underline{y}_i, \bar{y}_i\right]$, then there is $m_i \in\left[\underline{y}_i, \bar{y}_i\right]$ such that $f\left(m_i\right)=v_i$. In this case, Condition (\ref{6}) ensures that $\int_s^{\bar{x}_i} f(t) \mathrm{d} t \leq \int_s^{\bar{x}_i} g(t) \mathrm{d} t$ for all $s \in\left[\underline{x}_i, \bar{x}_i\right)$ and thus that $f \prec g$. If $v_i \notin$ $\left(f\left(\underline{y}_i\right), f\left(\bar{y}_i\right)\right)$, Condition (\ref{5}) is enough to ensure that $f \prec g$ and thus Condition (\ref{6}) is not necessary.





\chapter{Bayesian Persuasion: Bi-Pooling}
Based on
\begin{enumerate}[$\circ$]
    \item $\bigstar$ Arieli, I., Babichenko, Y., Smorodinsky, R., \& Yamashita, T. (2023). Optimal persuasion via bi-pooling. \textit{Theoretical Economics}, 18(1), 15-36.
    \item Gentzkow, Matthew and Emir Kamenica (2016), “A Rothschild-Stiglitz approach to Bayesian persuasion.” \textit{American Economic Review}, 106, 597-601.
    \item Kolotilin, Anton (2018), “Optimal information disclosure: A linear programming ap-proach.” \textit{Theoretical Economics}, 13, 607-635.
\end{enumerate}

\section{Persuation Model}
Consider a persuation model where the state space is the interval $[0,1]$ with a common prior $F\in\Delta([0,1])$ that has full support (i.e., $[0,1]$ is the smallest closed set that has probability one). The sender knows the realized state and the receiver is uninformed.
\begin{enumerate}
    \item \underline{Singaling:} Prior to the realization of the state, the sender commits to a \textbf{signaling policy} $$\pi : [0, 1] \rightarrow \Delta(S)$$
    where $S$ is an arbitrary measurable space. Once the state $\omega\in[0,1]$ is realized, the sender sends a signal $s\in S$ to the receiver based on the committed signaling policy, i.e., $s \sim \pi(\omega)$. Without loss of generality, we may assume that $S=[0, 1]$, and that the posterior mean of the state, given signal $s$, is $s$ itself.
    
    Hence, the distribution of the posterior mean $s$ given the signal policy $\pi$, denoted by $F_\pi\in\Delta([0,1])$ is a \textit{mean-preserving contraction} of $F$.
    
    It is also easy to note that for any $G\in \textnormal{MPC}(F)$, there exists a signaling policy $\pi$ (may not be unique) that makes $F_\pi=G$ (e.g., Gentzkow and Kamenica(2016), Kolotilin (2018)).
    \item \underline{Persuation problem:} The sender's indirect utility is denoted by $u:[0,1] \rightarrow \mathbb{R}$, where $u(x)$ is the sender's expected utility in case the receiver's posterior mean is $x$. $u$ is assume to be upper semicontinuous. $(F,u)$ is referred as a \textbf{persuation problem}. The sender's problem takes the form:
    \begin{equation}
        \begin{aligned}
            \max_{G\in \textnormal{MPC}(F)}\mathbb{E}_{x\sim G}[u(x)]
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}


\section{Bi-Pooling}
\subsection{Bi-pooling Distribution}
\begin{note}
    For a distribution $H \in \Delta([0,1])$ and a measurable set $C \subseteq[0,1]$ we denote by $H|_C$ the distribution of $h \sim H$ conditional on the event that $h\in C$.
\end{note}

\begin{definition}[Bi-pooling Distribution (Arieli et al. (2023), Definition 1)]
    \normalfont
    A distribution $G \in \textnormal{MPC}(F)$ is called a \textbf{bi-pooling distribution} (with respect to $F$) if there exists a collection of pairwise disjoint open intervals $\{(\underline{y}_i,\overline{y}_i)\}_{i\in A}$ such that
    \begin{enumerate}[$\circ$]
        \item For every $i\in A$, $$G((\underline{y}_i,\overline{y}_i))=F((\underline{y}_i,\overline{y}_i))$$
        where $G((\underline{y}_i,\overline{y}_i))=G(\overline{y}_i)-G(\underline{y}_i)=\int_{\underline{y}_i}^{\overline{y}_i}g(x)dx$, $F((\underline{y}_i,\overline{y}_i))=F(\overline{y}_i)-F(\underline{y}_i)=\int_{\underline{y}_i}^{\overline{y}_i}f(x)dx$.
        \item The remaining intervals are the same: $$G|_{[0,1]\backslash \cup_{i\in A}(\underline{y}_i,\overline{y}_i)}=F|_{[0,1]\backslash \cup_{i\in A}(\underline{y}_i,\overline{y}_i)}$$
        \item For every $i\in A$, $$|\textnormal{supp}(G|_{(\underline{y}_i,\overline{y}_i)})|\leq 2$$
        which means there are at most two different values of $G$ over $(\underline{y}_i,\overline{y}_i)$. If $|\textnormal{supp}(G|_{(\underline{y}_i,\overline{y}_i)})|=2$, we call $(\underline{y}_i,\overline{y}_i)$ a \textbf{bi-pooling interval}; If $|\textnormal{supp}(G|_{(\underline{y}_i,\overline{y}_i)})|=1$, we call $(\underline{y}_i,\overline{y}_i)$ a \textbf{pooling interval}. In the case where all intervals are pooling intervals, we say that $G$ is a \textbf{pooling distribution} (with respect to $F$).
    \end{enumerate}
\end{definition}
\begin{example}
    Consider the persuasion problem $(F, u)$, where $F = U [0, 1]$ is the uniform
    distribution over $[0, 1]$ and $u:[0,1] \rightarrow \mathbb{R}$ is an arbitrary function satisfying $u(\frac{1}{3})=u(\frac{2}{3})=0$ and $u(x)<0,\forall x\notin \{\frac{1}{3},\frac{2}{3}\}$.\\Consider using a binary signal space $S=\{s_1,s_2\}$, where $s_1$ is sent with probability $1$ over the interval $(\frac{1}{12},\frac{7}{12})$ and $s_2$ is sent with probability $1$ over the interval $[0,\frac{1}{12}]\cup[\frac{7}{12},1]$. This policy is a bi-pooling policy for the singleton collection $\{[0,1]\}$.
\end{example}

\section{Applying Bi-pooling Distributions to Persuasion Problems}
\subsection{It works for all}
\begin{theorem}[Arieli et al. (2023), Theorem 1]
    Every persuasion problem $(F,u)$ admits an optimal bi-pooling distribution.
\end{theorem}

\begin{proposition}[Arieli et al. (2023), Proposition 1]
    The set of extreme points of $\textnormal{MPC}(F)$ is precisely the set of bi-pooling distributions.
\end{proposition}

\begin{theorem}[Arieli et al. (2023), Theorem 2]
    For every bi-pooling distribution $G \in \textnormal{MPC}(F)$ there exists a continuous utility function $u$ for which $G$ is the unique optimal solution of $\max_{G\in \textnormal{MPC}(F)}\mathbb{E}_{x\sim G}[u(x)]$. That is, every extreme point of $\textnormal{MPC}(F)$ is exposed.
\end{theorem}


\subsection{How it works}
\begin{definition}[Bi-pooling Policy (Arieli et al. (2023), Definition 3)]
    \normalfont
    A signaling policy $\pi$ is called a \textbf{bi-pooling policy} if there exists a collection of pairwise disjoint intervals $\{(\underline{y}_i,\overline{y}_i)\}_{i\in A}$ such that
    \begin{enumerate}[$\circ$]
        \item for every state $\omega\in (\underline{y}_i,\overline{y}_i)$ we have $\textnormal{supp}(\pi(\omega))\subseteq\{\underline{z}_i,\overline{z}_i\}$ (either $\pi(\omega)=\overline{z}_i$ or $\pi(\omega)=\underline{z}_i$) for some $\underline{z}_i\leq\overline{z}_i$ and $\underline{z}_i,\overline{z}_i\in [\underline{y}_i,\overline{y}_i]$;
        \item for every $\omega\notin \cup_{i\in A}(\underline{y}_i,\overline{y}_i)$, the policy sends the signal $\pi(\omega) = \omega$ (i.e., it reveals the state).
    \end{enumerate}
    In the case where $\underline{z}_i=\overline{z}_i$ for all $i\in A$, we refer to $\pi$ as a \textbf{pooling policy}.
\end{definition}

\begin{definition}[Monotonic Signaling Policy (Arieli et al. (2023), Definition 4)]
    \normalfont
    A (possibly mixed) signaling policy, $\pi : [0, 1] \rightarrow \Delta ([0,1])$, is \textbf{monotonic} if
    \begin{center}
        $\pi(x)$ \underline{first-order stochastically dominates} $\pi(y)$ for every $x\geq y$.
    \end{center}
\end{definition}

\begin{proposition}[Arieli et al. (2023), Proposition 2]
    Every persuasion problem admits an optimal (mixed) monotonic signaling policy.
\end{proposition}

\begin{lemma}[Arieli et al. (2023), Lemma 3]
    A persuasion problem $(F,u)$ admits an optimal pure monotonic signaling policy \underline{if and only if} it admits an optimal pooling policy.
\end{lemma}

\begin{definition}[Double-Interval Nested Structure]
    \normalfont
    A pure signaling policy: for each bi-pooling interval $(\underline{y}_i,\overline{y}_i)$, we can find a sub-interval $(\underline{w}_i,\overline{w}_i) \subseteq (\underline{y}_i,\overline{y}_i)$ such that $\pi$ is constant over the interval $(\underline{w}_i,\overline{w}_i)$ as well as over its complement $(\underline{y}_i,\overline{y}_i)\backslash (\underline{w}_i,\overline{w}_i)$.
\end{definition}

\begin{corollary}[Arieli et al. (2023), Corollary 2]
    Every persuasion problem $(F,u)$ has an optimal bi-pooling policy that has a double-interval nested structure.
\end{corollary}


\chapter{Optimization Methods}
\section{Neyman-Pearson Lemma}




\chapter{Political Models}
\section{Normal-Normal Learning}
Suppose $\theta$ has a prior $N(\mu_\theta,\sigma_\theta^2)$.

We observe $s=\theta+\varepsilon$, where $\varepsilon\sim N(0,\sigma_\varepsilon^2)$. Then, the posterior beliefs about $\theta$ given $s$ is also normal with mean $\mu_1=\lambda\mu_\theta+(1-\lambda)s$ and variance $\lambda\sigma_\theta^2$, where $\lambda=\frac{\sigma_\theta^{-2}}{\sigma_\theta^{-2}+\sigma_\varepsilon^{-2}}$ is the precision weight.



\section{Voting Model}
Consider an incumbent $I$ and a citizen/voter $v$.
\begin{enumerate}[$\bullet$]
    \item $I$ picks $x_1\in \mathbb{R}$;
    \item $v$ observes $u_1=-x_1^2+\epsilon$, where $\epsilon\sim f$ and $f$ is uninormal of $0$, symmetric, continuous, and differentiable. $f'(z)$ is positive for $z<0$, negative for $z>0$, and zero for $z=0$.
    \item $v$ re-elects or not
    \item (new) $I$ chooses $x_2$
    \item ...
\end{enumerate}

\subsection{Case 1}
Incumbents have $\alpha\in (0,1)$ probability to be "good" type who picks $x_1=x_2=0$ and $1-\alpha$ probability to be "bad" type who picks $\hat{x}=x_1=x_2>0$.

Bayesian posterior beliefs are
\begin{equation}
    \begin{aligned}
        \textnormal{Pr}(\textnormal{good}\mid u_1)=\frac{\alpha f(u_1)}{\alpha f(u_1)+(1-\alpha)f(u_1+\hat{x}^2)}
    \end{aligned}
    \nonumber
\end{equation}
where $\textnormal{Pr}(\textnormal{good}\mid u_1)\geq \alpha$ $\Leftrightarrow$ $f(u_1)\geq f(u_1+\hat{x}^2)$.

By our assumption about $f$, $f(u_1)\geq f(u_1+\hat{x}^2)$ means $u_1$ is closer to zero than $u_1+\hat{x}^2$ $\Rightarrow$ $u_1^2\leq (u_1+\hat{x}^2)^2=u_1^2+2u_1\hat{x}^2+\hat{x}^4$, that is, $u_1>-\frac{\hat{x}^2}{2}$.



\subsection{Case 2: Moral Hazard Version}
All incumbents are "bad": ideal policy is $1$. Assume voters re-elect if and only if $u_1\geq k$, where $k$ is endogenous.

Based on this rule, the probability of an incumbent being re-elected is
\begin{equation}
    \begin{aligned}
        \textnormal{Pr}(\textnormal{re-elect}|x_1)=\textnormal{Pr}(-x_1^2+\epsilon\geq k)=1-F(k+x_1^2)
    \end{aligned}
    \nonumber
\end{equation}

Suppose the utility of the incumbent is $$U_I(x_1,x_2)=w-(1-x_1)^2+\delta (w-(1-x_2)^2) \mathbf{1}_\textnormal{re-elect}$$
Specifically, the expected utility with $u_2=1$ is
\begin{equation}
    \begin{aligned}
        U_I(x_1,x_2=1)=w-(1-x_1)^2+\delta w \left[1-F(k+x_1^2)\right]
    \end{aligned}
    \nonumber
\end{equation}
Then, $x_1^*$ should solve
\begin{equation}
    \begin{aligned}
        \frac{\partial U_I}{\partial x_1}=2(1-x_1)-2\delta w x_1 f(k+x_1^2)=0\\
        \Rightarrow f(k+x_1^2)=-\frac{1}{\delta w}+\frac{1}{x_1}\frac{1}{\delta w}
    \end{aligned}
    \nonumber
\end{equation}

\textbf{Apply Implicit Function Theorem}

Let $g(k,x)=f(k+x_1^2)+\frac{1}{\delta w}-\frac{1}{x_1}\frac{1}{\delta w}$.

The goal of the voter is to find the $k$ that minimizes $x_1^*$. By the implicit function theorem
\begin{equation}
    \begin{aligned}
        \frac{\partial x_1^*}{\partial k}=-\frac{\frac{\partial g}{\partial k}\big|_{x_1^*}}{\frac{\partial g}{\partial x}\big|_{x_1^*}}
    \end{aligned}
    \nonumber
\end{equation}
As $\frac{\partial g}{\partial k}=f'(k+x_1^2)$ and $\frac{\partial g}{\partial x}=2x_1 f'(k+x_1^2)+\frac{1}{x_1^2}\frac{1}{\delta w}$, we can conclude the optimal $k$ satisfies $k=-{x_1^*}^2$. Then, $f(0)=-\frac{1}{\delta w}+\frac{1}{x_1^*}\frac{1}{\delta w} \Rightarrow $
\begin{equation}
    \begin{aligned}
        x_1^*=\frac{1}{1+\delta w f(0)},\ k^*=-\left(\frac{1}{1+\delta w f(0)}\right)^2
    \end{aligned}
    \nonumber
\end{equation}

\subsection{Case 3}
Suppose the incumbent has probability $\alpha$ being "good" with $y_I=0$ and probability $1-\alpha$ being "bad" with $y_I=1$. He chooses $x_2=y_I$ at stage $2$.

Given the strategy $x_g$ and $x_b$
Bayesian posterior beliefs are
\begin{equation}
    \begin{aligned}
        \textnormal{Pr}(\textnormal{good}\mid u_1)=\frac{\alpha f(x_g^2+u_1)}{\alpha f(x_g^2+u_1)+(1-\alpha)f(x_b^2+u_1)}
    \end{aligned}
    \nonumber
\end{equation}
Hence, $\textnormal{Pr}(\textnormal{good}\mid u_1)\geq \alpha$ if and only if $f(x_g^2+u_1)\geq f(x_b^2+u_1)$.

The voter's strategy is also represented by "re-elect" iff $u_1\geq k$. At the critical point $u_1=k$,
\begin{equation}
    \begin{aligned}
        f(x_g^2+k)=f(x_b^2+k) \Rightarrow k=-\frac{x_g^2+x_b^2}{2}
    \end{aligned}
    \nonumber
\end{equation}
Suppose the expected utility (constructed based on avoiding deviations from the incumbent's true type) of the incumbent is $$\mathbb{E}U_I(x_1,x_2=y_I)=w-(x_1-y_I)^2+\delta w\left(1-F(k+x_1^2)\right)$$
Obviously, $x_1^*=0$ for good incumbent. (i.e., $x_g=0$). Then, $k=-\frac{x_b^2}{2}$, and
$$\mathbb{E}U_b(x_1)=w-(x_1-1)^2+\delta w\left(1-F(k+x_1^2)\right)$$
which has derivative
\begin{equation}
    \begin{aligned}
        -2(x_1-1)-2\delta w x_1 f(k+x_1^2)
    \end{aligned}
    \nonumber
\end{equation}
So, the optimal $x_1^*$ of "bad" type should satisfy
\begin{equation}
    \begin{aligned}
        f(k+x_1^2)+\frac{1}{\delta w}-\frac{1}{\delta w x_1}=0
    \end{aligned}
    \nonumber
\end{equation}
Consider the $x_1=\sqrt{-2k}$ (by what we induced, $k=-\frac{x_b^2}{2}$), the optimal $k$ should be solved by
\begin{equation}
    \begin{aligned}
        H(k)&=f(-k)+\frac{1}{\delta w}-\frac{1}{\delta w \sqrt{-2k}}\\
        &=f(k)+\frac{1}{\delta w}-\frac{1}{\delta w \sqrt{-2k}}=0
    \end{aligned}
    \nonumber
\end{equation}
By our assumption about $f$, $f(k)=f(-k)$.

Also, by the implicit function theorem, we can analyze how the $w$ affects $k$
\begin{equation}
    \begin{aligned}
        \frac{\partial k}{\partial w}=-\frac{\frac{\partial H}{\partial w}\big|_{k^*}}{\frac{\partial H}{\partial k}\big|_{k^*}}
    \end{aligned}
    \nonumber
\end{equation}

















\end{document}