\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nty/global//global/global}
\pgfsyspdfmark {pgfid1}{28388828}{16845296}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Statistics Basics}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Random Sampling}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Sample Mean and Sample Variance}{1}{subsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Distributional Properties}{1}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Order Statistics}{2}{subsection.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Basic Statistics}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Point Estimation}{3}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Method of Moments (MM)}{3}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Maximum Likelihood (ML)}{5}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Comparing Estimators: Mean Squared Error}{6}{subsection.1.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Sufficiency}{7}{subsection.1.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Information-Theoretic Functional}{8}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Statistical Inference}{9}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Basics}{9}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Decision Rule Examples}{9}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Maximum-Likelihood Principle (state is norandom)}{10}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Bayesian Decision Rule (state is random)}{11}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Rules}{11}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Maximum A Posteriori (MAP) Decision Rule (Binary example)}{12}{subsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Minimum Mean Squared Error (MMSE) Rule ($\mathbb  {R}^n$ example)}{12}{subsection.3.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Comparison}{13}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Machine Learning in Inference}{14}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Empirical Risk Minimization (ERM)}{14}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Example: Linear MMSE (LMMSE) estimator}{14}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Penalized ERM}{15}{subsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Stochastic Approximation}{16}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Stochastic Gradient Descent (SGD)}{18}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}SGD Application to Empirical Risk Minimization (ERM)}{19}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Different Gradient Descent for ERM}{20}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Constraints on Learning Problem}{20}{subsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Stochastic Integration Methods}{22}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Deterministic Methods (Better in Low Dimension)}{22}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Riemann Integration}{22}{subsection.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Trapezoidal Rule}{22}{subsection.5.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces (a) Riemann approximation; (b) Trapezoidal approximation.\relax }}{22}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{}{{5.1}{22}{(a) Riemann approximation; (b) Trapezoidal approximation.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Multidimensional Integration}{23}{subsection.5.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Two-dimensional integration using regular grid.\relax }}{23}{figure.caption.3}\protected@file@percent }
\newlabel{}{{5.2}{23}{Two-dimensional integration using regular grid.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Stachastic Methods (Better in High Dimension)}{23}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Classical Monte Carlo Integration}{23}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Importance Sampling}{24}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Bootstrap (not enough data)}{26}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Residual Bootstrap}{26}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Particle Filtering}{28}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Kalman Filtering (Linear Dynamic System)}{28}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Particle Filtering (Nonlinear Dynamic System)}{28}{section.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Hidden Markov Model\relax }}{29}{figure.caption.4}\protected@file@percent }
\newlabel{}{{7.1}{29}{Hidden Markov Model\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Bayesian Recursive Filtering}{29}{subsection.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Particle Filter (bootstrap filter)}{29}{subsection.7.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {8}EM Algorithm}{31}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}General Structure of the EM Algorithm}{31}{section.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Complete and incomplete data spaces $Z$ and $Y$.\relax }}{32}{figure.caption.5}\protected@file@percent }
\newlabel{}{{8.1}{32}{Complete and incomplete data spaces $Z$ and $Y$.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Example 1: Variance Estimation}{33}{section.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Maximum-Likelihood (ML) Estimation}{33}{subsection.8.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}EM Algorithm}{33}{subsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Example 2: Estimation of Gaussian Mixtures}{34}{section.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Unknown Means: ML estimation is hard}{34}{subsection.8.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Unknown Means: EM Algorithm}{35}{subsection.8.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.3}Unknown Mixture Probabilities, Means and Variances}{36}{subsection.8.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Convergence of EM Algorithm}{36}{section.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.5}EM As an Alternating Maximization Algorithm}{37}{section.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Hidden Markov model (HMM)}{39}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces Hidden Markov Model (HMM)\relax }}{39}{figure.caption.6}\protected@file@percent }
\newlabel{}{{9.1}{39}{Hidden Markov Model (HMM)\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Viterbi Algorithm: (MAP) estimate $X_{1:t}$ given $Y_{1:t}$}{39}{section.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}MAP estimation problem}{39}{subsection.9.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.2}Viterbi Algorithm}{40}{subsection.9.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces (a) Trellis diagram; (b)—(e) evolution of the Viterbi algorithm, showing surviving paths and values $V(t,x)$ at times $t = 2,3,4,5$; (f) optimal path $\vec  {x}^* = (0,2,0,2,0)$ and its value $\varepsilon (\vec  {x}^*) = 11$.\relax }}{40}{figure.caption.7}\protected@file@percent }
\newlabel{}{{9.2}{40}{(a) Trellis diagram; (b)—(e) evolution of the Viterbi algorithm, showing surviving paths and values $V(t,x)$ at times $t = 2,3,4,5$; (f) optimal path $\vec {x}^* = (0,2,0,2,0)$ and its value $\varepsilon (\vec {x}^*) = 11$.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Bayesian Estimation of a Sequence: Need (MMSE) estimate $X_{1:t}$ given $Y_{1:t}$}{41}{section.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Forward-Backward Algorithm: (MMSE) estimate $X_{1:t+1}$ given $Y_{1:t}$}{41}{section.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}$\gamma _t(x) \triangleq \mathrm  {P}\left \{X_t=x \mid \vec  {Y}=\vec  {y}\right \}$}{41}{subsection.9.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}$\xi _t\left (x, x^{\prime }\right ) \triangleq \mathrm  {P}\left \{X_t=x, X_{t+1}=x^{\prime } \mid \vec  {Y}=\vec  {y}\right \}$}{43}{subsection.9.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}Scaling Factors}{43}{subsection.9.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Graphic Models}{44}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Graph Theory}{44}{section.10.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces (a) Directed and (b) Undirected graph.\relax }}{45}{figure.caption.8}\protected@file@percent }
\newlabel{}{{10.1}{45}{(a) Directed and (b) Undirected graph.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Bayesian Networks}{45}{section.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10.3}Markov Networks}{45}{section.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.1}General Form}{45}{subsection.10.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces (a) (b) Two Bayesian networks and (c) a Markov network.\relax }}{45}{figure.caption.9}\protected@file@percent }
\newlabel{}{{10.2}{45}{(a) (b) Two Bayesian networks and (c) a Markov network.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.2}Hammersley-Clifford theorem}{46}{subsection.10.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.3}Form of Gibbs distribution (Boltzmann distribution)}{46}{subsection.10.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10.4}Conversion of directed graph to undirected graph}{47}{section.10.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces Graph moralization\relax }}{47}{figure.caption.10}\protected@file@percent }
\newlabel{}{{10.3}{47}{Graph moralization\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.5} Inference and Learning}{47}{section.10.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.1}Inference on Trees}{47}{subsection.10.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces Example 1\relax }}{47}{figure.caption.11}\protected@file@percent }
\newlabel{}{{10.4}{47}{Example 1\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces Belief propagation in a tree\relax }}{48}{figure.caption.12}\protected@file@percent }
\newlabel{}{{10.5}{48}{Belief propagation in a tree\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Variational Inference, Mean-Field Techniques}{50}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Naive Mean-Field Methods}{50}{section.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.1}Graphical Models}{51}{subsection.11.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.2}Ising Model}{51}{subsection.11.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.2}Exponential Families of Probability Distributions}{53}{section.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.3}ML Estimation}{55}{section.11.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.4}Maximum Entropy}{55}{section.11.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.5}}{56}{section.11.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.6}Connection between Exponential Families and Graphic Models}{57}{section.11.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.1}Marginal polytope}{57}{subsection.11.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.2}Locally Consistent Marginal Distributions}{57}{subsection.11.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.3}Entropy on Tree Graphs}{59}{subsection.11.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.4}Naive Mean-Field Methods In Graph}{60}{subsection.11.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.5}Structural Mean Field Optimization}{60}{subsection.11.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.6}Bethe Entropy Approximation}{60}{subsection.11.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {12}$\ell _1$ Penalized Least Squares Minimization}{62}{chapter.12}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Problem Statement}{62}{section.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.2}Special Cases}{63}{section.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.1}Definition: Soft Threshold}{63}{subsection.12.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.2}Identity $A$}{63}{subsection.12.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.3}Orthonormal $A$}{63}{subsection.12.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.4}Quadratic Optimization ($\lambda =0$)}{63}{subsection.12.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.3}General Solution: Lasso}{64}{section.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.4}General Solution: Iterative Soft Thresholding Algorithm (ISTA)}{64}{section.12.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4.1}Proximal Minimization Algorithm}{64}{subsection.12.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4.2}Apply to $\ell _1$-penalized least-squares}{65}{subsection.12.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.5}Convergence Rate}{65}{section.12.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.6}Fast Iterative Soft Thresholding Algorithm (FISTA)}{66}{section.12.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.7}Alternating Direction Method of Multipliers (ADMM)}{66}{section.12.7}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {13}Compressive Sensing}{68}{chapter.13}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {13.1}Definitions related to Sparsity}{68}{section.13.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13.2}Measurement Matrix}{70}{section.13.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13.1}{\ignorespaces Measurement of sparse signal $\vec  {x}$ with support set $\Omega $ of size $k$.\relax }}{70}{figure.caption.13}\protected@file@percent }
\newlabel{}{{13.1}{70}{Measurement of sparse signal $\vec {x}$ with support set $\Omega $ of size $k$.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.1}Matrix Preliminaries}{70}{subsection.13.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.2}Recovery of k-Sparse Signals}{71}{subsection.13.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.3}Restricted Isometry Property}{72}{subsection.13.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13.3}Robust Signal Recovery from Noiseless Observations}{72}{section.13.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13.2}{\ignorespaces $\ell _1$ recovery for $n=2$ and $m=1$\relax }}{73}{figure.caption.14}\protected@file@percent }
\newlabel{}{{13.2}{73}{$\ell _1$ recovery for $n=2$ and $m=1$\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.4}Robust Signal Recovery from Noisy Observations}{74}{section.13.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.1}Bounded Noise}{74}{subsection.13.4.1}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{nobblfile}
\gdef \@abspage@last{79}
