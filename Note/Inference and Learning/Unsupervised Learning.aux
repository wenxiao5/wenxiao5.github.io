\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nty/global//global/global}
\pgfsyspdfmark {pgfid1}{28388828}{17950938}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Clustering}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}K-Means}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}K-Means Clustering Optimization Problem}{1}{subsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Lloyd's Algorithm}{2}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Benefits and Drawbacks}{2}{subsection.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Elbow Method}{3}{subsection.1.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Elbow Plot Example\relax }}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{}{{1.1}{3}{Elbow Plot Example\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Types of Clusters Definitions}{4}{section.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces (1). Well-Separated Cluster; (2). Density-Based Cluster\relax }}{5}{figure.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Contiguity-Based Cluster\relax }}{5}{figure.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Prototype-Based Cluster\relax }}{6}{figure.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}K-Medians}{6}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}K-Medians Clustering Optimization Problem}{6}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}K-Medians Heuristic Algorithm}{7}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}K-Medoids}{7}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}K-Medoids Clustering Optimization Problem}{7}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}K-Medoids Clustering Algorithm}{8}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}K-Medoids vs. K-Means}{8}{subsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Types of Clustering Algorithms Results}{9}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Partitional vs. Hierarchical Clustering Results}{9}{subsection.1.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Hierarchical Clustering Example\relax }}{9}{figure.caption.5}\protected@file@percent }
\newlabel{}{{1.5}{9}{Hierarchical Clustering Example\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}Exclusive vs. Overlapping vs. Fuzzy Clustering Results}{10}{subsection.1.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Clustering Evaluation Metrics}{11}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Evaluation Metrics\relax }}{11}{figure.caption.6}\protected@file@percent }
\newlabel{}{{2.1}{11}{Evaluation Metrics\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Clusterability Evaluation Metric: Is the dataset clusterable?}{12}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Hopkin's Statistics}{12}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces When Hopkins Statistic works well and not well\relax }}{13}{figure.caption.7}\protected@file@percent }
\newlabel{}{{2.2}{13}{When Hopkins Statistic works well and not well\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Unsupervised Clustering Evaluation Metrics: How cohesive and well separated are the clusters in the clustering?}{13}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Definition}{13}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Graph-based view of cohesion and separation for a clustering}{14}{subsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Proximity Matrix\relax }}{14}{figure.caption.8}\protected@file@percent }
\newlabel{}{{2.3}{14}{Proximity Matrix\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Silhouette Coefficients (Scores)}{14}{subsection.2.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Silhouette Plots\relax }}{15}{figure.caption.9}\protected@file@percent }
\newlabel{}{{2.4}{15}{Silhouette Plots\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Prototype-Based View of Cohesion and Separation for a Clustering}{15}{subsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Cluster-Sorted Similarity Matrix}{16}{subsection.2.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Example of Cluster-Sorted Similarity Matrix\relax }}{16}{figure.caption.10}\protected@file@percent }
\newlabel{}{{2.5}{16}{Example of Cluster-Sorted Similarity Matrix\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Cluster Number Evaluation Metrics: What is the 'correct' number of clusters?}{16}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}General Elbow Plot Method}{16}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Average Silhouette Score Plot Method}{17}{subsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Average Silhouette Score Plot\relax }}{17}{figure.caption.11}\protected@file@percent }
\newlabel{}{{2.6}{17}{Average Silhouette Score Plot\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Supervised Clustering Evaluation Metrics: How similar is the clustering to a set of (external) pre-assigned class labels?}{18}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Rand Index and Jaccard Coefficient of Two Partitions}{18}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Adjusted Rand Index}{18}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Clustering Comparison Metrics: Which clustering is better for a given dataset?}{19}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}'Clusterable'? and 'Correct' Number of Clusters: $t$-SNE}{19}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Goal}{19}{subsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Input/Output for the Algorithm}{20}{subsection.2.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Example of Output\relax }}{20}{figure.caption.12}\protected@file@percent }
\newlabel{}{{2.7}{20}{Example of Output\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Main Idea of The Algorithm}{21}{subsection.2.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Optimization Problem\relax }}{21}{figure.caption.13}\protected@file@percent }
\newlabel{}{{2.8}{21}{Optimization Problem\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Hierarchical Clustering}{23}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Agglomerative and Divisive Hierarchical Clustering Algorithms}{23}{section.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Hierarchical Clustering Example\relax }}{23}{figure.caption.14}\protected@file@percent }
\newlabel{}{{3.1}{23}{Hierarchical Clustering Example\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Agglomerative Hierarchical Clustering}{23}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}General Algorithm for Agglomerative Hierarchical Clustering}{23}{subsection.3.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Proximity Matrix\relax }}{24}{figure.caption.15}\protected@file@percent }
\newlabel{}{{3.2}{24}{Proximity Matrix\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}with Single Linkage}{24}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}with Complete Linkage}{24}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}with Average Linkage}{25}{subsection.3.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}with Ward's Linkage}{25}{subsection.3.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Divisive Hierarchical Clustering}{26}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}General Algorithm for Divisive Hierarchical Clustering}{26}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}with the Bisecting $k$-Means Algorithm}{26}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Categorical Data Clustering}{28}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Dataset Clustering with Categorical Variables}{28}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1} t-SNE Algorithm - Using a Distance Matrix Input}{28}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Creating a Distance Matrix for Datasets with Categorical Variables}{28}{subsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Partitional Clustering Algorithms for Datasets with Categorical Variables}{29}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Just Categorical Variables: $k$-Modes Clustering Algorithm}{29}{subsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces $k$-modes vs. $k$-means\relax }}{30}{figure.caption.16}\protected@file@percent }
\newlabel{}{{4.1}{30}{$k$-modes vs. $k$-means\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Numerical and Categorical Variables: $k$-Prototypes Clustering Algorithm}{30}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Hierarchical Clustering Algorithms for Datasets with Categorical Variables}{30}{section.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces $k$-prototypes vs. $k$-means\relax }}{31}{figure.caption.17}\protected@file@percent }
\newlabel{}{{4.2}{31}{$k$-prototypes vs. $k$-means\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Principal Component Analysis (PCA)}{32}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Assumption}{32}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Principal component analysis- general goals}{32}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Process of PCA}{33}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}How Do We Choose $p<n$?}{33}{section.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces How Do We Choose $p<n$\relax }}{33}{figure.caption.18}\protected@file@percent }
\newlabel{}{{5.1}{33}{How Do We Choose $p<n$\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Gaussian Mixture Models}{35}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Why use model-based clustering?}{35}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Overview of Mixture Models}{35}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Gaussian Mixture Models}{36}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Step 1: Finding each $\vec  {\mu }_k$ and $\vec  {\Sigma }_k$ with maximum likelihood estimation}{36}{subsection.6.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Step 2: Estimate the probability by $\vec  {\mu }_k$ and $\vec  {\Sigma }_k$}{37}{subsection.6.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Expectation Maximization (EM) Algorithm for GMM}{37}{subsection.6.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Benefits/Drawbacks of Gaussian Mixture Model Clustering (with EM Algorithm)}{38}{section.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}How to choose the number of clusters in a Gaussian Mixture Model}{39}{section.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Evaluation Metric 1: Akaike Information Criterion (AIC)}{39}{subsection.6.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Evaluation Metric 2: Bayes Information Criterion (BIC)}{39}{subsection.6.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Nonnegative Matrix Factorization (NMF)}{40}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Goals of NMF}{40}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}NMF Optimization - Unsupervised Case}{40}{section.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Input\relax }}{40}{figure.caption.19}\protected@file@percent }
\newlabel{}{{7.1}{40}{Input\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Optimization\relax }}{41}{figure.caption.20}\protected@file@percent }
\newlabel{}{{7.2}{41}{Optimization\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Output\relax }}{41}{figure.caption.21}\protected@file@percent }
\newlabel{}{{7.3}{41}{Output\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}BIRCH Clustering}{42}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH) Algorithm}{42}{section.8.1}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{nobblfile}
\gdef \@abspage@last{46}
