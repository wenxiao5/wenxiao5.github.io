\contentsline {chapter}{\numberline {1}Statistics Basics}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Random Sampling}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Sample Mean and Sample Variance}{2}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Distributional Properties}{2}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Order Statistics}{2}{subsection.1.1.3}%
\contentsline {section}{\numberline {1.2}Statistics Model (ECON 240B)}{4}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Model}{4}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Parametric Model}{4}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Parameter}{5}{subsection.1.2.3}%
\contentsline {section}{\numberline {1.3}Model Estimation (ECON 240B)}{6}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Plug-In Estimation}{6}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Bootstrap}{7}{subsection.1.3.2}%
\contentsline {section}{\numberline {1.4}Point Estimation}{12}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Method of Moments (MM)}{13}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Maximum Likelihood (ML)}{14}{subsection.1.4.2}%
\contentsline {section}{\numberline {1.5}Comparing Estimators: Mean Squared Error}{16}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}Mean Squared Error = $\textnormal {Bias}^2$ + Variance}{16}{subsection.1.5.1}%
\contentsline {subsection}{\numberline {1.5.2}Uniform Minimum Variance Unbiased (UMVU)}{17}{subsection.1.5.2}%
\contentsline {section}{\numberline {1.6}Sufficient Statistics}{17}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1}Sufficient Statistic: contains all information of $\theta $}{17}{subsection.1.6.1}%
\contentsline {subsection}{\numberline {1.6.2}Rao-Blackwell Theorem}{17}{subsection.1.6.2}%
\contentsline {subsection}{\numberline {1.6.3}Fisher-Neyman Factorization Theorem}{18}{subsection.1.6.3}%
\contentsline {subsection}{\numberline {1.6.4}Minimal Sufficient Statistic}{19}{subsection.1.6.4}%
\contentsline {section}{\numberline {1.7}Complete Statistic}{20}{section.1.7}%
\contentsline {subsection}{\numberline {1.7.1}Complete Statistic}{20}{subsection.1.7.1}%
\contentsline {subsection}{\numberline {1.7.2}Unbiased $\hat {\theta }(T)$ with sufficient and complete $T$ is UMVU}{20}{subsection.1.7.2}%
\contentsline {section}{\numberline {1.8}Fisher Information}{22}{section.1.8}%
\contentsline {subsection}{\numberline {1.8.1}Score Function}{22}{subsection.1.8.1}%
\contentsline {subsection}{\numberline {1.8.2}Fisher Information}{23}{subsection.1.8.2}%
\contentsline {subsection}{\numberline {1.8.3}Cram√©r-Rao Lower Bound}{24}{subsection.1.8.3}%
\contentsline {section}{\numberline {1.9}Hypothesis Testing}{25}{section.1.9}%
\contentsline {subsection}{\numberline {1.9.1}Formulation of Testing Problem}{25}{subsection.1.9.1}%
\contentsline {subsection}{\numberline {1.9.2}Errors, Power Function, and Agenda}{26}{subsection.1.9.2}%
\contentsline {subsection}{\numberline {1.9.3}Choice of Critical Value}{27}{subsection.1.9.3}%
\contentsline {subsection}{\numberline {1.9.4}Choice of Test Statistic: Uniformly Most Powerful (UMP) Level $\alpha $ Test}{28}{subsection.1.9.4}%
\contentsline {subsection}{\numberline {1.9.5}Generalized Neyman-Pearson Lemma}{30}{subsection.1.9.5}%
\contentsline {section}{\numberline {1.10}Trinity of Classical Tests}{30}{section.1.10}%
\contentsline {subsection}{\numberline {1.10.1}Test Statistics}{30}{subsection.1.10.1}%
\contentsline {subsection}{\numberline {1.10.2}Approximation to $T_{LR}$}{31}{subsection.1.10.2}%
\contentsline {section}{\numberline {1.11}Interval Estimation}{32}{section.1.11}%
\contentsline {chapter}{\numberline {2}Decision Rule Based Statistical Inference}{34}{chapter.2}%
\contentsline {section}{\numberline {2.1}Decision Rule}{34}{section.2.1}%
\contentsline {section}{\numberline {2.2}Maximum-Likelihood Principle (state is norandom)}{35}{section.2.2}%
\contentsline {section}{\numberline {2.3}Bayesian Decision Rule (state is random)}{36}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Rules}{36}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Optimization Problem in Bayes Form}{36}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Maximum A Posteriori (MAP) Decision Rule (Binary example)}{38}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}Minimum Mean Squared Error (MMSE) Rule ($\mathbb {R}^n$ example)}{38}{subsection.2.3.4}%
\contentsline {section}{\numberline {2.4}Comparison}{39}{section.2.4}%
\contentsline {chapter}{\numberline {3}Non-parameteric Prediction Problem}{40}{chapter.3}%
\contentsline {section}{\numberline {3.1}$K$-normal Means Probelm}{41}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Assumptions}{41}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Maximum Likelihood Estimator}{42}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}Risk of MLE}{42}{subsection.3.1.3}%
\contentsline {subsection}{\numberline {3.1.4}James-Stein Type Estimator}{43}{subsection.3.1.4}%
\contentsline {subsubsection}{\numberline {3.1.4.1}Stein's Unbiased Risk Estimate (SURE)}{43}{subsubsection.3.1.4.1}%
\contentsline {subsubsection}{\numberline {3.1.4.2}James and Stein Estimator}{44}{subsubsection.3.1.4.2}%
\contentsline {subsubsection}{\numberline {3.1.4.3}A more general form of estimator $\mathcal {L}=\{C \mathbf {Z}:C=\text {diag}\vec {c},\vec {c}\in [0,1]^K\}$}{45}{subsubsection.3.1.4.3}%
\contentsline {chapter}{\numberline {4}M-Estimation}{46}{chapter.4}%
\contentsline {section}{\numberline {4.1}Consistency and Asymptotic Normality of M-estimator}{47}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Identification of M-estimator: $\theta _0=\operatorname *{argmin}_{\theta \in \Theta } Q(\theta )$}{47}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Consistency of M-estimators}{48}{subsection.4.1.2}%
\contentsline {subsection}{\numberline {4.1.3}Asymptotic Normality of M-estimators}{49}{subsection.4.1.3}%
\contentsline {section}{\numberline {4.2}Efficiency and Misspecification}{51}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Efficiency of Asymptotically Linear Estimator}{51}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Misspecification and Pseudo-true Parameter}{51}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Example of Misspecification}{52}{subsection.4.2.3}%
\contentsline {section}{\numberline {4.3}Binary Choice}{53}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Latent Utility Models (structural motivation for probit model)}{54}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Estimation: Binary Regression}{55}{subsection.4.3.2}%
\contentsline {subsection}{\numberline {4.3.3}Consistency and Asymptotic Normality}{56}{subsection.4.3.3}%
\contentsline {subsection}{\numberline {4.3.4}Example: Logistic Regression $F(t)=\frac {e^t}{1+e^t}$}{57}{subsection.4.3.4}%
\contentsline {section}{\numberline {4.4}Large Sample Testing}{57}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Wald Test: Distance on ``$x$ axis''}{57}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Lagrange Multiplier Test: Distance using ``gradient''}{58}{subsection.4.4.2}%
\contentsline {subsection}{\numberline {4.4.3}Likelihood Ratio Test}{58}{subsection.4.4.3}%
\contentsline {subsection}{\numberline {4.4.4}Wald is not invariant to parametrization}{59}{subsection.4.4.4}%
\contentsline {section}{\numberline {4.5}Nonlinear Least Square}{59}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}Efficient NLS: Weighted NLS}{61}{subsection.4.5.1}%
\contentsline {section}{\numberline {4.6}(Linear) Quantile Regression}{62}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Linear Quantile Regression Model}{62}{subsection.4.6.1}%
\contentsline {subsection}{\numberline {4.6.2}Quantile Causal Effects}{63}{subsection.4.6.2}%
\contentsline {subsection}{\numberline {4.6.3}M-estimator of Quantile}{65}{subsection.4.6.3}%
\contentsline {section}{\numberline {4.7}Example of M-estimator}{66}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}Optimal Weighting Example: $y_i=x_i^{\beta _0}+\epsilon _i$}{66}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}Conditional Beta Distribution $Beta(\alpha , 1)$}{67}{subsection.4.7.2}%
\contentsline {subsection}{\numberline {4.7.3}``Two-Sided'' Censored Regression Model}{69}{subsection.4.7.3}%
\contentsline {subsection}{\numberline {4.7.4}Regression Example: $y_i=exp\{x_i^T\beta _0\}+\epsilon _i$}{70}{subsection.4.7.4}%
\contentsline {subsection}{\numberline {4.7.5}Regression Example: $y_i=(\beta _0)^{x_i}+\epsilon _i$}{70}{subsection.4.7.5}%
\contentsline {subsection}{\numberline {4.7.6}Regression Example: $y_i=\log (x_i^T\beta _0)+\epsilon _i$}{70}{subsection.4.7.6}%
\contentsline {subsection}{\numberline {4.7.7}$Geometric(exp\{x_i^T\beta _0\})$ Distribution: $f(y;x_i^T\beta _0)=(1-exp\{x_i^T\beta _0\})exp\{y\cdot (x_i^T\beta _0)\}$}{71}{subsection.4.7.7}%
\contentsline {subsection}{\numberline {4.7.8}$Exponential(x_i^T\beta _0)$ Distribution: $f(y;x_i^T\beta _0)=x_i^T\beta _0\cdot exp\{-(x_i^T\beta _0) y\}$}{71}{subsection.4.7.8}%
\contentsline {chapter}{\numberline {5}Bootstrap}{73}{chapter.5}%
\contentsline {section}{\numberline {5.1}Traditional Monte-Carlo Approach}{73}{section.5.1}%
\contentsline {section}{\numberline {5.2}Bootstrap (When data is not enough)}{74}{section.5.2}%
\contentsline {section}{\numberline {5.3}Residual Bootstrap (for problem with not i.i.d. data)}{74}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Example: Linear}{75}{subsection.5.3.1}%
\contentsline {subsection}{\numberline {5.3.2}Example: Nonlinear Markov Process}{75}{subsection.5.3.2}%
\contentsline {section}{\numberline {5.4}Posterior Simulation / Bayesian (Weighted) Bootstrap}{76}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}Dirichlet Distribution Prior}{76}{subsection.5.4.1}%
\contentsline {subsection}{\numberline {5.4.2}Haldane Prior}{77}{subsection.5.4.2}%
\contentsline {subsection}{\numberline {5.4.3}Linear Model Case}{77}{subsection.5.4.3}%
\contentsline {subsection}{\numberline {5.4.4}Bernoulli Case}{78}{subsection.5.4.4}%
