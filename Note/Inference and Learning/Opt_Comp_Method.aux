\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nty/global//global/global}
\pgfsyspdfmark {pgfid1}{28388828}{17982806}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Math Foundations}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Strongly Convexity}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}$\mu $-Strongly Convex: $ \langle \nabla f(w)-\nabla f(v), w-v\rangle \geq \mu \|w-v\|^{2}$}{1}{subsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}$\mu $-strongly convex $\Leftrightarrow \nabla ^{2} f(x) \succeq \mu I\Leftrightarrow $"$f(x)-\frac  {m}{2}\|x\|^2$ is convex"}{1}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Lemma: Strongly convexity $\Rightarrow $ Strictly convexity}{1}{subsection.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Lemma: $\nabla ^2 f(x)\succeq mI$ $\Rightarrow $ $f(y)\geq f(x)+\nabla f(x)^T(y-x)+\frac  {m}{2}\|y-x\|^2$}{2}{subsection.1.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Lipschitz Gradient ($L$-Smooth)}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Theorem: $-MI\preceq \nabla ^2 f(x)\preceq MI$ $\Rightarrow $ $f$ is $M$-smooth}{3}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Descent Lemma: $f$ is $L$-smooth $\Rightarrow $ $f(y)\leq f(x)+\nabla f(x)^T(y-x)+\frac  {L}{2}\|y-x\|^2$}{3}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Co-coercivity Condition: $(\nabla f(x)-\nabla f(y))^T(x-y)\geq \frac  {1}{L}\|\nabla f(x)-\nabla f(y)\|^2$}{4}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}(Unconstrained Optimization) Gradient Methods}{5}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Steepest Descent}{5}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Methods for Choosing Step Size $\alpha _k$}{6}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Optimal (Exact) Line Search}{6}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Armijo's Rule}{6}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Armijo's Rule for Steepest Descent}{7}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Algorithm Convergence}{8}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Convergence of The Steepest Descent with Fixed Step Size}{8}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Theorem: $f$ is $L$-smooth $\Rightarrow $ $\{x_k\}$ converges to stationary point}{8}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Theorem: $f$ is convex and $L$-smooth $\Rightarrow $ $f(x_k)$ converges to global-min value with rate $\frac  {1}{k}$}{10}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Theorem: $f$ is strongly convex and $L-$smooth $\Rightarrow $ $\{x_k\}$ converges to global-min geometrically}{11}{subsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Convergence of Gradient Descent on Smooth Strongly-Convex Functions}{13}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}From convergence rate to iteration complexity}{17}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}(Unconstrained Optimization) Gradient Projection Methods}{18}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Projection onto Closed Convex Set}{18}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Def: Projection $[z]^\&$}{18}{subsection.3.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Projection onto Closed Convex Set\relax }}{18}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{}{{3.1}{18}{Projection onto Closed Convex Set\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Projection onto Closed non-Convex Set\relax }}{18}{figure.caption.3}\protected@file@percent }
\newlabel{}{{3.2}{18}{Projection onto Closed non-Convex Set\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Prop: \underline  {unique} projection $[z]^\&$ on \underline  {closed convex} subset of $\mathbb  {R}^n$}{19}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Obtuse Angle Criterion: $x=[z]^\&$ is projection on \underline  {closed convex}subset of $\mathbb  {R}^n$$\Leftrightarrow $ $(z-x)^T(y-x)\leq 0, \forall y\in \&$}{19}{subsection.3.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Obtuse angle criterion\relax }}{19}{figure.caption.4}\protected@file@percent }
\newlabel{}{{3.3}{19}{Obtuse angle criterion\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Necessary and Sufficient Condition for Projection\relax }}{20}{figure.caption.5}\protected@file@percent }
\newlabel{}{{3.4}{20}{Necessary and Sufficient Condition for Projection\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Prop: Projection is non-expansive $\|[x]^\&-[z]^\&\|\leq \|x-z\|,\forall x,z\in \mathbb  {R}^n$}{20}{subsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Projection on (Linear) Subspaces of $\mathbb  {R}^n$}{21}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Orthogonality Principle in subspaces of $\mathbb  {R}^n$: $(z-y^*)^Tx= 0,\forall x\in \&$}{21}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Gradient Projection Method}{21}{section.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Point from $\mathbb  {R}^2$ to $\mathbb  {R}$\relax }}{22}{figure.caption.6}\protected@file@percent }
\newlabel{}{{3.5}{22}{Point from $\mathbb {R}^2$ to $\mathbb {R}$\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Def: \underline  {fixed point} in fixed step-size steepest descent method, $\tilde  {x}=[\tilde  {x}-\alpha \nabla f(\tilde  {x})]^\&$}{22}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Prop: $L-$smooth, $0<\alpha <\frac  {2}{L}$ $\Rightarrow $ limit point is a fixed point (in fixed step-size steepest descent method)}{22}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Prop: $x$ is minimizer in convex func $\Leftrightarrow $ fixed point (in fixed step-size steepest descent method)}{23}{subsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Thm: Convergence of Gradient Projection: Convex, $L-$smooth, $0<\alpha <\frac  {2}{L}$ $\Rightarrow $ $f(x_k)\rightarrow f(x^*)$ at rate $\frac  {1}{k}$}{23}{subsection.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Thm: Strongly convex, Lipschitz gradient $\Rightarrow $ $\{x_k\}$ converges to $x^*$ geometrically}{23}{subsection.3.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}(Unconstrained Optimization) Sub-gradient Methods}{25}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Sub-gradient}{25}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Sub-differential}{25}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}More examples}{26}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}First-order necessary conditions for optimality in terms of subgradient}{27}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Properties of Subgradients}{28}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Sub-gradient Descent for Unconstrained Optimization}{28}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}(Revised) Sub-gradient "descent" with diminishing stepsize}{29}{section.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}(Unconstrained Optimization) Newton's Method}{32}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Classical Newton's Method}{32}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Variants of Newton's Method}{32}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Generalization to Optimization}{32}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}The Secant Method}{33}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}A New Interpretation of Newton's Method}{33}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Convergence of Newton's Method}{33}{section.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Guarantees of Convergence}{33}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Convergence Rate}{33}{subsection.5.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Note: Cons and Pros}{35}{section.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Modifications to ensure global convergence}{36}{section.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Quasi-Newton Methods}{36}{section.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}BFGS Method}{37}{subsection.5.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Trust-Region Method}{38}{section.5.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.9}Cubic Regularization}{38}{section.5.9}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}(Constrained Optimization) Barrier Method}{39}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Barrier Method}{39}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}An Exmaple Using KKT or Barrier}{40}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Solution using KKT conditions}{41}{subsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Solution using logarithmic barrier}{41}{subsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Penalty Method (For ECP)}{42}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Descent Method}{44}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Method of Steepest Descent}{44}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}The Method of Steepest Descent}{44}{subsection.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Properties of steepest descent}{45}{subsection.7.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}General Descent Method}{45}{section.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Criteria for a descent method}{45}{subsection.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Wolfe's theorem}{46}{subsection.7.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Picking step sizes}{46}{subsection.7.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.4}Picking descent directions}{47}{subsection.7.2.4}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{nobblfile}
\gdef \@abspage@last{51}
