\contentsline {chapter}{\numberline {1}Information-Theoretic Functional}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Definitions}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Entropy}{1}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Kullback-Leibler Divergence}{1}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Cross-Entropy}{2}{subsection.1.1.3}%
\contentsline {subsection}{\numberline {1.1.4}Mutual Information}{2}{subsection.1.1.4}%
\contentsline {chapter}{\numberline {2}Machine Learning in Inference}{3}{chapter.2}%
\contentsline {section}{\numberline {2.1}Empirical Risk Minimization (ERM)}{3}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Example: Linear MMSE (LMMSE) estimator}{3}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}Penalized ERM}{4}{subsection.2.1.2}%
\contentsline {section}{\numberline {2.2}Stochastic Approximation}{5}{section.2.2}%
\contentsline {section}{\numberline {2.3}Stochastic Gradient Descent (SGD)}{7}{section.2.3}%
\contentsline {section}{\numberline {2.4}SGD Application to Empirical Risk Minimization (ERM)}{8}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Different Gradient Descent for ERM}{9}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Constraints on Learning Problem}{9}{subsection.2.4.2}%
\contentsline {chapter}{\numberline {3}Stochastic Integration Methods}{12}{chapter.3}%
\contentsline {section}{\numberline {3.1}Deterministic Methods (Better in Low Dimension)}{12}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Riemann Integration}{12}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Trapezoidal Rule}{12}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}Multidimensional Integration}{13}{subsection.3.1.3}%
\contentsline {section}{\numberline {3.2}Stochastic Methods (Better in High Dimension)}{13}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Classical Monte Carlo Integration}{13}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Importance Sampling}{14}{subsection.3.2.2}%
\contentsline {chapter}{\numberline {4}Particle Filtering}{16}{chapter.4}%
\contentsline {section}{\numberline {4.1}Kalman Filtering (Linear Dynamic System)}{16}{section.4.1}%
\contentsline {section}{\numberline {4.2}Particle Filtering (Nonlinear Dynamic System)}{16}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Bayesian Recursive Filtering}{17}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Particle Filter (bootstrap filter)}{17}{subsection.4.2.2}%
\contentsline {chapter}{\numberline {5}EM Algorithm}{19}{chapter.5}%
\contentsline {section}{\numberline {5.1}General Structure of the EM Algorithm}{19}{section.5.1}%
\contentsline {section}{\numberline {5.2}Example 1: Variance Estimation}{21}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Maximum-Likelihood (ML) Estimation}{21}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}EM Algorithm}{21}{subsection.5.2.2}%
\contentsline {section}{\numberline {5.3}Example 2: Estimation of Gaussian Mixtures}{22}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Unknown Means: ML estimation is hard}{22}{subsection.5.3.1}%
\contentsline {subsection}{\numberline {5.3.2}Unknown Means: EM Algorithm}{23}{subsection.5.3.2}%
\contentsline {subsection}{\numberline {5.3.3}Unknown Mixture Probabilities, Means and Variances}{24}{subsection.5.3.3}%
\contentsline {section}{\numberline {5.4}Convergence of EM Algorithm}{24}{section.5.4}%
\contentsline {section}{\numberline {5.5}EM As an Alternating Maximization Algorithm}{25}{section.5.5}%
\contentsline {chapter}{\numberline {6}Hidden Markov model (HMM)}{27}{chapter.6}%
\contentsline {section}{\numberline {6.1}Viterbi Algorithm: (MAP) estimate $X_{1:t}$ given $Y_{1:t}$}{27}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}MAP estimation problem}{27}{subsection.6.1.1}%
\contentsline {subsection}{\numberline {6.1.2}Viterbi Algorithm}{28}{subsection.6.1.2}%
\contentsline {section}{\numberline {6.2}Bayesian Estimation of a Sequence: Need (MMSE) estimate $X_{1:t}$ given $Y_{1:t}$}{29}{section.6.2}%
\contentsline {section}{\numberline {6.3}Forward-Backward Algorithm: (MMSE) estimate $X_{1:t+1}$ given $Y_{1:t}$}{29}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}$\gamma _t(x) \triangleq \mathrm {P}\left \{X_t=x \mid \vec {Y}=\vec {y}\right \}$}{29}{subsection.6.3.1}%
\contentsline {subsection}{\numberline {6.3.2}$\xi _t\left (x, x^{\prime }\right ) \triangleq \mathrm {P}\left \{X_t=x, X_{t+1}=x^{\prime } \mid \vec {Y}=\vec {y}\right \}$}{31}{subsection.6.3.2}%
\contentsline {subsection}{\numberline {6.3.3}Scaling Factors}{31}{subsection.6.3.3}%
\contentsline {chapter}{\numberline {7}Graphic Models}{32}{chapter.7}%
\contentsline {section}{\numberline {7.1}Graph Theory}{32}{section.7.1}%
\contentsline {section}{\numberline {7.2}Bayesian Networks}{33}{section.7.2}%
\contentsline {section}{\numberline {7.3}Markov Networks}{33}{section.7.3}%
\contentsline {subsection}{\numberline {7.3.1}General Form}{33}{subsection.7.3.1}%
\contentsline {subsection}{\numberline {7.3.2}Hammersley-Clifford theorem}{34}{subsection.7.3.2}%
\contentsline {subsection}{\numberline {7.3.3}Form of Gibbs distribution (Boltzmann distribution)}{34}{subsection.7.3.3}%
\contentsline {section}{\numberline {7.4}Conversion of directed graph to undirected graph}{35}{section.7.4}%
\contentsline {section}{\numberline {7.5} Inference and Learning}{35}{section.7.5}%
\contentsline {subsection}{\numberline {7.5.1}Inference on Trees}{35}{subsection.7.5.1}%
\contentsline {chapter}{\numberline {8}Variational Inference, Mean-Field Techniques}{38}{chapter.8}%
\contentsline {section}{\numberline {8.1}Naive Mean-Field Methods}{38}{section.8.1}%
\contentsline {subsection}{\numberline {8.1.1}Graphical Models}{39}{subsection.8.1.1}%
\contentsline {subsection}{\numberline {8.1.2}Ising Model}{39}{subsection.8.1.2}%
\contentsline {section}{\numberline {8.2}Exponential Families of Probability Distributions}{41}{section.8.2}%
\contentsline {section}{\numberline {8.3}ML Estimation}{43}{section.8.3}%
\contentsline {section}{\numberline {8.4}Maximum Entropy}{43}{section.8.4}%
\contentsline {section}{\numberline {8.5}}{44}{section.8.5}%
\contentsline {section}{\numberline {8.6}Connection between Exponential Families and Graphic Models}{45}{section.8.6}%
\contentsline {subsection}{\numberline {8.6.1}Marginal polytope}{45}{subsection.8.6.1}%
\contentsline {subsection}{\numberline {8.6.2}Locally Consistent Marginal Distributions}{45}{subsection.8.6.2}%
\contentsline {subsection}{\numberline {8.6.3}Entropy on Tree Graphs}{47}{subsection.8.6.3}%
\contentsline {subsection}{\numberline {8.6.4}Naive Mean-Field Methods In Graph}{48}{subsection.8.6.4}%
\contentsline {subsection}{\numberline {8.6.5}Structural Mean Field Optimization}{48}{subsection.8.6.5}%
\contentsline {subsection}{\numberline {8.6.6}Bethe Entropy Approximation}{48}{subsection.8.6.6}%
\contentsline {chapter}{\numberline {9}$\ell _1$ Penalized Least Squares Minimization}{50}{chapter.9}%
\contentsline {section}{\numberline {9.1}Problem Statement}{50}{section.9.1}%
\contentsline {section}{\numberline {9.2}Special Cases}{51}{section.9.2}%
\contentsline {subsection}{\numberline {9.2.1}Definition: Soft Threshold}{51}{subsection.9.2.1}%
\contentsline {subsection}{\numberline {9.2.2}Identity $A$}{51}{subsection.9.2.2}%
\contentsline {subsection}{\numberline {9.2.3}Orthonormal $A$}{51}{subsection.9.2.3}%
\contentsline {subsection}{\numberline {9.2.4}Quadratic Optimization ($\lambda =0$)}{51}{subsection.9.2.4}%
\contentsline {section}{\numberline {9.3}General Solution: Lasso}{52}{section.9.3}%
\contentsline {section}{\numberline {9.4}General Solution: Iterative Soft Thresholding Algorithm (ISTA)}{52}{section.9.4}%
\contentsline {subsection}{\numberline {9.4.1}Proximal Minimization Algorithm}{52}{subsection.9.4.1}%
\contentsline {subsection}{\numberline {9.4.2}Apply to $\ell _1$-penalized least-squares}{53}{subsection.9.4.2}%
\contentsline {section}{\numberline {9.5}Convergence Rate}{53}{section.9.5}%
\contentsline {section}{\numberline {9.6}Fast Iterative Soft Thresholding Algorithm (FISTA)}{54}{section.9.6}%
\contentsline {section}{\numberline {9.7}Alternating Direction Method of Multipliers (ADMM)}{54}{section.9.7}%
\contentsline {chapter}{\numberline {10}Compressive Sensing}{56}{chapter.10}%
\contentsline {section}{\numberline {10.1}Definitions related to Sparsity}{56}{section.10.1}%
\contentsline {section}{\numberline {10.2}Measurement Matrix}{58}{section.10.2}%
\contentsline {subsection}{\numberline {10.2.1}Matrix Preliminaries}{58}{subsection.10.2.1}%
\contentsline {subsection}{\numberline {10.2.2}Recovery of k-Sparse Signals}{59}{subsection.10.2.2}%
\contentsline {subsection}{\numberline {10.2.3}Restricted Isometry Property}{60}{subsection.10.2.3}%
\contentsline {section}{\numberline {10.3}Robust Signal Recovery from Noiseless Observations}{60}{section.10.3}%
\contentsline {section}{\numberline {10.4}Robust Signal Recovery from Noisy Observations}{62}{section.10.4}%
\contentsline {subsection}{\numberline {10.4.1}Bounded Noise}{62}{subsection.10.4.1}%
