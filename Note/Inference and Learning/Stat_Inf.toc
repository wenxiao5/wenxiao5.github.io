\contentsline {chapter}{\numberline {1}Statistics Basics}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Random Sampling}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Sample Mean and Sample Variance}{1}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Distributional Properties}{1}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Order Statistics}{2}{subsection.1.1.3}%
\contentsline {section}{\numberline {1.2}Basic Statistics}{2}{section.1.2}%
\contentsline {section}{\numberline {1.3}Point Estimation}{3}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Method of Moments (MM)}{3}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Maximum Likelihood (ML)}{5}{subsection.1.3.2}%
\contentsline {chapter}{\numberline {2}Information-Theoretic Functional}{7}{chapter.2}%
\contentsline {chapter}{\numberline {3}Statistical Inference}{8}{chapter.3}%
\contentsline {section}{\numberline {3.1}Basics}{8}{section.3.1}%
\contentsline {section}{\numberline {3.2}Decision Rule Examples}{8}{section.3.2}%
\contentsline {section}{\numberline {3.3}Maximum-Likelihood Principle (state is norandom)}{9}{section.3.3}%
\contentsline {section}{\numberline {3.4}Bayesian Decision Rule (state is random)}{10}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Rules}{10}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Maximum A Posteriori (MAP) Decision Rule (Binary example)}{11}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Minimum Mean Squared Error (MMSE) Rule ($\mathbb {R}^n$ example)}{11}{subsection.3.4.3}%
\contentsline {section}{\numberline {3.5}Comparison}{12}{section.3.5}%
\contentsline {chapter}{\numberline {4}Machine Learning in Inference}{13}{chapter.4}%
\contentsline {section}{\numberline {4.1}Empirical Risk Minimization (ERM)}{13}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Example: Linear MMSE (LMMSE) estimator}{13}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Penalized ERM}{14}{subsection.4.1.2}%
\contentsline {section}{\numberline {4.2}Stochastic Approximation}{15}{section.4.2}%
\contentsline {section}{\numberline {4.3}Stochastic Gradient Descent (SGD)}{17}{section.4.3}%
\contentsline {section}{\numberline {4.4}SGD Application to Empirical Risk Minimization (ERM)}{18}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Different Gradient Descent for ERM}{19}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Constraints on Learning Problem}{19}{subsection.4.4.2}%
\contentsline {chapter}{\numberline {5}Stochastic Integration Methods}{21}{chapter.5}%
\contentsline {section}{\numberline {5.1}Deterministic Methods (Better in Low Dimension)}{21}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Riemann Integration}{21}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Trapezoidal Rule}{21}{subsection.5.1.2}%
\contentsline {subsection}{\numberline {5.1.3}Multidimensional Integration}{22}{subsection.5.1.3}%
\contentsline {section}{\numberline {5.2}Stachastic Methods (Better in High Dimension)}{22}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Classical Monte Carlo Integration}{22}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Importance Sampling}{23}{subsection.5.2.2}%
\contentsline {chapter}{\numberline {6}Bootstrap (not enough data)}{25}{chapter.6}%
\contentsline {section}{\numberline {6.1}Residual Bootstrap}{25}{section.6.1}%
\contentsline {chapter}{\numberline {7}Particle Filtering}{27}{chapter.7}%
\contentsline {section}{\numberline {7.1}Kalman Filtering (Linear Dynamic System)}{27}{section.7.1}%
\contentsline {section}{\numberline {7.2}Particle Filtering (Nonlinear Dynamic System)}{27}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}Bayesian Recursive Filtering}{28}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}Particle Filter (bootstrap filter)}{28}{subsection.7.2.2}%
\contentsline {chapter}{\numberline {8}EM Algorithm}{30}{chapter.8}%
\contentsline {section}{\numberline {8.1}General Structure of the EM Algorithm}{30}{section.8.1}%
\contentsline {section}{\numberline {8.2}Example 1: Variance Estimation}{32}{section.8.2}%
\contentsline {subsection}{\numberline {8.2.1}Maximum-Likelihood (ML) Estimation}{32}{subsection.8.2.1}%
\contentsline {subsection}{\numberline {8.2.2}EM Algorithm}{32}{subsection.8.2.2}%
\contentsline {section}{\numberline {8.3}Example 2: Estimation of Gaussian Mixtures}{33}{section.8.3}%
\contentsline {subsection}{\numberline {8.3.1}Unknown Means: ML estimation is hard}{33}{subsection.8.3.1}%
\contentsline {subsection}{\numberline {8.3.2}Unknown Means: EM Algorithm}{34}{subsection.8.3.2}%
\contentsline {subsection}{\numberline {8.3.3}Unknown Mixture Probabilities, Means and Variances}{35}{subsection.8.3.3}%
\contentsline {section}{\numberline {8.4}Convergence of EM Algorithm}{35}{section.8.4}%
\contentsline {section}{\numberline {8.5}EM As an Alternating Maximization Algorithm}{36}{section.8.5}%
\contentsline {chapter}{\numberline {9}Hidden Markov model (HMM)}{38}{chapter.9}%
\contentsline {section}{\numberline {9.1}Viterbi Algorithm: (MAP) estimate $X_{1:t}$ given $Y_{1:t}$}{38}{section.9.1}%
\contentsline {subsection}{\numberline {9.1.1}MAP estimation problem}{38}{subsection.9.1.1}%
\contentsline {subsection}{\numberline {9.1.2}Viterbi Algorithm}{39}{subsection.9.1.2}%
\contentsline {section}{\numberline {9.2}Bayesian Estimation of a Sequence: Need (MMSE) estimate $X_{1:t}$ given $Y_{1:t}$}{40}{section.9.2}%
\contentsline {section}{\numberline {9.3}Forward-Backward Algorithm: (MMSE) estimate $X_{1:t+1}$ given $Y_{1:t}$}{40}{section.9.3}%
\contentsline {subsection}{\numberline {9.3.1}$\gamma _t(x) \triangleq \mathrm {P}\left \{X_t=x \mid \vec {Y}=\vec {y}\right \}$}{40}{subsection.9.3.1}%
\contentsline {subsection}{\numberline {9.3.2}$\xi _t\left (x, x^{\prime }\right ) \triangleq \mathrm {P}\left \{X_t=x, X_{t+1}=x^{\prime } \mid \vec {Y}=\vec {y}\right \}$}{42}{subsection.9.3.2}%
\contentsline {subsection}{\numberline {9.3.3}Scaling Factors}{42}{subsection.9.3.3}%
\contentsline {chapter}{\numberline {10}Graphic Models}{43}{chapter.10}%
\contentsline {section}{\numberline {10.1}Graph Theory}{43}{section.10.1}%
\contentsline {section}{\numberline {10.2}Bayesian Networks}{44}{section.10.2}%
\contentsline {section}{\numberline {10.3}Markov Networks}{44}{section.10.3}%
\contentsline {subsection}{\numberline {10.3.1}General Form}{44}{subsection.10.3.1}%
\contentsline {subsection}{\numberline {10.3.2}Hammersley-Clifford theorem}{45}{subsection.10.3.2}%
\contentsline {subsection}{\numberline {10.3.3}Form of Gibbs distribution (Boltzmann distribution)}{45}{subsection.10.3.3}%
\contentsline {section}{\numberline {10.4}Conversion of directed graph to undirected graph}{46}{section.10.4}%
\contentsline {section}{\numberline {10.5} Inference and Learning}{46}{section.10.5}%
\contentsline {subsection}{\numberline {10.5.1}Inference on Trees}{46}{subsection.10.5.1}%
\contentsline {chapter}{\numberline {11}Variational Inference, Mean-Field Techniques}{49}{chapter.11}%
\contentsline {section}{\numberline {11.1}Naive Mean-Field Methods}{49}{section.11.1}%
\contentsline {subsection}{\numberline {11.1.1}Graphical Models}{50}{subsection.11.1.1}%
\contentsline {subsection}{\numberline {11.1.2}Ising Model}{50}{subsection.11.1.2}%
\contentsline {section}{\numberline {11.2}Exponential Families of Probability Distributions}{52}{section.11.2}%
\contentsline {section}{\numberline {11.3}ML Estimation}{54}{section.11.3}%
\contentsline {section}{\numberline {11.4}Maximum Entropy}{54}{section.11.4}%
\contentsline {section}{\numberline {11.5}}{55}{section.11.5}%
\contentsline {section}{\numberline {11.6}Connection between Exponential Families and Graphic Models}{56}{section.11.6}%
\contentsline {subsection}{\numberline {11.6.1}Marginal polytope}{56}{subsection.11.6.1}%
\contentsline {subsection}{\numberline {11.6.2}Locally Consistent Marginal Distributions}{56}{subsection.11.6.2}%
\contentsline {subsection}{\numberline {11.6.3}Entropy on Tree Graphs}{58}{subsection.11.6.3}%
\contentsline {subsection}{\numberline {11.6.4}Naive Mean-Field Methods In Graph}{59}{subsection.11.6.4}%
\contentsline {subsection}{\numberline {11.6.5}Structural Mean Field Optimization}{59}{subsection.11.6.5}%
\contentsline {subsection}{\numberline {11.6.6}Bethe Entropy Approximation}{59}{subsection.11.6.6}%
\contentsline {chapter}{\numberline {12}$\ell _1$ Penalized Least Squares Minimization}{61}{chapter.12}%
\contentsline {section}{\numberline {12.1}Problem Statement}{61}{section.12.1}%
\contentsline {section}{\numberline {12.2}Special Cases}{62}{section.12.2}%
\contentsline {subsection}{\numberline {12.2.1}Definition: Soft Threshold}{62}{subsection.12.2.1}%
\contentsline {subsection}{\numberline {12.2.2}Identity $A$}{62}{subsection.12.2.2}%
\contentsline {subsection}{\numberline {12.2.3}Orthonormal $A$}{62}{subsection.12.2.3}%
\contentsline {subsection}{\numberline {12.2.4}Quadratic Optimization ($\lambda =0$)}{62}{subsection.12.2.4}%
\contentsline {section}{\numberline {12.3}General Solution: Lasso}{63}{section.12.3}%
\contentsline {section}{\numberline {12.4}General Solution: Iterative Soft Thresholding Algorithm (ISTA)}{63}{section.12.4}%
\contentsline {subsection}{\numberline {12.4.1}Proximal Minimization Algorithm}{63}{subsection.12.4.1}%
\contentsline {subsection}{\numberline {12.4.2}Apply to $\ell _1$-penalized least-squares}{64}{subsection.12.4.2}%
\contentsline {section}{\numberline {12.5}Convergence Rate}{64}{section.12.5}%
\contentsline {section}{\numberline {12.6}Fast Iterative Soft Thresholding Algorithm (FISTA)}{65}{section.12.6}%
\contentsline {section}{\numberline {12.7}Alternating Direction Method of Multipliers (ADMM)}{65}{section.12.7}%
\contentsline {chapter}{\numberline {13}Compressive Sensing}{67}{chapter.13}%
\contentsline {section}{\numberline {13.1}Definitions related to Sparsity}{67}{section.13.1}%
\contentsline {section}{\numberline {13.2}Measurement Matrix}{69}{section.13.2}%
\contentsline {subsection}{\numberline {13.2.1}Matrix Preliminaries}{69}{subsection.13.2.1}%
\contentsline {subsection}{\numberline {13.2.2}Recovery of k-Sparse Signals}{70}{subsection.13.2.2}%
\contentsline {subsection}{\numberline {13.2.3}Restricted Isometry Property}{71}{subsection.13.2.3}%
\contentsline {section}{\numberline {13.3}Robust Signal Recovery from Noiseless Observations}{71}{section.13.3}%
\contentsline {section}{\numberline {13.4}Robust Signal Recovery from Noisy Observations}{73}{section.13.4}%
\contentsline {subsection}{\numberline {13.4.1}Bounded Noise}{73}{subsection.13.4.1}%
