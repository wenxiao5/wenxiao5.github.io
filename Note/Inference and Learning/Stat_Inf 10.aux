\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nty/global//global/global}
\pgfsyspdfmark {pgfid1}{28388828}{16769476}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Information-Theoretic Functional}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Definitions}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Entropy}{1}{subsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Kullback-Leibler Divergence}{1}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Cross-Entropy}{2}{subsection.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Mutual Information}{2}{subsection.1.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Statistics Basics}{3}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Random Sampling}{3}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Sample Mean and Sample Variance}{3}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Distributional Properties}{4}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Order Statistics}{4}{subsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Basic Statistics}{5}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Point Estimation}{6}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Method of Moments (MM)}{6}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Maximum Likelihood (ML)}{8}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Comparing Estimators: Mean Squared Error}{9}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Sufficient Statistic}{10}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Minimal Sufficient Statistic}{12}{subsection.2.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Complete Statistic}{13}{subsection.2.3.6}\protected@file@percent }
\newlabel{comp_exp_fam}{{2.7}{14}{Statistics Basics}{tcb@cnt@theorem.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.7}Cramér-Rao Lower Bound}{14}{subsection.2.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Hypothesis Testing}{16}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Formulation of Testing Problem}{16}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Errors, Power Function, and Agenda}{17}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Choice of Critical Value}{18}{subsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Choice of Test Statistic: Uniformly Most Powerful (UMP) Level $\alpha $ Test}{19}{subsection.2.4.4}\protected@file@percent }
\newlabel{NP_lemma}{{2.9}{19}{Statistics Basics}{tcb@cnt@theorem.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}Generalized Neyman-Pearson Lemma}{21}{subsection.2.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Trinity of Classical Tests}{21}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Test Statistics}{22}{subsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Approximation to $T_{LR}$}{23}{subsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Interval Estimation}{23}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Decision Rule Based Statistical Inference}{25}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Decision Rule}{25}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Maximum-Likelihood Principle (state is norandom)}{25}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Bayesian Decision Rule (state is random)}{26}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Rules}{26}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Optimization Problem in Bayes Form}{27}{subsection.3.3.2}\protected@file@percent }
\newlabel{ex:square_loss}{{3.4}{28}{Decision Rule Based Statistical Inference}{exam.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Maximum A Posteriori (MAP) Decision Rule (Binary example)}{29}{subsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Minimum Mean Squared Error (MMSE) Rule ($\mathbb  {R}^n$ example)}{29}{subsection.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Comparison}{30}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Bootstrap}{31}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Traditional Monte-Carlo Approach}{31}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Bootstrap (When data is not enough)}{32}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Residual Bootstrap (for problem with not i.i.d. data)}{32}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Example: Linear}{33}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Example: Nonlinear Markov Process}{33}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Posterior Simulation / Bayesian (Weighted) Bootstrap}{34}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Dirichlet Distribution Prior}{34}{subsection.4.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Dirichlet Distribution Examples}}{35}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{}{{4.1}{35}{Dirichlet Distribution Examples}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Haldane Prior}{35}{subsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Linear Model Case}{35}{subsection.4.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Bernoulli Case}{36}{subsection.4.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Nonparameteric Prediction Probelm}{37}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{SSR}{{{SSR}}{37}{Nonparameteric Prediction Probelm}{AMS.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.1}$K$-normal Means Probelm}{38}{subsection.5.0.1}\protected@file@percent }
\newlabel{GSO}{{5.2}{38}{Nonparameteric Prediction Probelm}{tcb@cnt@definition.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Stein's Unbiased Risk Estimate (SURE)}{40}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Machine Learning in Inference}{41}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Empirical Risk Minimization (ERM)}{41}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Example: Linear MMSE (LMMSE) estimator}{41}{subsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Penalized ERM}{42}{subsection.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Stochastic Approximation}{43}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Stochastic Gradient Descent (SGD)}{45}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.4}SGD Application to Empirical Risk Minimization (ERM)}{46}{section.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Different Gradient Descent for ERM}{47}{subsection.6.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Constraints on Learning Problem}{47}{subsection.6.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Stochastic Integration Methods}{49}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Deterministic Methods (Better in Low Dimension)}{49}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Riemann Integration}{49}{subsection.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Trapezoidal Rule}{49}{subsection.7.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces (a) Riemann approximation; (b) Trapezoidal approximation.}}{49}{figure.caption.4}\protected@file@percent }
\newlabel{}{{7.1}{49}{(a) Riemann approximation; (b) Trapezoidal approximation}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}Multidimensional Integration}{50}{subsection.7.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Two-dimensional integration using regular grid.}}{50}{figure.caption.5}\protected@file@percent }
\newlabel{}{{7.2}{50}{Two-dimensional integration using regular grid}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Stochastic Methods (Better in High Dimension)}{50}{section.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Classical Monte Carlo Integration}{50}{subsection.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Importance Sampling}{51}{subsection.7.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Particle Filtering}{53}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Kalman Filtering (Linear Dynamic System)}{53}{section.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Particle Filtering (Nonlinear Dynamic System)}{53}{section.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Hidden Markov Model}}{54}{figure.caption.6}\protected@file@percent }
\newlabel{}{{8.1}{54}{Hidden Markov Model}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Bayesian Recursive Filtering}{54}{subsection.8.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Particle Filter (bootstrap filter)}{54}{subsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {9}EM Algorithm}{56}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}General Structure of the EM Algorithm}{56}{section.9.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces Complete and incomplete data spaces $Z$ and $Y$.}}{57}{figure.caption.10}\protected@file@percent }
\newlabel{}{{9.1}{57}{Complete and incomplete data spaces $Z$ and $Y$}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Example 1: Variance Estimation}{58}{section.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Maximum-Likelihood (ML) Estimation}{58}{subsection.9.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}EM Algorithm}{58}{subsection.9.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Example 2: Estimation of Gaussian Mixtures}{59}{section.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}Unknown Means: ML estimation is hard}{59}{subsection.9.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Unknown Means: EM Algorithm}{60}{subsection.9.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}Unknown Mixture Probabilities, Means and Variances}{61}{subsection.9.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Convergence of EM Algorithm}{61}{section.9.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.5}EM As an Alternating Maximization Algorithm}{62}{section.9.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Hidden Markov model (HMM)}{64}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces Hidden Markov Model (HMM)}}{64}{figure.caption.12}\protected@file@percent }
\newlabel{}{{10.1}{64}{Hidden Markov Model (HMM)}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Viterbi Algorithm: (MAP) estimate $X_{1:t}$ given $Y_{1:t}$}{64}{section.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.1}MAP estimation problem}{64}{subsection.10.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.2}Viterbi Algorithm}{65}{subsection.10.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces (a) Trellis diagram; (b)—(e) evolution of the Viterbi algorithm, showing surviving paths and values $V(t,x)$ at times $t = 2,3,4,5$; (f) optimal path $\vec  {x}^* = (0,2,0,2,0)$ and its value $\varepsilon (\vec  {x}^*) = 11$.}}{65}{figure.caption.13}\protected@file@percent }
\newlabel{}{{10.2}{65}{(a) Trellis diagram; (b)—(e) evolution of the Viterbi algorithm, showing surviving paths and values $V(t,x)$ at times $t = 2,3,4,5$; (f) optimal path $\vec {x}^* = (0,2,0,2,0)$ and its value $\varepsilon (\vec {x}^*) = 11$}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Bayesian Estimation of a Sequence: Need (MMSE) estimate $X_{1:t}$ given $Y_{1:t}$}{66}{section.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10.3}Forward-Backward Algorithm: (MMSE) estimate $X_{1:t+1}$ given $Y_{1:t}$}{66}{section.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.1}$\gamma _t(x) \triangleq \mathrm  {P}\left \{X_t=x \mid \vec  {Y}=\vec  {y}\right \}$}{66}{subsection.10.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.2}$\xi _t\left (x, x^{\prime }\right ) \triangleq \mathrm  {P}\left \{X_t=x, X_{t+1}=x^{\prime } \mid \vec  {Y}=\vec  {y}\right \}$}{68}{subsection.10.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.3}Scaling Factors}{68}{subsection.10.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Graphic Models}{69}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Graph Theory}{69}{section.11.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.1}{\ignorespaces (a) Directed and (b) Undirected graph.}}{70}{figure.caption.14}\protected@file@percent }
\newlabel{}{{11.1}{70}{(a) Directed and (b) Undirected graph}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.2}Bayesian Networks}{70}{section.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.3}Markov Networks}{70}{section.11.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.1}General Form}{70}{subsection.11.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.2}{\ignorespaces (a) (b) Two Bayesian networks and (c) a Markov network.}}{70}{figure.caption.15}\protected@file@percent }
\newlabel{}{{11.2}{70}{(a) (b) Two Bayesian networks and (c) a Markov network}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.2}Hammersley-Clifford theorem}{71}{subsection.11.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.3}Form of Gibbs distribution (Boltzmann distribution)}{71}{subsection.11.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.4}Conversion of directed graph to undirected graph}{72}{section.11.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.3}{\ignorespaces Graph moralization}}{72}{figure.caption.16}\protected@file@percent }
\newlabel{}{{11.3}{72}{Graph moralization}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.5} Inference and Learning}{72}{section.11.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.1}Inference on Trees}{72}{subsection.11.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.4}{\ignorespaces Example 1}}{72}{figure.caption.17}\protected@file@percent }
\newlabel{}{{11.4}{72}{Example 1}{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.5}{\ignorespaces Belief propagation in a tree}}{73}{figure.caption.18}\protected@file@percent }
\newlabel{}{{11.5}{73}{Belief propagation in a tree}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Variational Inference, Mean-Field Techniques}{75}{chapter.12}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Naive Mean-Field Methods}{75}{section.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.1}Graphical Models}{76}{subsection.12.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.2}Ising Model}{76}{subsection.12.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.2}Exponential Families of Probability Distributions}{78}{section.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.3}ML Estimation}{80}{section.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.4}Maximum Entropy}{80}{section.12.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.5}}{81}{section.12.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.6}Connection between Exponential Families and Graphic Models}{82}{section.12.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.6.1}Marginal polytope}{82}{subsection.12.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.6.2}Locally Consistent Marginal Distributions}{82}{subsection.12.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.6.3}Entropy on Tree Graphs}{84}{subsection.12.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.6.4}Naive Mean-Field Methods In Graph}{85}{subsection.12.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.6.5}Structural Mean Field Optimization}{85}{subsection.12.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.6.6}Bethe Entropy Approximation}{85}{subsection.12.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {13}$\ell _1$ Penalized Least Squares Minimization}{87}{chapter.13}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {13.1}Problem Statement}{87}{section.13.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13.2}Special Cases}{88}{section.13.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.1}Definition: Soft Threshold}{88}{subsection.13.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.2}Identity $A$}{88}{subsection.13.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.3}Orthonormal $A$}{88}{subsection.13.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.4}Quadratic Optimization ($\lambda =0$)}{88}{subsection.13.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13.3}General Solution: Lasso}{89}{section.13.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13.4}General Solution: Iterative Soft Thresholding Algorithm (ISTA)}{89}{section.13.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.1}Proximal Minimization Algorithm}{89}{subsection.13.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.2}Apply to $\ell _1$-penalized least-squares}{90}{subsection.13.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13.5}Convergence Rate}{90}{section.13.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13.6}Fast Iterative Soft Thresholding Algorithm (FISTA)}{91}{section.13.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13.7}Alternating Direction Method of Multipliers (ADMM)}{91}{section.13.7}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {14}Compressive Sensing}{93}{chapter.14}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {14.1}Definitions related to Sparsity}{93}{section.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14.2}Measurement Matrix}{95}{section.14.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.1}{\ignorespaces Measurement of sparse signal $\vec  {x}$ with support set $\Omega $ of size $k$.}}{95}{figure.caption.19}\protected@file@percent }
\newlabel{}{{14.1}{95}{Measurement of sparse signal $\vec {x}$ with support set $\Omega $ of size $k$}{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.1}Matrix Preliminaries}{95}{subsection.14.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.2}Recovery of k-Sparse Signals}{96}{subsection.14.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.3}Restricted Isometry Property}{97}{subsection.14.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14.3}Robust Signal Recovery from Noiseless Observations}{97}{section.14.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.2}{\ignorespaces $\ell _1$ recovery for $n=2$ and $m=1$}}{98}{figure.caption.20}\protected@file@percent }
\newlabel{}{{14.2}{98}{$\ell _1$ recovery for $n=2$ and $m=1$}{figure.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.4}Robust Signal Recovery from Noisy Observations}{99}{section.14.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.1}Bounded Noise}{99}{subsection.14.4.1}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{nobblfile}
\gdef \@abspage@last{105}
