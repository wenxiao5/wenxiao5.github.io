\documentclass[12pt]{elegantbook}
\usepackage{graphicx}
%\usepackage{float}
\definecolor{structurecolor}{RGB}{40,58,129}
\linespread{1.6}
\setlength{\footskip}{20pt}
\setlength{\parindent}{0pt}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\elegantnewtheorem{proof}{Proof}{}{Proof}
\elegantnewtheorem{claim}{Claim}{prostyle}{Claim}
\DeclareMathOperator{\col}{col}
\title{Time Series}
\author{Wenxiao Yang}
\institute{Haas School of Business, University of California Berkeley}
\date{2024}
\setcounter{tocdepth}{2}
\extrainfo{All models are wrong, but some are useful.}

\cover{cover.png}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}

%\addbibresource[location=local]{reference.bib} % bib

\begin{document}
\maketitle

\frontmatter
\tableofcontents

\mainmatter

\chapter{Univariate Stationary Time Series Analysis}
\section{Goals and Challenge}
\textbf{Data} in time series is denoted by
\begin{equation}
    \begin{aligned}
        \{\underbrace{y_t}_{n\times 1}:1\leq t\leq T\}
    \end{aligned}
    \nonumber
\end{equation}
%Some fundamental assumptions are needed for statistics.
\begin{assumption}
    Each $y_t$ is the realization of some random vector $Y_t$.
\end{assumption}
The \textbf{objective} is to provide data-based answers to questions about the distribution of $\{Y_t:1\leq t\leq T\}$.

The \textbf{challenge} we face is $Y_1,Y_2,...,Y_T$ are \textit{not necessarily independent}. Time series analysis gives the models and methods that can accommodate dependence.

\section{Stochastic Processes}
Some terminologies we need to know:
\begin{definition}[Stochastic Process]
    A \textbf{stochastic process} is a collection $\{Y_t:t\in\mathcal{T}\}$ of random variables/vectors (defined on the same probability space).
    \begin{enumerate}
        \item $\{Y_t:t\in\mathcal{T}\}$ is \textbf{discrete time process} if $\mathcal{T}=\{1,...,T\}$ or $\mathcal{T}=\mathbb{N}=\{1,2,...\}$ or $\mathcal{T}=\mathbb{Z}=\{...,-1,0,1,...\}$.
        \item $\{Y_t:t\in\mathcal{T}\}$ is \textbf{continuous time process} if $\mathcal{T}=[0,1]$ or $\mathcal{T}=\mathbb{R}_+$ or $\mathcal{T}=\mathbb{R}$.
    \end{enumerate}
\end{definition}
Observed data $Y_t$ is a realization of a discrete time process with $\mathcal{T}=\{1,...,T\}$.

\subsection{Strictly Stationary}
\begin{definition}[Strictly Stationary (Discrete and Scalar Process)]
    A scalar\footnote{i.e., $Y_t$ is $1\times 1$} process $\{Y_t:t\in \mathbb{Z}\}$ is \textbf{strictly stationary} \textit{if and only if}
    \begin{equation}
        \begin{aligned}
            \left(Y_t,...,Y_{t+k}\right)\underbrace{\sim}_\textnormal{``is distributed as''} \left(Y_0,...,Y_{k}\right),\ \forall t\in \mathbb{Z},k\geq 0
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}
\begin{note}
    \begin{enumerate}
        \item If $Y_t\sim i.i.d.$, then $\{Y_t:t\in \mathbb{Z}\}$ is strictly stationary.
        \item If $\{Y_t:t\in \mathbb{Z}\}$ is strictly stationary, then $Y_t$ are identically distributed (i.e., ``marginal stationary'').
        \begin{example}[ Strictly Stationary and Dependent]
            A constant process that $...=Y_{-1}=Y_0=Y_1=...$ is strictly stationary.
        \end{example}
    \end{enumerate}
    All these above hold for strictly stationary vector process.
\end{note}

\begin{lemma}[Property of Strictly Stationary]
    If $\{Y_t:t\in \mathbb{Z}\}$ is strictly stationary with $\mathbb{E}[Y_t^2]<\infty$ ($\forall t$), then
    \begin{enumerate}
        \item Same Expectation:
        \begin{equation}
            \begin{aligned}
                \mathbb{E}[Y_t]=\mu,\ \forall t \textnormal{ (for some constant $\mu$)}
            \end{aligned}
            \label{s}
            \tag{*}
        \end{equation}
        \item Covariance only depends on time length:
        \begin{equation}
            \begin{aligned}
                \textnormal{Cov}(Y_t,Y_{t-j})=\gamma(j),\ \forall t,j \textnormal{ (for some function $\gamma(\cdot)$)}
            \end{aligned}
            \label{ss}
            \tag{**}
        \end{equation}
        Note $\gamma(0)= \textnormal{Var}(Y_t),\forall t$.
    \end{enumerate}
\end{lemma}

\subsection{Covariance Stationary}
A subset of strictly stationary processes that has second moment (i.e., $\mathbb{E}[Y_t^2]<\infty$) can be defined as \textbf{covariance stationary}.
\begin{definition}[Covariance Stationary]
    A process $\{Y_t:t\in \mathbb{Z}\}$ is \textbf{covariance stationary} \textit{iff} $\mathbb{E}[Y_t^2]<\infty$ ($\forall t$) and it satisfies \eqref{s} and \eqref{ss}.
\end{definition}
\begin{note}
    Not every strictly stationary process is covariance stationary. (e.g., if it does not have second moment).
\end{note}

\subsection{Autocovariance and Autocorrelation Functions}
\begin{definition}[Autocovariance and Autocorrelation Functions]
    $\gamma(\cdot)$ in \eqref{ss} is called \textbf{autocovariance function} of $\{Y_t:t\in \mathbb{Z}\}$.\\
    The \textbf{autocorrelation function} is $\rho(j)=\textnormal{Corr}(Y_t,Y_{t-j})=\frac{Cov(Y_t,Y_{t-j})}{\sqrt{\textnormal{Var}(Y_t)\textnormal{Var}(Y_{t-j})}}=\frac{\gamma(j)}{\gamma(0)}$.
\end{definition}
\begin{lemma}[ACF Property]\label{lemma_ACF property}
    The autocovariance function satisfies the following properties:
    \begin{enumerate}
        \item $\gamma(\cdot)$ is \textbf{even} i.e., $\gamma(j)=\gamma(-j)$.
        \item $\gamma(\cdot)$ is \textbf{positive semi-definite} (psd) i.e., for any  $n\in \mathbb{N}$ and any $a_1,...,a_n$,
        \begin{equation}
            \begin{aligned}
                \sum_{i=1}^n\sum_{j=1}^na_ia_j\gamma(i-j)=\textnormal{Var}(\sum_{i=1}^na_iY_i)\geq 0
            \end{aligned}
            \nonumber
        \end{equation}
    \end{enumerate}
\end{lemma}


\section{Moving-Average (MA) Process}
\begin{definition}[White Noise]
    A process $\{\epsilon_t:t\in \mathbb{Z}\}$ is a \textbf{white noise} process iff it is covariance stationary with $\mathbb{E}[\epsilon_t]=0$ and
    \begin{equation}
        \begin{aligned}
            \textnormal{Cov}(\epsilon_t,\epsilon_{t-j})=\left\{\begin{matrix}
                \sigma^2,& \textnormal{ if }j=0\\
                0,& \textnormal{ otherwise}
            \end{matrix}\right.
        \end{aligned}
        \nonumber
    \end{equation}
    We use \textit{notation} $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$.
\end{definition}
\begin{note}
    \begin{enumerate}
        \item If $\epsilon_t\sim \textnormal{i.i.d.}(0,\sigma^2)$, then $\{\epsilon_t:t\in \mathbb{Z}\}$ is white noise, i.e., $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$.
        \item Gauss-Markov theorem assumes WN errors.
        \item WN terms are used as ``building blocks'': often a variable can be generated as
        \begin{equation}
            \begin{aligned}
                Y_t=h(\epsilon_t,\epsilon_{t-1},...) \textnormal{ for some function $h(\cdot)$ and some $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$.}
            \end{aligned}
            \nonumber
        \end{equation}
    \end{enumerate}
\end{note}

\subsection{Moving-Average Process}
\begin{definition}[MA(1)]
    First-order moving average process: $Y_t\sim \textnormal{MA}(1)$ iff
    \begin{equation}
        \begin{aligned}
            Y_t=\mu+\epsilon_t+\theta\epsilon_{t-1}
        \end{aligned}
        \nonumber
    \end{equation}
    where $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$.
\end{definition}
\begin{claim}[ACF of MA(1)]
    $\{Y_t\}$ is covariance stationary: $\mathbb{E}[Y_t]=\mu$ and its autocovariance function is
    \begin{equation}
        \begin{aligned}
            \gamma(j):=\textnormal{Cov}(Y_t,Y_{t-j})=\left\{\begin{matrix}
                (1+\theta^2)\sigma^2,&j=0\\
                \theta\sigma^2,&j=1\\
                0,&j\geq 2
            \end{matrix}\right.
        \end{aligned}
        \nonumber
    \end{equation}
\end{claim}
\begin{definition}[MA(p)]
    $Y_t\sim \textnormal{MA}(q)$ (for some $q\in \mathbb{N}$) iff
    \begin{equation}
        \begin{aligned}
            Y_t=\mu+\epsilon_t+\sum_{i=1}^{q}\theta_i\epsilon_{t-i}
        \end{aligned}
        \nonumber
    \end{equation}
    where $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$.
\end{definition}
\begin{claim}[ACF of MA(p)]
    $\{Y_t\}$ is covariance stationary: $\mathbb{E}[Y_t]=\mu$ and its autocovariance function is
    \begin{equation}
        \begin{aligned}
            \gamma(j):=\textnormal{Cov}(Y_t,Y_{t-j})=\left\{\begin{matrix}
                \left(\sum_{i=0}^{q-j}\theta_i\theta_{i+j}\right)\sigma^2,&j\leq q\\
                0,&j\geq q+1
            \end{matrix}\right.
        \end{aligned}
        \nonumber
    \end{equation}
    where $\theta_0=1$.
\end{claim}

\begin{definition}[Infinite Moving-Average Process]
    $Y_t\sim \textnormal{MA}(\infty)$ iff
        \begin{equation}
            \begin{aligned}
                Y_t=\mu+\sum_{i=0}^{\infty}\psi_i\epsilon_{t-i}
            \end{aligned}
            \nonumber
        \end{equation}
        where $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$ and \underline{$\sum_{i=0}^{\infty}\psi_i^2<\infty$}
\end{definition}

\subsection{Conditions for Infinite Moving-Average Process}
\begin{note}
    Conjecture:
    \begin{enumerate}
        \item $\{Y_t\}$ is covariance stationary;
        \item $\mathbb{E}[Y_t]=\mu$ and
        \item its autocovariance function is $$\gamma(j):=\textnormal{Cov}(Y_t,Y_{t-j})=(\sum_{i=0}^{\infty}\psi_i\psi_{i+j})\sigma^2,\forall j\geq 0.$$
    \end{enumerate}
\end{note}
The necessary condition to make these conjectures correct is
\begin{equation}
    \begin{aligned}
        \mathbb{E}[Y_t^2]&=(\mathbb{E}[Y_t])^2+\Gamma(0)\\
        &=\mu^2+(\sum_{i=0}^{\infty}\psi_i^2)\sigma^2<\infty\\
        \Leftrightarrow \sum_{i=0}^{\infty}\psi_i^2&<\infty
    \end{aligned}
    \nonumber
\end{equation}
\begin{claim}
    With the `right' definition of ``$\sum_{i=0}^{\infty}$'', the conjecture is correct.
\end{claim}
\begin{remark}
    \begin{enumerate}
        \item If $X_0,X_1,...$ are i.i.d. with $X_0=0$, then $\sum_{i=0}^\infty X_i$ denote $\lim_{n \rightarrow \infty}\sum_{i=0}^n X_i$ (assuming the limit exists).
        \item $\exists$ various models of stochastic convergence.
        \item There: convergence in mean square.
    \end{enumerate}
\end{remark}
\begin{definition}[Stochastic Convergence in Mean Square]
    If $X_0,X_1,...$ are random (with $\mathbb{E}[X_i^2]<\infty,\forall i$), then $\sum_{i=0}^\infty X_i$ denotes any $S$ such that $\lim_{n \rightarrow \infty}\mathbb{E}[(S-\sum_{i=0}^n X_i)^2]=0$.
\end{definition}
\begin{lemma}
    The properties of the $S$ are
    \begin{enumerate}
        \item $S$ is ``essentially unique.''
        \item $\mathbb{E}[S]=\sum_{i=0}^\infty \mathbb{E}[X_i]=\lim_{n \rightarrow \infty}\sum_{i=0}^n \mathbb{E}[X_i]$
        \item $\textnormal{Var}[S]=...=\lim_{n \rightarrow \infty}\textnormal{Var}[\sum_{i=0}^n X_i]$
        \item (Higher order moments of $S$ are similar) $\cdots$
    \end{enumerate}
\end{lemma}

\begin{theorem}[Cauchy Criterion]
    $\sum_{i=0}^\infty X_i$ exists iff
    \begin{equation}
        \begin{aligned}
            \lim_{n \rightarrow \infty}\sup_{m>n}\mathbb{E}[(S_m-S_n)^2]=0,
        \end{aligned}
        \nonumber
    \end{equation}
    where $S_n=\sum_{i=0}^n X_i$.
\end{theorem}
In the \underline{$MA(\infty)$ context}: The condition that can make
\begin{equation}
    \begin{aligned}
        \lim_{n \rightarrow \infty}\sup_{m>n}\mathbb{E}[(Y_{t,m}-Y_{t,n})^2]=0
    \end{aligned}
    \nonumber
\end{equation}
where $Y_{t,n}=\mu+\sum_{i=0}^n\psi_i\epsilon_{t-i}$.\\
This condition is given as: If $m>n$,
\begin{equation}
    \begin{aligned}
        &Y_{t,m}-Y_{t,n}=\sum_{i=n+1}^m\psi_i\epsilon_{t-i}\\
        \Rightarrow & \mathbb{E}[(Y_{t,m}-Y_{t,n})^2]=\mathbb{E}\left[\left(\sum_{i=n+1}^m\psi_i\epsilon_{t-i}\right)^2\right]=\left(\sum_{i=n+1}^m\psi_i^2\right)\sigma^2\\
        \Rightarrow & \sup_{m>n}\mathbb{E}[(Y_{t,m}-Y_{t,n})^2]=\left(\sum_{i=n+1}^\infty\psi_i^2\right)\sigma^2\\
        \Rightarrow & \lim_{n \rightarrow \infty}\sup_{m>n}\mathbb{E}[(Y_{t,m}-Y_{t,n})^2]=\left(\lim_{n \rightarrow \infty}\sum_{i=n+1}^\infty\psi_i^2\right)\sigma^2
    \end{aligned}
    \nonumber
\end{equation}
Thus,
\begin{equation}
    \begin{aligned}
        \lim_{n \rightarrow \infty}\sup_{m>n}\mathbb{E}[(Y_{t,m}-Y_{t,n})^2]=0 &\textnormal{ iff }\lim_{n \rightarrow \infty}\sum_{i=n+1}^\infty\psi_i^2=0\\
        &\textnormal{ iff }\sum_{i=0}^\infty\psi_i^2<\infty
    \end{aligned}
    \nonumber
\end{equation}


\subsection{Remarks about $MA(\infty)$ models}
\begin{enumerate}
    \item $MA(\infty)$ models are useful in theoretical work.
    \item The $MA(\infty)$ class is ``large'': Wold decomposition (theorem).
    \item Parametric $MA(\infty)$ models are useful in inference.
\end{enumerate}

\section{Autoregressive (AR) Model}
\subsection{Autoregressive Model as a Special Case of $MA(\infty)$}
Autoregressive model is an example of well-defined $MA(\infty)$ model.
\begin{example}[ (Autoregressive model)]
    Suppose
    \begin{equation}
        \begin{aligned}
            Y_{t}=\sum_{i=0}^\infty \psi_i\epsilon_{t-i},\ \forall t
        \end{aligned}
        \nonumber
    \end{equation}
    where
    \begin{enumerate}[$\circ$]
        \item $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$;
        \item $\psi_i=\phi^i$ ($\forall i\geq 0$) for some $|\phi|<1$.
    \end{enumerate}
\end{example}
Checking the condition: $\lim_{n \rightarrow \infty}\sum_{i=0}^n\psi_i^2=\lim_{n \rightarrow \infty}\sum_{i=0}^n\phi^{2i}=\lim_{n \rightarrow \infty}\frac{1-\phi^{2(n+1)}}{1-\phi^2}=\frac{1}{1-\phi^2}<\infty$.

\begin{lemma}[Property of ACF of Autoregressive Model]
    For $j\geq 0$, the autocovariance function is
    \begin{equation}
        \begin{aligned}
            \gamma(j)=\phi^j \gamma(0)
        \end{aligned}
        \nonumber
    \end{equation}
    \begin{note}
        \begin{enumerate}
            \item $\gamma(j)\neq 0, \forall j$ if $\phi\neq 0$.
            \item $\gamma(j)\propto \phi^j$ decays exponentially.
        \end{enumerate}
    \end{note}
\end{lemma}
\begin{proof}
    $\gamma(j):=\textnormal{Cov}(Y_t,Y_{t-j})=(\sum_{i=0}^{\infty}\psi_i\psi_{i+j})\sigma^2=\phi^{j}(\sum_{i=0}^{\infty}\phi^{2i})\sigma^2=\phi^j\frac{\sigma^2}{1-\phi^2}=\phi^j \gamma(0)$
\end{proof}

\subsection{Alternative Representation of AR Model}
\begin{definition}[Alternative Representation of AR]
    Alternatively, the AR model can be represented as
    \begin{equation}
        \begin{aligned}
            Y_t=\phi Y_{t-1}+\epsilon_t,\ \forall t
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}
\begin{proof}
    \begin{equation}
        \begin{aligned}
            Y_{t}=\sum_{i=0}^\infty \psi_i\epsilon_{t-i}=\epsilon_t+\sum_{i=1}^\infty \psi_i\epsilon_{t-i}=\epsilon_t+\sum_{i=0}^\infty \psi_{i+1}\epsilon_{t-i-1}=\epsilon_t+\phi\sum_{i=0}^\infty \psi_{i}\epsilon_{t-i-1}=\epsilon_t+\phi Y_{t-1}
        \end{aligned}
        \nonumber
    \end{equation}
\end{proof}

The natural estimator of $\phi$ (OLS) is
\begin{equation}
    \begin{aligned}
        \hat{\phi}=\frac{\sum_{t=2}^TY_{t-1}Y_t}{\sum_{t=2}^T Y_{t-1}^2}
    \end{aligned}
    \nonumber
\end{equation}



\begin{definition}[Model for Finite AR]
    \begin{equation}
        \begin{aligned}
            Y_t=\phi Y_{t-1}+\epsilon_t,\ 2\leq t\leq T
        \end{aligned}
        \nonumber
    \end{equation}
    where
    \begin{enumerate}[$\circ$]
        \item $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$;
        \item $|\phi|<1$;
        \item $Y_1=\sum_{i=0}^\infty \phi^i\epsilon_{1-i}$
    \end{enumerate}
\end{definition}


More generally, consider an AR with a drift,
\begin{equation}
    \begin{aligned}
        Y_t=c+\phi Y_{t-1}+\epsilon_t,\ \forall t
    \end{aligned}
    \nonumber
\end{equation}
where $c=\mu(1-\phi)$.

\subsection{AR(1)}
\begin{definition}[$AR(1)$]
    $\{Y_t:1\leq t\leq T\}$ is an \textbf{autoregreessive process} of order $1$, $Y_t\sim AR(1)$, if
    \begin{equation}
        \begin{aligned}
            Y_t=c+\phi Y_{t-1}+\epsilon_t,\ 2\leq t\leq T
        \end{aligned}
        \nonumber
    \end{equation}
    where $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$.
    \begin{note}
        $|\phi|<1$ is not assumed (yet) and $Y_1=\mu+\sum_{i=0}^\infty \phi^i\epsilon_{1-i}$ is not assumed.
    \end{note}
    We call the AR(1) model is \textbf{stable} iff $|\phi|<1$.
\end{definition}
\begin{enumerate}[$\circ$]
    \item If $|\phi|<1$ and $Y_1=\mu+\sum_{i=0}^\infty \phi^i\epsilon_{1-i}$, $$Y_t=\mu+\sum_{i=0}^\infty \phi^i\epsilon_{t-i},$$ where $\mu=\frac{c}{1-\phi}$.
    \item OLS ``works'' when $|\phi|<1$.
    \item The $AR(1)$ model admits and $MA(\infty)$ solution
    \begin{equation}
        \begin{aligned}
            Y_t=\mu+\sum_{i=0}^\infty \psi_i\epsilon_{t-i},\quad \sum_{i=0}^\infty \psi_i^2<\infty
        \end{aligned}
        \nonumber
    \end{equation}
    \underline{iff} $|\phi|<1$.
    \item The AR(1) model admits a covariance stationary solution \underline{iff} $|\phi|\neq 1$.
    \begin{note}
        Consider the case that $\phi>1$, the intuition is
        \begin{equation}
            \begin{aligned}
                Y_t=\phi Y_{t-1}+\epsilon_t \Leftrightarrow Y_{t-1}=\phi^{-1}(Y_t-\epsilon_t)
            \end{aligned}
            \nonumber
        \end{equation}
    \end{note}
\end{enumerate}

\subsection{AR(p)}
\begin{definition}[AR(p)]
    $\{Y_t:t\in \mathbb{N}\}$ is a \textbf{$p^{th}$-order autoregressive process}, $Y_t\sim AR(p)$, iff
    \begin{equation}
        \begin{aligned}
            Y_t=c+\phi_1 Y_{t-1}+\phi_2 Y_{t-2}+\cdots+\phi_p Y_{t-p}+\epsilon_t,\ t\geq p+1
        \end{aligned}
        \nonumber
    \end{equation}
    where $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$.
\end{definition}
In vector notation, we can write
\begin{equation}
    \begin{aligned}
        Y_t=\beta'X_t+\epsilon_t,\ t\geq p+1
    \end{aligned}
    \nonumber
\end{equation}
where $\beta=(c,\phi_1,\phi_2,\cdots,\phi_p)'$ and $X_t=(1,Y_{t-1},Y_{t-2},\cdots,Y_{t-p})'$.
\begin{claim}
    OLS ``works'' when the $AR(p)$ model is \underline{stable}. Then the \textit{OLS estimator} is given by
    \begin{equation}
        \begin{aligned}
            \hat{\beta}=(\sum_{t=p+1}^TX_t'X_t)^{-1}(\sum_{t=p+1}^TX_t'Y_t)
        \end{aligned}
        \nonumber
    \end{equation}
\end{claim}


\paragraph*{Lag Operator Notation} There is an alternative way to write the $AR(p)$ model.
\begin{definition}[Lag Operator]
    The \textbf{lag operator} ($L$) operates on an element of a time series to produce the previous element. That is, For a time series $\{X_t\}$,
    \begin{equation}
        \begin{aligned}
            L X_t&=X_{t-1}\\
            &\vdots\\
            L^k X_t&=X_{t-k},\ \forall t\in \mathbb{Z}
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}

Then, in this notation, the $AR(p)$ model can be written as
\begin{equation}
    \begin{aligned}
        \phi(L)Y_t=c+q_t,\ t\geq p+1
    \end{aligned}
    \nonumber
\end{equation}
where $\phi(L)=1-\phi_1 L - \phi_2 L^2 - \cdots - \phi_p L^p$.

\begin{definition}[Stability of $AR(p)$]
    The $AR(p)$ model is \textbf{stable} if \begin{equation}
        \begin{aligned}
            \phi(z)=1-\phi_1 z - \phi_2 z^2 - \cdots - \phi_p z^p=0 \Rightarrow |z|>1
        \end{aligned}
        \nonumber
    \end{equation}
    (All solutions are greater than 1).
\end{definition}

\begin{enumerate}[$\circ$]
    \item The $AR(p)$ model admits an $MA(\infty)$ solution
    \begin{equation}
        \begin{aligned}
            Y_t=\mu+\sum_{i=0}^\infty \psi_i\epsilon_{t-i},\quad \sum_{i=0}^\infty \psi_i^2<\infty
        \end{aligned}
        \nonumber
    \end{equation}
    \underline{iff} it is \textit{stable}.
    The $MA(\infty)$ solution has
    \begin{equation}
        \begin{aligned}
            \mu=\frac{c}{1-\phi_1-\cdots-\phi_p}=\frac{c}{\phi(1)}
        \end{aligned}
        \nonumber
    \end{equation}
    and (computable) $\psi_i$'s satisfy
    \begin{equation}
        \begin{aligned}
            |\psi_i|\leq M \lambda^i,\ \forall i,
        \end{aligned}
        \nonumber
    \end{equation}
    where $M<\infty$ and $|\lambda|<1$.
\end{enumerate}

\section{More On $MA(q)$}
\subsection{Lag Operator Notation and Invertible $MA(q)$}
\paragraph*{$MA(q)$ model in lag operator notation}:
\begin{equation}
    \begin{aligned}
        Y_t&=\mu+\underbrace{\epsilon_t+\sum_{i=1}^{q}\theta_i\epsilon_{t-i}}_{:=\theta(L)\epsilon_t}\\
        &=\mu + \theta(L)\epsilon_t,
    \end{aligned}
    \nonumber
\end{equation}
where $\theta(L)=1+\theta_1 L + \theta_2 L^2 + \cdots + \theta_q L^q$.
\begin{definition}[Invertibility of $MA(q)$]
    The $MA(q)$ model is \textbf{invertible} if \begin{equation}
        \begin{aligned}
            \theta(z)=1+\theta_1 z + \theta_2 z^2 + \cdots + \theta_q z^q=0 \Rightarrow |z|>1
        \end{aligned}
        \nonumber
    \end{equation}
    (All solutions are greater than 1).
\end{definition}
\begin{note}
    If the $MA(q)$ model is invertible, then
    \begin{equation}
        \begin{aligned}
            \epsilon_t=\Pi(L)(Y_t-\mu),
        \end{aligned}
        \nonumber
    \end{equation}
    where $\Pi(L)=\sum_{i=0}^\infty \pi_i L^i$ with $\sum_{i=0}^\infty |\pi_i|<\infty$.
\end{note}

\paragraph*{Technicalities}
\begin{enumerate}[$\circ$]
    \item If $\sum_{i=0}^\infty |\pi_i|<\infty$, then $\sum_{i=0}^\infty\pi_i^2<\infty$.
    \item If
    \begin{equation}
        \begin{aligned}
            |\pi_i|\leq M \lambda^i,\ \forall i \textnormal{ (some $M<\infty$ and $|\lambda|<1$)},
        \end{aligned}
        \label{(*)}
        \tag{*}
    \end{equation}
    then
    \begin{equation}
        \begin{aligned}
            \sum_{i=0}^\infty i^r|\pi_i|^s<\infty,\ \forall r\geq 0,s>0
        \end{aligned}
        \nonumber
    \end{equation}
    \item Invertibility $\Rightarrow$ \eqref{(*)}.
    \item If $X_0,X_1,...$ are random variables with $\sup_i \mathbb{E}X_i^2<\infty$, then $\sum_{i=0}^\infty \pi_i X_i$ exists (as a limit in mean squared) if $\sum_{i=0}^\infty |\pi_i|<\infty$.
\end{enumerate}


\subsection{$MA(q)$ is the only covariance stationary process with $\gamma(j)=0,\forall j>q$}
\begin{proposition}[$MA(q)$ $\Leftrightarrow$ covariance stationary and $\gamma(j)=0,\forall j>q$]
    If $\{Y_t\}$ is covariance stationary, then $\gamma(j)=0,\forall j>q$ iff $Y_t\sim MA(q)$.
\end{proposition}
\textbf{Question}: Is there a ``$q=\infty$'' analog?
\begin{example}
    Suppose $Y_t=Z\sim \mathcal{N}(0,1), \forall t$. Then, $\textnormal{Cov}(Y_t,Y_{t-1})=1,\forall j$.
    \begin{enumerate}
        \item $Y_t$ is covariance stationary.
        \item It is not a $MA(\infty)$.
        \item $Y_t$ can be predicted without error using $\{Y_s:s\leq t-1\}$.
        \item $Y_t$ is ``deterministic''.
    \end{enumerate}
\end{example}

\subsection{Deterministic covariance stationary process}
\begin{definition}[Deterministic]
    A mean zero covariance stationary process $\{v_t\}$ is \textbf{deterministic} iff $\exists p$ and $\{\phi_i:1\leq i\leq p\}$ such that
    \begin{equation}
        \begin{aligned}
            \mathbb{E}[(v_t-\phi v_{t-1}-\cdots -\phi_p v_{t-p})^2]\leq \epsilon^2,\ \forall t
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}
\begin{claim}
    If $v_t$ is deterministic, then $v_t$ is not a $MA(\infty)$.
\end{claim}

\section{Spectral Representation}
\begin{definition}[Wold Decomposition]
    If $\{Y_t\}$ is a mean zero covariance stationary process, then
    \begin{equation}
        \begin{aligned}
            Y_t=\sum_{i=0}^\infty \psi_i \epsilon_{t-i} + v_t, \forall t,
        \end{aligned}
        \nonumber
    \end{equation}
    where
    \begin{enumerate}
        \item $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$
        \item $\psi_0=1$ and $\sum_{i=0}^\infty \psi_i^2<\infty$
        \item $\mathbb{E}[\epsilon_t v_s]=0, \forall t,s$
        \item $\{v_t\}$ is deterministic
    \end{enumerate}
\end{definition}


\textit{Question}:
When is a function $\gamma(\cdot)$ the autocovariance function (ACF) of a covariance stationary process?

Recall that, if $\gamma(\cdot)$ is an ACF, it is given by
\begin{equation}
    \begin{aligned}
        \gamma(j)=\mathbb{E}[(Y_t-\mu)(Y_{t-j}-\mu)]
    \end{aligned}
    \nonumber
\end{equation}
and satisfies the following properties by Lemma \ref{lemma_ACF property}.
\begin{enumerate}
    \item \underline{Even}: $\gamma(j)=\gamma(-j),\forall j\in \mathbb{N}$.
    \item \underline{Positive semi-definite (PSD)} i.e., for any  $n\in \mathbb{N}$ and any $a_1,...,a_n$,
    \begin{equation}
        \begin{aligned}
            \sum_{i=1}^n\sum_{j=1}^na_ia_j\gamma(i-j)=\textnormal{Var}(\sum_{i=1}^na_iY_i)\geq 0
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}
\subsection{ACF $\Leftrightarrow$ Even and PSD}
\begin{proposition}[ACF $\Leftrightarrow$ Even and PSD]
    A function $\gamma(\cdot)$ is an ACF iff it is even and positive semi-definite.
\end{proposition}

\begin{theorem}[Herglotz's Theorem]
    A function $\gamma: \mathbb{Z}\rightarrow \mathbb{R}$ is \textit{even} and \textit{positive semi-definite} iff
    \begin{equation}
        \begin{aligned}
            \gamma(j)=\int_{-\pi}^\pi \exp\left(ij\lambda\right)d F(\lambda)
        \end{aligned}
        \nonumber
    \end{equation}
    for some $F:[-\pi, \pi] \rightarrow \mathbb{R}_+$ that is bounded, non-decreasing, and right-continuous (and has $F(-\pi)=0$).
\end{theorem}

\begin{remark}
    \begin{enumerate}
        \item $F(\cdot)$ is called the \underline{spectral distribution function} (of $\gamma(\cdot)$).
        \item If $\exists f:[-\pi, \pi] \rightarrow \mathbb{R}$ such that $$F(\lambda)=\int_{-\pi}^\lambda f(r)d r, \forall \lambda\in[-\pi, \pi],$$
        then $f(\cdot)$ is called a \underline{spectral density function} (of $\gamma(\cdot)$) and
        \begin{equation}
            \begin{aligned}
                \gamma(j)=\int_{-\pi}^\pi \exp\left(ij\lambda\right)f(\lambda)d \lambda = \int_{-\pi}^\pi \cos(j\lambda) f(\lambda) d \lambda
            \end{aligned}
            \nonumber
        \end{equation}
    \end{enumerate}
\end{remark}


\paragraph*{Symmetry} Suppose $\gamma(j)=\int_{-\pi}^\pi \exp\left(ij\lambda\right)d F(\lambda),j\in \mathbb{Z}$, where
\begin{equation}
    \begin{aligned}
        \int_{-\pi}^\pi \exp\left(ij\lambda\right)d F(\lambda)&=\int_{-\pi}^\pi \left(\cos(j\lambda)+i\sin(j\lambda)\right)d F(\lambda)\\
        &=\int_{-\pi}^\pi \cos(j\lambda)d F(\lambda) + i\int_{-\pi}^\pi\sin(j\lambda)d F(\lambda)\\
    \end{aligned}
    \nonumber
\end{equation}
Given $\gamma(j)\in \mathbb{R},\forall j$, we must have $\int_{-\pi}^\pi\sin(j\lambda)d F(\lambda)=0$. Therefore,
\begin{equation}
    \begin{aligned}
        \gamma(j)=\int_{-\pi}^\pi \cos(j\lambda)d F(\lambda),
    \end{aligned}
    \nonumber
\end{equation}
which is even by the property of $\cos(\cdot)$.

Then, $\frac{F(\cdot)}{F(\pi)}$ is the CDF of a symmetric distribution on $[-\pi, \pi]$.

\begin{example}
    Suppose $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$. Then,
    \begin{equation}
        \begin{aligned}
            \gamma(j)&=\left\{\begin{matrix}
                1,&\textnormal{ if }j=0\\
                0,&\textnormal{ otherwise}
            \end{matrix}\right.\\
            &=\int_{-\pi}^\pi \cos(j\lambda)f(\lambda) d \lambda\\
            &\Rightarrow f(\lambda)=\frac{1}{2\pi}
        \end{aligned}
        \nonumber
    \end{equation}
\end{example}

\begin{example}
    Suppose $Y_t=Z\sim \mathcal{N}(0,1)$ for all $t$. Then,
    \begin{equation}
        \begin{aligned}
            \gamma(j)&=1\\
            &=\int_{-\pi}^\pi \cos(j\lambda) d F(\lambda)\\
            & \Rightarrow F(\lambda)=
            \left\{\begin{matrix}
                1,&\lambda\geq 0\\
                0,&\lambda<0
            \end{matrix}\right.
        \end{aligned}
        \nonumber
    \end{equation}
\end{example}

\textit{Question}: When does an ACF $\gamma(\cdot)$ admits a spectral density function?

\textit{Partial Answer}: An even function $\gamma: \mathbb{Z}\rightarrow \mathbb{R}$ with ``$\sum_{j=-\infty}^\infty |\gamma(j)|<\infty$'' is psd iff
\begin{equation}
    \begin{aligned}
        f(\lambda)=\frac{1}{2\pi}\sum_{j=-\infty}^\infty \cos\left(j\lambda\right)\gamma(j)\geq 0,\ \forall \lambda\in[-\pi, \pi],
    \end{aligned}
    \label{eq:partial_answer}
\end{equation}
in which case $f(\cdot)$ is a spectral density function of $\gamma(\cdot)$.


\begin{remark}
    A covariance stationary process with an ACF $\gamma(\cdot)$ has \textbf{short memory} if ``$\sum_{j=-\infty}^\infty |\gamma(j)|<\infty$''.
\end{remark}

\begin{proposition}[Implication of Short Memory]
    Given the covariance stationary process has \textbf{short memory} ($\sum_{j=-\infty}^\infty |\gamma(j)|<\infty$), we have
    \begin{enumerate}
        \item $f(\cdot)$ exists (given as \eqref{eq:partial_answer}) and is bounded.
        \item $\gamma(0)=\int_{-\pi}^\pi f(\lambda) d \lambda$.
        \item $f(0)=\frac{1}{2\pi}\sum_{j=-\infty}^\infty \gamma(j)$.
    \end{enumerate}
\end{proposition}

\paragraph*{$MA(\infty)$ Case:}
Suppose
\begin{equation}
    \begin{aligned}
        Y_t=\mu+\sum_{i=0}^\infty \psi_i\epsilon_{t-i},\ \forall t,
    \end{aligned}
    \nonumber
\end{equation}
where
\begin{enumerate}[$\cdot$]
    \item $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$
    \item $\sum_{i=0}^\infty |\psi_i|<\infty$
\end{enumerate}
Then,
\begin{enumerate}[$\circ$]
    \item $\gamma(\cdot)$ has short memory
    \item $\gamma(\cdot)$ has spectral density function given by
    \begin{equation}
        \begin{aligned}
            f(\lambda)&=\frac{1}{2\pi}\sum_{j=-\infty}^\infty \cos\left(j\lambda\right)\gamma(j)\\
            &=\frac{\sigma^2}{2\pi}|\psi(e^{i\lambda})|^2
        \end{aligned}
        \nonumber
    \end{equation}
    where $\gamma(j)=(\sum_{i=0}^\infty \psi_i\psi_{i+j})\sigma^2$ and $\psi(z)=\sum_{i=0}^\infty \psi_iz^i$.
    \item $f(0)=\frac{\sigma^2}{2\pi}\psi(1)^2$
\end{enumerate}


\chapter{Estimation and Inference}

\section{OLS Estimation in $AR(1)$ Model}
Suppose
\begin{equation}
    \begin{aligned}
        Y_t=\phi Y_{t-1}+\epsilon_t,\ \forall t\geq 2,
    \end{aligned}
    \nonumber
\end{equation}
where $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$.

The \textbf{OLS Estimator of $\phi$} is
\begin{equation}
    \begin{aligned}
        \hat{\phi}_{OLS}=\frac{\sum_{t=2}^TY_{t-1}Y_t}{\sum_{t=2}^T Y_{t-1}^2}
    \end{aligned}
    \nonumber
\end{equation}

\begin{claim}[OLS Estimator is MLE]
    If $\epsilon_t\sim \textnormal{i.i.d.} \mathcal{N}(0,\sigma^2)$ and if $(\epsilon_2,\epsilon_3,...)\perp Y_1$, then $\hat{\phi}_{OLS}$ is the (conditional) MLE of $\phi$.
\end{claim}
The (conditional) MLE of $(\phi,\sigma^2)$ is
\begin{equation}
    \begin{aligned}
        (\hat{\phi}_{ML},\hat{\sigma}^2_{ML})=\argmax_{(\phi,\sigma^2)} f_{2:T}\left(Y_2,...Y_T\mid Y_1;\phi,\sigma^2\right),
    \end{aligned}
    \nonumber
\end{equation}
where $f_{2:T}(\cdot\mid Y_1; \phi,\sigma^2)$ is the (conditional) pdf of $(Y_2,...,Y_T)$ given $Y_1$.


\begin{definition}[Prediction-error Decomposition]
    The objective function (conditional likelihood function) can be written as
    \begin{equation}
        \begin{aligned}
            f_{2:T}\left(Y_2,...,Y_T\mid Y_1;\phi,\sigma^2\right)=\prod_{t=2}^Tf_t\left(Y_t\mid Y_1,...,Y_{t-1};\phi,\sigma^2\right),
        \end{aligned}
        \nonumber
    \end{equation}
    where $f_t\left(Y_t\mid Y_1,...,Y_{t-1};\phi,\sigma^2\right)$ is the conditional pdf of $Y_t$ given $Y_1,...,Y_{t-1}$.
\end{definition}
By the definition that $Y_t=\phi Y_{t-1}+\epsilon_t,\ \forall t\geq 2$ and $\epsilon_t\mid Y_{1},...,Y_{t-1}\sim \mathcal{N}(0,\sigma^2)$, we have
\begin{equation}
    \begin{aligned}
        &Y_t\mid Y_1,...,Y_{t-1}\sim \mathcal{N}(\phi Y_{t-1},\sigma^2)\\
        \Rightarrow& f_t\left(Y_t\mid Y_1,...,Y_{t-1};\phi,\sigma^2\right)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}\left(Y_t-\phi Y_{t-1}\right)^2\right)
    \end{aligned}
    \nonumber
\end{equation}
\begin{equation}
    \begin{aligned}
        \Rightarrow f_{2:T}\left(Y_2,...,Y_T\mid Y_1;\phi,\sigma^2\right)%&=\prod_{t=2}^T\left[\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}\left(Y_t-\phi Y_{t-1}\right)^2\right)\right]\\
        &=\left(2\pi\sigma^2\right)^{-\frac{T-1}{2}}\exp\left(-\frac{1}{2\sigma^2}\sum_{t=2}^T\left(Y_t-\phi Y_{t-1}\right)^2\right)
    \end{aligned}
    \nonumber
\end{equation}
Therefore,
\begin{equation}
    \begin{aligned}
        \hat{\phi}_{ML}&=\argmin_{\phi} f_{2:T}\left(Y_2,...,Y_T\mid Y_1;\phi,\sigma^2\right)=\frac{\sum_{t=2}^TY_{t-1}Y_t}{\sum_{t=2}^T Y_{t-1}^2}=\hat{\phi}_{OLS}\\
        \hat{\sigma}^2_{ML}&=\argmin_{\sigma^2} f_{2:T}\left(Y_2,...,Y_T\mid Y_1;\phi,\sigma^2\right)=\frac{1}{T-1}\sum_{t=2}^T (Y_t-\hat{\phi}_{ML}Y_{t-1})^2
    \end{aligned}
    \nonumber
\end{equation}

\section{Properties of OLS Estimators (in time series)}
\subsection{OLS Review}
The OLS model can be written as
\begin{equation}
    \begin{aligned}
        y_i=\beta'x_i+\epsilon_i,\ i=1,...,n
    \end{aligned}
    \nonumber
\end{equation}
Iff $\sum_{i=1}^n x_ix'_i$ is positive definite ($\sum_{i=1}^n x_ix'_i\succ 0$), the OLS estimator (of $\beta$) is given by
\begin{equation}
    \begin{aligned}
        \hat{\beta}_{OLS}&=\argmin_{\beta} \left\{\sum_{i=1}^n (y_i-\beta'x_i)^2\right\}\\
        &=\left(\sum_{i=1}^n x_ix'_i\right)^{-1}\left(\sum_{i=1}^n x_iy_i\right)=\beta+\left(\sum_{i=1}^n x_ix'_i\right)^{-1}\left(\sum_{i=1}^n x_i\epsilon_i\right)
    \end{aligned}
    \nonumber
\end{equation}
\begin{lemma}[Unbiasedness]
    Suppose that
    \begin{enumerate}[(i).]
        \item $\textnormal{Pr}[\sum_{i=1}^n x_ix'_i\succ 0]=1$ and $\mathbb{E}[\hat{\beta}_{OLS}]$ exists.
        \item \underline{Strict exogeneity}: $\mathbb{E}[\epsilon_i\mid x_1,...,x_n]=0,\forall i$.
    \end{enumerate}
    Then, $\mathbb{E}[\hat{\beta}_{OLS}]=\beta$.
\end{lemma}
\begin{remark}
    \begin{enumerate}
        \item If $(x_i,\epsilon_i)\sim i.i.d.$, then the ``strictly exogeneity'' holds iff $\mathbb{E}[\epsilon_i\mid x_i]=0$.
        \item The first assumption (i.e., $\textnormal{Pr}[\sum_{i=1}^n x_ix'_i\succ 0]=1$ and $\mathbb{E}[\hat{\beta}_{OLS}]$ exists) is necessary and cannot be reduced in i.i.d. case, we need additional assumptions.
    \end{enumerate}
\end{remark}


\begin{lemma}[Consistency]
    Suppose that
    \begin{enumerate}[(i).]
        \item $\frac{1}{n}\sum_{i=1}^n x_ix_i' \stackrel{P}{\longrightarrow} Q$ for some $Q\succ 0$.
        \item $\frac{1}{n}\sum_{i=1}^n x_i\epsilon_i \stackrel{P}{\longrightarrow} 0$.
    \end{enumerate}
    Then, $\hat{\beta}_{OLS} \stackrel{P}{\longrightarrow} \beta$.
\end{lemma}
\begin{proof}
    With probability approaching one (as $n\to\infty$),
    \begin{equation}
        \begin{aligned}
            \hat{\beta}=\beta+\left(\underbrace{\sum_{i=1}^n x_ix'_i}_{\stackrel{P}{\longrightarrow} Q}\right)^{-1}\underbrace{\left(\sum_{i=1}^n x_i\epsilon_i\right)}_{\stackrel{P}{\longrightarrow} 0}\stackrel{P}{\longrightarrow} \beta + Q^{-1}\cdot 0=\beta
        \end{aligned}
        \nonumber
    \end{equation}
    by the continuity theorem (for $\stackrel{P}{\longrightarrow}$).
\end{proof}
\begin{remark}
    If $\begin{bmatrix}
        x_i\\
        \epsilon_i
    \end{bmatrix}\sim i.i.d. \left(\begin{bmatrix}
        \mu_x\\
        0
    \end{bmatrix},\begin{bmatrix}
        \Sigma_{xx}&	0\\
        0 & \sigma^2
    \end{bmatrix}\right)$, then
    \begin{equation}
        \begin{aligned}
            \frac{1}{n}\sum_{i=1}^n x_ix_i' \stackrel{P}{\longrightarrow} \mu_x\mu'_x+\Sigma_{xx}=\mathbb{E}[x_ix'_i]\\
            \frac{1}{n}\sum_{i=1}^n x_i\epsilon_i \stackrel{P}{\longrightarrow} 0=\mathbb{E}[x_i\epsilon_i]
        \end{aligned}
        \nonumber
    \end{equation}
    by LLN.
\end{remark}

\begin{lemma}[Asymptotic Normality]
    Suppose that
    \begin{enumerate}[(i).]
        \item $\frac{1}{n}\sum_{i=1}^n x_ix_i' \stackrel{P}{\longrightarrow} Q$ for some $Q\succ 0$.
        \item $\frac{1}{\sqrt{n}}\sum_{i=1}^n x_i\epsilon_i \stackrel{d}{\longrightarrow} \mathcal{N}(0,V)$ for some $V\succ 0$.
    \end{enumerate}
    Then, $\sqrt{n}\left(\hat{\beta}_{OLS}-\beta\right) \stackrel{d}{\longrightarrow} N\left(0,\Omega\right)$, where $\Omega:=Q^{-1} V Q^{-1}$
\end{lemma}
\begin{proof}
    With probability approaching one (as $n\to\infty$),
    \begin{equation}
        \begin{aligned}
            \sqrt{n}(\hat{\beta}-\beta)=\left(\underbrace{\sum_{i=1}^n x_ix'_i}_{\stackrel{P}{\longrightarrow} Q}\right)^{-1}\left(\underbrace{\sqrt{n}\left(\hat{\beta}_{OLS}-\beta\right)}_{\stackrel{d}{\longrightarrow} \mathcal{N}(0,V)}\right)\stackrel{d}{\longrightarrow} Q^{-1}\mathcal{N}(0,V)=\mathcal{N}(0,Q^{-1}VQ^{-1})
        \end{aligned}
        \nonumber
    \end{equation}
    by the continuous mapping theorem (CMT).
\end{proof}
\begin{remark}
    If $\begin{bmatrix}
        x_i\\
        x_i\epsilon_i
    \end{bmatrix}\sim i.i.d. \left(\begin{bmatrix}
        \mu_x\\
        0
    \end{bmatrix},\begin{bmatrix}
        \Sigma_{xx}&	C'\\
        C & V
    \end{bmatrix}\right)$, then
    \begin{equation}
        \begin{aligned}
            \frac{1}{n}\sum_{i=1}^n x_ix_i' \stackrel{P}{\longrightarrow} \mu_x\mu'_x+\Sigma_{xx}=\mathbb{E}[x_ix'_i]\\
            \frac{1}{n}\sum_{i=1}^n x_i\epsilon_i \stackrel{P}{\longrightarrow} 0=\mathbb{E}[x_i\epsilon_i]
        \end{aligned}
        \nonumber
    \end{equation}
    by LLN. Moreover,
    \begin{equation}
        \begin{aligned}
            \frac{1}{\sqrt{n}}\sum_{i=1}^n x_i\epsilon_i \stackrel{d}{\longrightarrow} \mathcal{N}(0,V)
        \end{aligned}
        \nonumber
    \end{equation}
    by CLT.
\end{remark}

\begin{proposition}[Variance Estimation]
    Suppose that
    \begin{enumerate}[(i).]
        \item $\hat{Q}=\frac{1}{n}\sum_{i=1}^n x_ix_i' \stackrel{P}{\longrightarrow} Q\succ 0$.
        \item $\hat{V} \stackrel{P}{\longrightarrow} V$.
    \end{enumerate}
    Then, $\hat{\Omega}:=\hat{Q}^{-1}\hat{V}\hat{Q}^{-1}\stackrel{P}{\longrightarrow} Q^{-1}VQ^{-1}:=\Omega$ (by the continuity theorem for $\stackrel{P}{\longrightarrow}$).
\end{proposition}
\begin{remark}
    To achieve these properties we need, except for $\begin{bmatrix}
        x_i\\
        x_i\epsilon_i
    \end{bmatrix}\sim i.i.d. \left(\begin{bmatrix}
        \mu_x\\
        0
    \end{bmatrix},\begin{bmatrix}
        \Sigma_{xx}&	C'\\
        C & V
    \end{bmatrix}\right)$, we need more conditions:
    \begin{enumerate}
        \item If also $\mathbb{E}[(x'_ix_i)^r]<\infty$ for some $r>1$, then
        \begin{equation}
            \begin{aligned}
                \hat{V}=\frac{1}{n}\sum_{i=1}^n x_ix'_i \hat{\epsilon}_i^2 \stackrel{P}{\longrightarrow} \mathbb{E}[x_ix'_i \epsilon_i^2]=V, \textnormal{ where } \hat{\epsilon}_i=y_i-\hat{\beta}'_{OLS}x_i
            \end{aligned}
            \nonumber
        \end{equation}
        \item If also $\mathbb{E}[\epsilon^2_i\mid x_i]=\sigma^2$ (aka ``homoskedasticity''), then
        \begin{equation}
            \begin{aligned}
                V=\mathbb{E}[x_ix'_i \hat{\epsilon}_i^2]=...\underbrace{=}_{LIE} \sigma^2 \mathbb{E}[x_ix'_i]=\sigma^2 Q
            \end{aligned}
            \nonumber
        \end{equation}
        and
        \begin{equation}
            \begin{aligned}
                \hat{V}=\hat{\sigma}^2\hat{Q}, \textnormal{ where }\hat{\sigma}^2=\frac{1}{n}\sum_{i=1}^n \left(y_i-\hat{\beta}'_{OLS}x_i\right)^2
            \end{aligned}
            \nonumber
        \end{equation}
    \end{enumerate}
\end{remark}


\subsection{OLS for $MA(\infty)$: $\bar{Y}=\frac{1}{T}\sum_{t=1}^T Y_t$}
Consider the $MA(\infty)$ model:
\begin{equation}
    \begin{aligned}
        Y_t=\mu+\sum_{i=0}^\infty \psi_i\epsilon_{t-i},\ t\geq 1
    \end{aligned}
    \nonumber
\end{equation}
where
\begin{enumerate}
    \item $\epsilon_t\sim i.i.d.(0,\sigma^2)$,
    \item $\sum_{i=0}^\infty i|\psi_i|<\infty$.
\end{enumerate}

\paragraph*{Mean Estimation}
Consider the estimator (for $\mu$):
\begin{equation}
    \begin{aligned}
        \bar{Y}=\frac{1}{T}\sum_{t=1}^T Y_t
    \end{aligned}
    \nonumber
\end{equation}

\begin{note}
    \begin{enumerate}
        \item $\bar{Y}=\argmin_m\sum_{t=1}^T (Y_t-m)^2$.
        \item $\epsilon_t\sim i.i.d.(0,\sigma^2) \Rightarrow $ $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$ (i.e., a stronger assumption than white noise).
        \item $\sum_{i=0}^\infty i|\psi_i|<\infty \Rightarrow \sum_{i=0}^\infty |\psi_i|<\infty \Rightarrow \sum_{i=0}^\infty \psi_i^2<\infty$ (also a stronger assumption)
    \end{enumerate}
\end{note}
The properties of $\bar{Y}$ can be checked in the following:
\begin{enumerate}
    \item \textbf{Unbiasedness}: Recall that $\mathbb{E}(Y_t)=\mu, \forall t$ because $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$ and $\sum_{i=0}^\infty \psi_i^2<\infty$. Then,
    \begin{equation}
        \begin{aligned}
            \mathbb{E}[\bar{Y}]=\mathbb{E}[\frac{1}{n}\sum_{i=1}^n Y_i]=\mu
        \end{aligned}
        \nonumber
    \end{equation}
    \item \textbf{Consistency}: $$\bar{Y}\stackrel{P}{\longrightarrow}\mu$$ which can be proven by $P(|\bar{Y}-\mu|>\eta)\stackrel{T \rightarrow \infty}{\longrightarrow} 0$ for all $\eta>0$. This can be given by \underline{Chebyshev's inequality}: $P(|\bar{Y}-\mu|>\eta)\leq \frac{\textnormal{Var}(\bar{Y})}{\eta^2}$ for all $\eta>0$.
    \begin{claim}[Bounded Variance]
        \begin{equation}
            \begin{aligned}
                \textnormal{Var}(\bar{Y})&=\textnormal{Cov}\left(\frac{1}{T}\sum_{t}Y_t, \frac{1}{T}\sum_{s}Y_s\right)\\
                &=\frac{1}{T^2}\sum_{t}\sum_{s}\textnormal{Cov}\left(Y_t,Y_s\right)\\
                &=\frac{1}{T^2}\sum_{t}\sum_{s}\gamma(t-s)\\
                &=\frac{1}{T^2}\sum_{j=1-T}^{T-1}(T-|j|)\gamma(j)\\
                &=\frac{1}{T}\sum_{j=1-T}^{T-1}(1-\frac{|j|}{T})\gamma(j)\\
                &\leq \frac{1}{T}\sum_{j=1-T}^{T-1} |\gamma(j)|\\
                &\leq\frac{1}{T}\sum_{j=-\infty}^\infty |\gamma(j)|
            \end{aligned}
            \nonumber
        \end{equation}
        where $\gamma(j):=\textnormal{Cov}(Y_t,Y_{t-j})$ is the autocovariance function.
    \end{claim}
    Recall that if $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$ and if $\sum_{i=0}^\infty|\psi_i|<\infty$, then $\sum_{i=0}^\infty |\gamma(i)|<\infty$ (aka ``short memory''). Therefore, we have $\bar{Y}\stackrel{P}{\longrightarrow}\mu$.
    \item \textbf{Asymptotic Consistency}: $$\sqrt{T}\left(\bar{Y}-\mu\right)\stackrel{d}{\longrightarrow} \mathcal{N}\left(0,\omega^2\right)$$ where $\omega^2\neq \textnormal{Var}\left(Y_t\right)$ (in general).\\
    \underline{Idea of proof}:
    \begin{equation}
        \begin{aligned}
            \sqrt{T}\left(\bar{Y}-\mu\right)=\underbrace{\psi(1)\frac{1}{\sqrt{T}}\sum_{t=1}^T\epsilon_t}_{\stackrel{d}{\longrightarrow} \psi(1)\mathcal{N}\left(0,\sigma^2\right)=\mathcal{N}\left(0,\omega^2\right)} + \underbrace{o_p(1)}_{\stackrel{p}{\longrightarrow} 0, \textnormal{ by definition}}
        \end{aligned}
        \nonumber
    \end{equation}
    where $\psi(1)=\sum_{i=0}^\infty \psi_i$ and $\omega^2=\psi(1)^2\sigma^2$. This is given by BN decomposition.
    \begin{theorem}[BN Decomposition]
        If $\psi(L)=\sum_{i=0}^\infty \psi_i L^i$ is a lag polynomial with $\sum_{i=0}^\infty i|\psi_i|<\infty$, then
        \begin{equation}
            \begin{aligned}
                \psi(L)=\psi(1)+\tilde{\psi}(L)(1-L)
            \end{aligned}
            \label{BN}
        \end{equation}
        where
        \begin{enumerate}[$\circ$]
            \item $\tilde{\psi}(L)=\sum_{i=0}^\infty \tilde{\psi}_i L^i$, $\tilde{\psi}_i=-\sum_{j=i+1}^\infty \psi_j$.
            \item $\sum_{i=0}^\infty |\tilde{\psi}_i|<\infty$.
        \end{enumerate}
    \end{theorem}
    \begin{proof}
        By the definition of $\tilde{\psi}(L)=\sum_{i=0}^\infty \tilde{\psi}_i L^i$, the RHS of \eqref{BN} can be written as
        \begin{equation}
            \begin{aligned}
                \psi(1)+\tilde{\psi}(L)(1-L)&=\psi(1)+\sum_{i=0}^\infty \tilde{\psi}_i L^i-\sum_{i=1}^\infty \tilde{\psi}_{i-1} L^i
            \end{aligned}
            \nonumber
        \end{equation}
        Let's check the coefficients of $L^i$:
        \begin{enumerate}
            \item $i=0$: $\psi(1)+\tilde{\psi}_0=\psi_0$
            \item $i\geq 1$: $\tilde{\psi}_i-\tilde{\psi}_{i-1}=\psi_{i}$
        \end{enumerate}
        The \eqref{BN} is proved. Moreover,
        \begin{equation}
            \begin{aligned}
                \sum_{i=0}^\infty |\tilde{\psi}_i|\leq \sum_{i=0}^\infty\sum_{j=i+1}^\infty|\psi_j|=\sum_{i=0}^\infty i|\psi_i|<\infty
            \end{aligned}
            \nonumber
        \end{equation}
    \end{proof}
    Given the BN decomposition, we have
    \begin{equation}
        \begin{aligned}
            \psi(L)&=\psi(1)+\tilde{\psi}(L)(1-L)\\
            \psi(L)\epsilon_t&=\psi(1)\epsilon_t+\tilde{\psi}(L)(\epsilon_t-\epsilon_{t-1})\\
            \sum_{t=1}^T\psi(L)\epsilon_t&=\psi(1)\sum_{t=1}^T\epsilon_t+\tilde{\psi}(L)(\epsilon_T-\epsilon_0)
        \end{aligned}
        \nonumber
    \end{equation}
    Thus,
    \begin{equation}
        \begin{aligned}
            \sqrt{T}\left(\bar{Y}-\mu\right)=\frac{1}{\sqrt{T}}\sum_{t=1}^T\psi(L)\epsilon_t=\psi(1)\frac{1}{\sqrt{T}}\sum_{t=1}^T\epsilon_t+\frac{1}{\sqrt{T}}\tilde{\psi}(L)(\epsilon_T-\epsilon_0)
        \end{aligned}
        \nonumber
    \end{equation}
    where $\frac{1}{\sqrt{T}}\tilde{\psi}(L)(\epsilon_T-\epsilon_0) \stackrel{p}{\longrightarrow} 0$ is proved by
    \begin{equation}
        \begin{aligned}
            \mathbb{E}\left[\frac{1}{\sqrt{T}}\tilde{\psi}(L)(\epsilon_T-\epsilon_0)\right]&=0\\
            \textnormal{Var}\left[\frac{1}{\sqrt{T}}\tilde{\psi}(L)(\epsilon_T-\epsilon_0)\right]&=\frac{1}{T}\textnormal{Var}\left[\tilde{\psi}(L)\epsilon_T-\tilde{\psi}(L)\epsilon_0\right]\\
            &\leq \frac{2}{T}\left[\textnormal{Var}\left(\tilde{\psi}(L)\epsilon_T\right)+\textnormal{Var}\left(\tilde{\psi}(L)\epsilon_0\right)\right]\\
            &=\frac{4}{T}\textnormal{Var}\left(\tilde{\psi}(L)\epsilon_T\right)=\frac{4\sigma^2}{T}\underbrace{\sum_{i=0}^\infty \tilde{\psi}_i^2}_{<\infty} \rightarrow 0
        \end{aligned}
        \nonumber
    \end{equation}
    \begin{remark}
        \begin{enumerate}
            \item If $\sum_{i=0}^\infty i|\psi_i|<\infty$, then $\sum_{i=0}^\infty |\psi_i|<\infty$ and $\sum_{i=0}^\infty |\tilde{\psi}_i|<\infty$. Note: we only need $\sum_{i=0}^\infty \tilde{\psi}_i^2<\infty$, so we can only require $\sum_{i=0}^\infty \sqrt{i}|\psi_i|<\infty$.
            \item If $\epsilon_t\sim{i.i.d.}\left(0,\sigma^2\right)$, then $\epsilon_t\sim \textnormal{WN}\left(0,\sigma^2\right)$ and $\frac{1}{\sqrt{T}}\sum_{t=1}^T\epsilon_t \stackrel{d}{\longrightarrow} \mathcal{N}\left(0,\sigma^2\right)$. (These two properties may hold even if $\epsilon_t\nsim{i.i.d.}\left(0,\sigma^2\right)$, i.e., there is a weaker condition can be used.)
            \item $\omega^2=\psi(1)^2\sigma^2\neq \left(\sum_{i=0}^\infty \psi_i^2\right)\sigma^2=\textnormal{Var}\left(Y_t\right)$ (in general.)
            \item $\omega^2$ is called the \textbf{``long-run variance''} of $Y_t$:
            \begin{equation}
                \begin{aligned}
                    \omega^2=\lim_{T \rightarrow \infty} T \textnormal{Var}\left(\bar{Y}\right)
                    =\lim_{T \rightarrow \infty} \frac{1}{T}\sum_{j=1-T}^{T-1}(1-\frac{|j|}{T})\gamma(j)
                    =\sum_{j=0}^\infty \gamma(j)
                \end{aligned}
                \nonumber
            \end{equation}
        \end{enumerate}
    \end{remark}
\end{enumerate}
\paragraph*{Variance Estimation}
The OLS (variance) estimator is
\begin{equation}
    \begin{aligned}
        S^2=\frac{1}{T-1}\sum_{t=1}^T\left(Y_t-\bar{Y}\right)^2
    \end{aligned}
    \nonumber
\end{equation}
\begin{claim}
    $S^2 \stackrel{p}{\longrightarrow} \textnormal{Var}(Y_t)$.
\end{claim}
Recall that
\begin{equation}
    \begin{aligned}
        \omega^2=\sigma^2\psi(1)^2=\sum_{j=-\infty}^\infty\gamma(j)=2\pi f(0),
    \end{aligned}
    \nonumber
\end{equation}
where $f(\cdot)$ is the spectral density function of $\gamma(\cdot)$:
\begin{equation}
    \begin{aligned}
        f(\lambda)&=\frac{1}{2\pi}\sum_{j=-\infty}^\infty \cos\left(j\lambda\right)\gamma(j)\\
        &=\frac{\sigma^2}{2\pi}|\psi(e^{i\lambda})|^2
    \end{aligned}
    \nonumber
\end{equation}
where $\gamma(j)=(\sum_{i=0}^\infty \psi_i\psi_{i+j})\sigma^2$ and $\psi(z)=\sum_{i=0}^\infty \psi_iz^i$.

The variance estimator can be given by
\begin{equation}
    \begin{aligned}
        \hat{\omega}^2=2\pi\hat{f}(0),
    \end{aligned}
    \nonumber
\end{equation}
where $\hat{f}$ is an estimator of $f$.
\begin{example}[ (Newey-West, 1987)]
    $\hat{\omega}^2=\hat{\gamma}(0)+2\sum_{j=1}^b\left(1-\frac{j}{b}\right)\hat{\gamma}(j)$, where $\hat{\gamma}(j)=\frac{1}{T}\sum_{t=1}^T(Y_t=\bar{Y})\left(Y_{t-j}-\bar{Y}\right)$ and $b$ is a ``turning'' parameter.
\end{example}
\begin{remark}
    If $\epsilon_t\sim{i.i.d.}(0,\sigma^2)$ and $\sum_{i=0}^\infty |\psi_i|<\infty$, then
    \begin{equation}
        \begin{aligned}
            \hat{\omega}^2 \stackrel{p}{\longrightarrow} \omega^2
        \end{aligned}
        \nonumber
    \end{equation}
    provided $b \rightarrow \infty$ and $\frac{b}{\sqrt{T}} \rightarrow 0$ as $T \rightarrow \infty$.
\end{remark}



\subsection{OLS of AR(1) Model}
Consider an AR(1) model
\begin{equation}
    \begin{aligned}
        Y_t=\phi Y_{t-1}+\epsilon_t,\ \forall t\geq 2,
    \end{aligned}
    \nonumber
\end{equation}
where
\begin{enumerate}
    \item $|\phi|<1$
    \item $\epsilon_t\sim{i.i.d.}(0,\sigma^2)$
    %\item $Y_1=\sum_{i=0}^\infty \phi^i\epsilon_{1-i}$ ($\perp (\epsilon_2,...,\epsilon_T)$)
\end{enumerate}

The \textbf{OLS Estimator of $\phi$} is
\begin{equation}
    \begin{aligned}
        \hat{\phi}=\frac{\sum_{t=2}^TY_{t-1}Y_t}{\sum_{t=2}^T Y_{t-1}^2}=\phi+\frac{\sum_{t=2}^TY_{t-1}\epsilon_t}{\sum_{t=2}^T Y_{t-1}^2}
    \end{aligned}
    \nonumber
\end{equation}

\subsubsection*{Unbiasedness}
Usual template (``strict exogeneity''): $\mathbb{E}[\epsilon_t\mid Y_1,...,Y_{T-1}]=0,\ t\geq 2$. However, it doesn't hold here: $\epsilon_t=Y_t-\phi Y_{t-1}, t\geq 2 \Rightarrow \mathbb{E}[\epsilon_t\mid Y_1,...,Y_{T-1}]=\epsilon_t\neq 0$ ($2\leq t\leq T-1$).
\begin{claim}
    The OLS estimator of $\phi$, $\hat{\phi}$, is biased (in general.)
\end{claim}

\subsubsection*{Consistency}
Usual template: If
\begin{enumerate}[(i).]
    \item $\frac{1}{T-1}\sum_{t=2}^T Y_{t-1}^2 \stackrel{p}{\longrightarrow} Q>0$,
    \item $\frac{1}{T-1}\sum_{t=2}^T Y_{t-1}\epsilon_t \stackrel{p}{\longrightarrow} 0$,
\end{enumerate}
then $\hat{\phi}\stackrel{p}{\longrightarrow} \phi$.
\begin{claim}
    These two conditions (i) and (ii) hold.
\end{claim}
Let $\tilde{Y}_t=\sum_{i=0}^\infty \phi^i\epsilon_{t-i}$, which equals to $Y_t$ iff $Y_1=\sum_{i=0}^\infty \phi^i\epsilon_{1-i}$. By assuming $Y_1=\sum_{i=0}^\infty \phi^i\epsilon_{1-i}$, we have
\begin{enumerate}
    \item $\sum_{t=2}^T Y_{t-1}^2=\sum_{t=2}^T \tilde{Y}_{t-1}^2+ O_P(1)$.
    \item $\sum_{t=2}^T Y_{t-1}\epsilon_t=\sum_{t=2}^T \tilde{Y}_{t-1}\epsilon_t+ O_P(1)$.
\end{enumerate}
(Proof by heuristics: $Y_{t-1}=\tilde{Y}_{t-1}+\phi^{t-2}(Y_1-\tilde{Y}_1)\approx \tilde{Y}_{t-1}$ when $t$ is large and $|\phi|<1$.)

Recall that if $\{X_t\}$ is non-random and bonded and if $r_t \rightarrow \infty$, $\frac{X_t}{r_t} \rightarrow 0$.
\begin{enumerate}
    \item If $X_t=O(1)$ and if $r_t \rightarrow \infty$, then $\frac{X_t}{r_t}=o(1)$ (``$\rightarrow 0$'').
    \item If $\{X_t\}$ is random with $X_t=O_P(1)$ and if $r_t \rightarrow \infty$, then $\frac{X_t}{r_t}=o_P(1)$ (``$\stackrel{P}{\longrightarrow} 0$'').
\end{enumerate}

\begin{definition}[Stochastically Bounded]
    A random sequence $\{X_t\}$ is \textbf{stochastically bounded}, $X_t=O_P(1)$, iff $\lim_{M \rightarrow \infty}\sup_{T\geq 1}P(|X_T|>M)=0$.
\end{definition}

Then, we can prove the consistency:
\begin{proof}
    \begin{equation}
        \begin{aligned}
            \frac{1}{T}\sum_{t=2}^T Y_{t-1}^2&=\frac{1}{T}\sum_{t=2}^T \tilde{Y}_{t-1}^2+\underbrace{\frac{O_P(1)}{T}}_{=o_P(1)}\\
            \frac{1}{T}\sum_{t=2}^T Y_{t-1}\epsilon_t&=\frac{1}{T}\sum_{t=2}^T \tilde{Y}_{t-1}\epsilon_t+\underbrace{\frac{O_P(1)}{T}}_{=o_P(1)}\\
            \frac{1}{\sqrt{T}}\sum_{t=2}^T Y_{t-1}\epsilon_t&=\frac{1}{\sqrt{T}}\sum_{t=2}^T \tilde{Y}_{t-1}\epsilon_t+\underbrace{\frac{O_P(1)}{\sqrt{T}}}_{=o_P(1)}
        \end{aligned}
        \nonumber
    \end{equation}
    If $\mathbb{E}[\epsilon_t^4]<\infty$, we have
    \begin{equation}
        \begin{aligned}
            \textnormal{Var}(\frac{1}{T}\sum_{t=2}^\infty\tilde{Y}_{t-1}^2) \rightarrow 0\ \& \ \textnormal{Var}(\frac{1}{T}\sum_{t=2}^\infty\tilde{Y}_{t-1}\epsilon_t) \rightarrow 0
        \end{aligned}
        \nonumber
    \end{equation}
    so,
    \begin{enumerate}
        \item $\frac{1}{T}\sum_{t=2}^T \tilde{Y}_{t-1}^2 \stackrel{p}{\longrightarrow} \mathbb{E}[\tilde{Y}_{t-1}^2]=\frac{\sigma^2}{1-\phi^2}>0$
        \item $\frac{1}{T}\sum_{t=2}^T \tilde{Y}_{t-1}\epsilon_t \stackrel{p}{\rightarrow} \mathbb{E}[\tilde{Y}_{t-1}\epsilon_t]=0$
    \end{enumerate}
    \begin{note}
        If $\mathbb{E}[|\epsilon_t|^r]<\infty$ for some $r>2$, then the consistency can hold by Mixingale LLN.
    \end{note}
\end{proof}
\begin{theorem}[Mixingale LLN]
    If $\{X_t\}$ is a \underline{uniformly integrable $L^1-$mixingale} with the upper bound of limitation
    \begin{equation}
        \begin{aligned}
            \underbrace{\overline{\lim_{T \rightarrow \infty}}}_{``\lim\sup T \rightarrow \infty''}\frac{1}{T}\sum_{t=1}^T C_t<\infty,
        \end{aligned}
        \nonumber
    \end{equation}
    then
    \begin{equation}
        \begin{aligned}
            \frac{1}{T}\sum_{t=1}^T X_t \stackrel{p}{\longrightarrow} 0
        \end{aligned}
        \nonumber
    \end{equation}
\end{theorem}

\subsubsection*{$L^1-$mixingale}
\begin{definition}[$L^1-$mixingale]\label{def:mixingale}
    A sequence $\{X_t\}$ is an \textbf{$L^1-$mixingale} \underline{iff} $\exists\{Z_t\},\{c_t\},\{\xi_m\}$ s.t.
    \begin{equation}
        \begin{aligned}
            \mathbb{E}[X_t\mid Z_t,Z_{t-1},...]=X_t,\forall t
        \end{aligned}
        \label{def:mixingale_1}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            \mathbb{E}\left(|\mathbb{E}[X_t\mid Z_{t-m},Z_{t-m-1},...]|\right)\leq c_t\xi_m,\forall t,m\geq 1
        \end{aligned}
        \label{def:mixingale_2}
    \end{equation}
    where $\lim_{m \rightarrow \infty}\xi_m =0$.
\end{definition}
\begin{lemma}[Some Properties of $L^1-$mixingale]
    If $X_t\sim i.i.d$ with $\mathbb{E}[X_t]=0$, then
    \begin{enumerate}[(i).]
        \item $\{X_t\}$ is an $L^1-$mixingale (with $Z_t=X_t$, $c_t=0$, $\xi_m=0$).
        \item $\frac{1}{T}\sum_{t=1}^T X_t \stackrel{p}{\longrightarrow} 0$.
    \end{enumerate}
    If $X_t=Z\sim \mathcal{N}(0,1),\forall t$, then
    \begin{enumerate}[(i).]
        \item $\{X_t\}$ is not an $L^1-$mixingale,
        \item $\frac{1}{T}\sum_{t=1}^T X_t = Z \stackrel{p}{\nrightarrow} 0$.
    \end{enumerate}
    If $\{X_t\}$ is an $L^1-$mixingale,
    \begin{equation}
        \begin{aligned}
            \mathbb{E}[X_t]=\mathbb{E}\left(\mathbb{E}[X_t\mid Z_{t-m},Z_{t-m-1},...]\right)=0
        \end{aligned}
        \nonumber
    \end{equation}
\end{lemma}
\begin{remark}
    \begin{enumerate}
        \item If $Z_t=X_t$, then \ref{def:mixingale_1} holds.
        \item If \ref{def:mixingale_1} and \ref{def:mixingale_2} hold, then they hold with $Z_t=X_t$.
        \item If $X_t=g\left(\epsilon_t,\epsilon_{t-1},...\right)$, then \ref{def:mixingale_1} holds with $Z_t=\epsilon_t$.
    \end{enumerate}
    In AR(1) examples:
    \begin{enumerate}
        \item $\{\underbrace{\tilde{Y}_{t-1}^2-\mathbb{E}[\tilde{Y}_{t-1}^2]}_{X_t}\}$ is an $L^1-$mixingale (with $Z_t=\epsilon_{t-1},c_t\equiv 1$).
        \item $\{\underbrace{\tilde{Y}_{t-1}\epsilon_t}_{X_t}\}$ is an $L^1-$mixingale (with $Z_t=\epsilon_t,\xi_1=0$).
    \end{enumerate}
    \begin{example}[ (Important Case)]
        If $\{X_t\}$ is an $L^1-$mixingale with $\xi_1=0$, then
        \begin{equation}
            \begin{aligned}
                \mathbb{E}\left[X_t\mid Z_{t-1},Z_{t-2},...\right]=0 \stackrel{LIE}{\Rightarrow} &\mathbb{E}\left[X_t\mid Z_{t-m},Z_{t-m-1},...\right]\\
                =&\mathbb{E}\left[\mathbb{E}\left[X_t\mid Z_{t-1},Z_{t-2},...\right]\mid Z_{t-m},Z_{t-m-1},...\right]=0,\forall m\\
                \Rightarrow & \xi_m=0,\forall m\geq 1\\
                \Rightarrow &\textnormal{we can have } c_t\equiv 1
            \end{aligned}
            \nonumber
        \end{equation}
        $\mathbb{E}\left[X_t\mid Z_{t-1},Z_{t-2},...\right]=0 \stackrel{LIE}{\Rightarrow} \mathbb{E}\left[X_t\mid X_{t-1},X_{t-2},...\right]=0$.\\
        \underline{Terminology:} $\{X_t\}$ is a \textbf{martingale difference sequence (MDS)} if $\mathbb{E}\left[X_t\mid X_{t-1},X_{t-2},...\right]=0$.\\
        \begin{definition}[Martingale Difference Sequence (MDS)]
            $\{X_t\}$ is an MDS iff it is an $L^1-$mixingale with $\xi_m=0$.
        \end{definition}
        $\{\tilde{Y}_{t-1}\epsilon_t\}$ is an MDS because
        \begin{equation}
            \begin{aligned}
                \mathbb{E}[\tilde{Y}_{t-1}\epsilon_t\mid \epsilon_{t-1},\epsilon_{t-2},...]=\tilde{Y}_{t-1}\mathbb{E}[\epsilon_t\mid \epsilon_{t-1},\epsilon_{t-2},...]=0
            \end{aligned}
            \nonumber
        \end{equation}
        Thus, $\mathbb{E}[\tilde{Y}_{t-1}\epsilon_t\mid \tilde{Y}_{t-2}\epsilon_{t-1},\tilde{Y}_{t-3}\epsilon_{t-2},...]=0$
    \end{example}
\end{remark}
\subsubsection*{Uniformly Integrality}
\begin{definition}[Uniformly Integrable]
    A sequence $\{X_t\}$ is \textbf{uniformly integrable} iff
    \begin{equation}
        \begin{aligned}
            \lim_{m \rightarrow \infty}\sup_t \mathbb{E}\left[|X_t|\mathbf{1}\left(|X_t|>M\right)\right]=0
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}
\begin{remark}
    \begin{enumerate}
        \item If $X_T \stackrel{d}{\longrightarrow}_{T \rightarrow \infty} \mathcal{N}\left(0,1\right)$ and if $\{X_T\}$ is uniformly integrable, then $\mathbb{E}[X_T] \rightarrow_{T \rightarrow \infty} 0$.
        \item \underline{Integrality:} $\mathbb{E}[|X_T|]<\infty$ iff $\lim_{m \rightarrow \infty}\mathbb{E}\left[|X_T|\mathbf{1}\left(|X_T|>M\right)\right]=0$.
        \item If $\{X_t\}$ is uniformly integrable, then $\sup_t \mathbb{E}[|X_t|]<\infty$.
        \item If $\sup_t \mathbb{E}[|X_t|^r]<\infty$ for some $r>1$, then $\{X_t\}$ is uniformly integrable.
        \subitem AR(1) example: If $\mathbb{E}[|\epsilon_t|^r]<\infty$ for some $r>2$, then $\sup_t \mathbb{E}[|\tilde{Y}_{t-1}^2-\mathbb{E}[\tilde{Y}_{t-1}^2]|^\frac{r}{2}]<\infty$. So, $\{\tilde{Y}_{t-1}^2-\mathbb{E}[\tilde{Y}_{t-1}^2]\}$ is uniformly integrable.
        \item If $\{X_t\}$ is strictly (marginally) stationary, then $\{X_t\}$ is uniformly integrable iff $\mathbb{E}[|X_T|]<\infty,\forall T$.
    \end{enumerate}
\end{remark}


\begin{example}[ (AR(1) Example)]
    If $\mathbb{E}[\epsilon_t^2]<\infty$, then $\{\tilde{Y}_{t-1}^2-\mathbb{E}[\tilde{Y}_{t-1}^2]\}$ and $\{\tilde{Y}_{t-1}\epsilon_t\}$ are uniformly integrable $L^1-$mixingales with $c_t\equiv 1$. Then,
    \begin{enumerate}
        \item $\frac{1}{T}\sum_{t=2}^T\tilde{Y}_{t-1}^2 \stackrel{P}{\longrightarrow} \mathbb{E}[\tilde{Y}_{t-1}^2]=\frac{\sigma^2}{1-\phi^2}$.
        \item $\frac{1}{T}\sum_{t=2}^T\tilde{Y}_{t-1}\epsilon_t \stackrel{P}{\longrightarrow} \mathbb{E}[\tilde{Y}_{t-1}\epsilon_t] = 0$.
    \end{enumerate}
    \underline{Strict Stationary:} If $\{(X_t,Z_t)\}$ is strictly stationary, then
    \begin{enumerate}[$\circ$]
        \item $\mathbb{E}\left[|X_t|\mathbf{1}\left(|X_t|>M\right)\right]$ does not depend on $t$. Then, $\{X_t\}$ is uniformly integrable iff $\mathbb{E}\left[|X_t|\right]<\infty,\forall t$.
        \item $\mathbb{E}[X_t\mid Z_{t-m},Z_{t-m-1},...]$ does not depend on $t$. Then, if $\{X_t\}$ is uniformly integrable, then $\{X_t\mid Z_{t-m},Z_{t-m-1},...\}$ is an $L^1-$mixingale, then $c_t\equiv 1$ ``works''.
    \end{enumerate}
\end{example}
\begin{corollary}[to Mixingale LLN]
    If $\{X_t\}$ is a strictly stationary $L^1-$mixingale, then
    \begin{equation}
        \begin{aligned}
            \frac{1}{T}\sum_{t=1}^T X_t \stackrel{P}{\rightarrow} \mathbb{E}[X_t]=0
        \end{aligned}
        \nonumber
    \end{equation}
\end{corollary}

\subsubsection*{Asymptotic Normality:} Suppose
\begin{enumerate}[(i).]
    \item $\frac{1}{T}\sum_{t=2}^T Y_{t-1}^2 \stackrel{p}{\longrightarrow} Q$ (some $Q\succ 0$);
    \item $\frac{1}{\sqrt{T}}\sum_{t=2}^T Y_{t-1}\epsilon_t \stackrel{d}{\longrightarrow} \mathcal{N}\left(0,V\right)$ (some $V\succ 0$).
\end{enumerate}
Then, $\sqrt{T}\left(\hat{\phi}-\phi\right) \stackrel{d}{\longrightarrow} \mathcal{N}\left(0,Q^{-1}VQ^{-1}\right)$.

\begin{claim}
    (i) and (ii) hold with $Q=\frac{\sigma^2}{1-\phi^2}$ and $V=\sigma^2Q$. Thus, $$\sqrt{T}\left(\hat{\phi}-\phi\right) \stackrel{d}{\longrightarrow} \mathcal{N}\left(0,1-\phi^2\right).$$
\end{claim}


\begin{remark}
    Recall that
    \begin{enumerate}
        \item We can assume $Y_{t-1}=\tilde{Y}_{t-1}=\sum_{i=0}^\infty\phi^i\epsilon_{t-1-i}$.
        \item (Definition \ref{def:mixingale}) $\{X_t\}$ is an $L^1-$mixingale iff $\exists\{Z_t\},\{c_t\},\{\xi_m\}$ s.t.
        \begin{equation}
            \begin{aligned}
                \mathbb{E}[X_t\mid Z_t,Z_{t-1},...]=X_t,\forall t
            \end{aligned}
            \nonumber
        \end{equation}
        \begin{equation}
            \begin{aligned}
                \mathbb{E}\left(|\mathbb{E}[X_t\mid Z_{t-m},Z_{t-m-1},...]|\right)\leq c_t\xi_m,\forall t,m\geq 1
            \end{aligned}
            \nonumber
        \end{equation}
        where $\lim_{m \rightarrow \infty}\xi_m =0$.
        \item $\{X_t\}$ is an MDS iff it is an $L^1-$mixingale with $\xi_m=0$.
    \end{enumerate}
\end{remark}

\begin{theorem}[Martingale CLT, (Brown, 1971)]
    If $\{X_t\}$ is an MDS with $\{(X_t,Z_t)\}$ strictly stationary and if
    \begin{equation}
        \begin{aligned}
            \frac{1}{T}\sum_{t=1}^T \mathbb{E}\left[X_t^2\mid Z_{t-1},Z_{t-2},...\right] \stackrel{P}{\longrightarrow} \mathbb{E}[X_1^2]\ (<\infty)
        \end{aligned}
        \nonumber
    \end{equation}
    (conditional second moment condition).
    Then,
    \begin{equation}
        \begin{aligned}
            \frac{1}{\sqrt{T}}\sum_{t=1}^T X_t \stackrel{d}{\longrightarrow} \mathcal{N}\left(0,\mathbb{E}[X_1^2]\right)
        \end{aligned}
        \nonumber
    \end{equation}
\end{theorem}
For the AR(1) example, we have
\begin{enumerate}[$\circ$]
    \item $X_t=\tilde{Y}_{t-1}\epsilon_t,Z_t=\epsilon_t$.
    \item MDS property:
    \begin{equation}
        \begin{aligned}
            \mathbb{E}\left[\tilde{Y}_{t-1}\epsilon_t\mid\epsilon_{t-1},\epsilon_{t-2},...\right]=\tilde{Y}_{t-1}\mathbb{E}\left[\epsilon_t\mid\epsilon_{t-1},\epsilon_{t-2},...\right]=0
        \end{aligned}
        \nonumber
    \end{equation}
    \item (Conditional) second moment condition:
    \begin{equation}
        \begin{aligned}
            \mathbb{E}\left[\tilde{Y}_{t-1}^2\epsilon_t^2\mid\epsilon_{t-1},\epsilon_{t-2},...\right]=\tilde{Y}_{t-1}^2\mathbb{E}\left[\epsilon_t^2\mid\epsilon_{t-1},\epsilon_{t-2},...\right]=\tilde{Y}_{t-1}^2\sigma^2\stackrel{P}{\longrightarrow} \mathbb{E}\left[\tilde{Y}_{t-1}^2\epsilon_t^2\right]
        \end{aligned}
        \nonumber
    \end{equation}
    \begin{proof}
        The Convergence of $\tilde{Y}_{t-1}^2\sigma^2$:
        \begin{equation}
            \begin{aligned}
                \frac{1}{T}\sum_{t=2}^T\left[\sigma^2\tilde{Y}_{t-1}^2\right]=\sigma^2\frac{1}{T}\sum_{t=2}^T\tilde{Y}_{t-1}^2 \stackrel{P}{\longrightarrow} \frac{\sigma^4}{1-\phi^2}
            \end{aligned}
            \nonumber
        \end{equation}
        and the expectation of $\tilde{Y}_{t-1}^2\epsilon_t^2$
        \begin{equation}
            \begin{aligned}
                \mathbb{E}\left[\tilde{Y}_{t-1}^2\epsilon_t^2\right]=\mathbb{E}[\tilde{Y}_{t-1}^2]\mathbb{E}[\epsilon_t^2]=\frac{\sigma^4}{1-\phi^2}
            \end{aligned}
            \nonumber
        \end{equation}
    \end{proof}
\end{enumerate}
Therefore, by the Martingale CLT, we have
\begin{equation}
    \begin{aligned}
        \frac{1}{\sqrt{T}}\sum_{t=1}^T \tilde{Y}_{t-1}\epsilon_t \stackrel{d}{\longrightarrow} \mathcal{N}\left(0,\frac{\sigma^4}{1-\phi^2}\right)
    \end{aligned}
    \nonumber
\end{equation}
Then, by the template of asymptotic normality, we have
\begin{equation}
    \begin{aligned}
        \sqrt{T}\left(\hat{\phi}-\phi\right) \stackrel{d}{\longrightarrow} \mathcal{N}\left(0,1-\phi^2\right).
    \end{aligned}
    \nonumber
\end{equation}


\subsection*{Variance Estimation}
To be estimated:
\begin{equation}
    \begin{aligned}
        1-\phi^2=\sigma^2Q^{-1};\ \sigma^2=\mathbb{E}[\epsilon_t^2],Q=\mathbb{E}[\tilde{Y}_{t-1}^2]
    \end{aligned}
    \nonumber
\end{equation}
Consistent estimators:
\begin{enumerate}[(i).]
    \item $1-\hat{\phi}^2$
    \item $\hat{\sigma}^2\hat{Q}^{-1}$, where $\hat{\sigma}^2=\frac{1}{T-1}\sum_{t=2}^T\left(Y_t-\hat{\phi}Y_{t-1}\right)^2$ and $\hat{Q}=\frac{1}{T-1}\sum_{t=2}^T\tilde{Y}_{t-1}^2$.
\end{enumerate}
\begin{remark}
    \begin{enumerate}
        \item (ii) is proportional to the ``homoskedasticity-only'' OLS variance estimator;
        \item (ii)/OLS variance estimator also works in AR(p) models.
    \end{enumerate}
\end{remark}


\chapter{Vector Time Series}
\section{Generalized Definitions}
Notation: $Y_t=\left(Y_{t,1},...,Y_{t,n}\right)'\in \mathbb{R}^{n\times 1}$.

The definition of strict stationarity and covariance stationary can be generalized to vector time series.
\begin{definition}[Strict Stationarity]
    A process $\{Y_t:t\in \mathbb{Z}\}$ is \textbf{strictly stationary} \textit{if and only if}
    \begin{equation}
        \begin{aligned}
            \left(Y_t,...,Y_{t+k}\right)\underbrace{\sim}_\textnormal{``is distributed as''} \left(Y_0,...,Y_{k}\right),\ \forall t\in \mathbb{Z},k\geq 0
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}

\begin{definition}[Covariance Stationary]
    A process $\{Y_t:t\in \mathbb{Z}\}$ is \textbf{covariance stationary} \textit{iff} $\mathbb{E}[Y_{t,i}^2]<\infty$ ($\forall t,i$) and it satisfies \eqref{v_s} and \eqref{v_ss}.
    \begin{enumerate}
        \item Same Expectation:
        \begin{equation}
            \begin{aligned}
                \mathbb{E}[Y_t]=\left(\mathbb{E}[Y_{t,1}],...,\mathbb{E}[Y_{t,n}]\right)'=\mu,\\ \forall t \textnormal{ (for some $\mu\in \mathbb{R}^{n\times 1}$)}
            \end{aligned}
            \label{v_s}
            \tag{*}
        \end{equation}
        \item Covariance only depends on time length:
        \begin{equation}
            \begin{aligned}
                \textnormal{Cov}(Y_t,Y_{t-j})=\mathbb{E}[\underbrace{(Y_t-\mu)(Y_{t-j}-\mu)'}_{n\times n}]=\Gamma(j),\\ \forall t,j \textnormal{ (for some $\Gamma(\cdot): \mathbb{Z} \rightarrow \mathbb{R}^{n\times n}$)}
            \end{aligned}
            \label{v_ss}
            \tag{**}
        \end{equation}
    \end{enumerate}
\end{definition}
\begin{note}
    Matrix multiplication is not commutative. Thus, $\Gamma(j)=\textnormal{Cov}(Y_t,Y_{t-j})\neq \textnormal{Cov}(Y_{t-j},Y_t)=\Gamma(-j)$. However, we have
    \begin{equation}
        \begin{aligned}
            \Gamma(j)=\textnormal{Cov}(Y_t,Y_{t-j})=\textnormal{Cov}(Y_{t-j},Y_t)'=\Gamma(-j)'
        \end{aligned}
        \nonumber
    \end{equation}
\end{note}
\begin{note}
    $\mathbb{E}[Y_{t,i}^2],\forall t,i<\infty \Leftrightarrow \sum_{i=1}^n \mathbb{E}[Y_{t,i}^2]<\infty,\forall t \Leftrightarrow \mathbb{E}[\sum_{i=1}^n Y_{t,i}^2]<\infty,\forall t\Leftrightarrow \mathbb{E}\left[\|Y_t\|^2\right]<\infty,\forall t$, where $\|Y_t\|^2=Y'_tY_t$ is the Euclidean norm.
\end{note}

\begin{definition}[White Noise]
    A process $\{\epsilon_t:t\in \mathbb{Z}\}$ is a \textbf{white noise} process iff it is covariance stationary with $\mathbb{E}[\epsilon_t]=0$ and
    \begin{equation}
        \begin{aligned}
            \textnormal{Cov}(\epsilon_t,\epsilon_{t-j})=\left\{\begin{matrix}
                \Sigma,& \textnormal{ if }j=0\\
                0,& \textnormal{ otherwise}
            \end{matrix}\right.
        \end{aligned}
        \nonumber
    \end{equation}
    We use \textit{notation} $\epsilon_t\sim \textnormal{WN}(\underbrace{0}_{n\times 1},\underbrace{\Sigma}_{n\times n})$.
\end{definition}



\section{Vector $MA(\infty)$}
\begin{definition}[Vector $MA(\infty)$]
    $Y_t\sim VMA(\infty)$ iff
    \begin{equation}
        \begin{aligned}
            \underbrace{Y_t}_{n\times 1}=\underbrace{\mu}_{n\times 1}+\sum_{i=0}^{\infty}\underbrace{\psi_i}_{n\times n}\underbrace{\epsilon_{t-i}}_{n\times 1},\ \forall t,
        \end{aligned}
        \nonumber
    \end{equation}
    where
    \begin{enumerate}[$\cdot$]
        \item $\epsilon_t\sim \textnormal{WN}(0,\Sigma)$.
        \item $\sum_{i=0}^\infty\|\psi_i\|^2<\infty$.
    \end{enumerate}
    \begin{note}
        The white noise can have different dimension than $Y_t$: $\epsilon_t\in \mathbb{R}^{m\times 1},\psi_i\in \mathbb{R}^{n\times m}$.
    \end{note}
\end{definition}
\underline{Existence:} $\sum_{i=0}^\infty\psi_i\epsilon_{t-i}$ exists (element-by-element, as a limit in mean square) iff
    \begin{equation}
        \begin{aligned}
            \sum_{i=0}^\infty\psi_{ijk}^2<\infty,\ j,k=1,...,n
        \end{aligned}
        \nonumber
    \end{equation}
    where $\psi_{ijk}$ is element $(j,k)$ of $\psi_i$.
\underline{Equivalent Formulations:}
\begin{equation}
    \begin{aligned}
        &\sum_{i=0}^\infty\psi_{ijk}^2<\infty,\ j,k=1,...,n\\
        \Leftrightarrow & \sum_{j,k=1}^n \sum_{i=0}^\infty\psi_{ijk}^2<\infty\\
        \Leftrightarrow & \sum_{i=0}^\infty\sum_{j,k=1}^n\psi_{ijk}^2<\infty\\
        \Leftrightarrow & \sum_{i=0}^\infty\|\psi_i\|^2<\infty
    \end{aligned}
    \nonumber
\end{equation}
where $\|\psi_i\|^2=\sum_{j,k=1}^n\psi_{ijk}^2=Tr(\psi'_i\psi_i)$ is (the squared) Frobenius norm of $\psi_i$.

\begin{lemma}[Properties of Vector $MA(\infty)$]
    For $Y_t\sim VMA(\infty)$, the following properties hold:
    \begin{enumerate}
        \item $\{Y_t\}$ is covariance stationary.
        \item $\mathbb{E}[Y_t]=\mu$.
        \item $\textnormal{Cov}[Y_t,Y_{t-j}]=\sum_{i=0}^\infty\psi_{i+j}\Sigma\psi_{i}'$.
    \end{enumerate}
\end{lemma}

\section{Vector $AR(1)$}
\begin{definition}[Vector $AR(1)$]
    $Y_t\sim VAR(1)$ iff
    \begin{equation}
        \begin{aligned}
            \underbrace{Y_t}_{n\times 1}=\underbrace{c}_{n\times 1}+\underbrace{\Phi}_{n\times n} \underbrace{Y_{t-1}}_{n\times 1} + \underbrace{\epsilon_t}_{n\times 1},\ t\geq 2
        \end{aligned}
        \nonumber
    \end{equation}
    where $\epsilon_t\sim \textnormal{WN}(0,\Sigma)$
\end{definition}

\begin{lemma}
    If $Y_t=\mu+\sum_{i=0}^\infty\Phi^i\epsilon_{t-i}$, then $Y_t=c+\Phi Y_{t-1} + \epsilon_t$, where $c=(I_n-\Phi)\mu$.
\end{lemma}

\begin{lemma}
    The existence of $\sum_{i=0}^\infty\Phi^i\epsilon_{t-i}$ can be given by one of the following \textit{equivalent} formulations:
    \begin{enumerate}
        \item $\sum_{i=0}^\infty\|\Phi^i\|^2<\infty$.
        \item $|\lambda|<1$, where $\lambda$ is an eigenvalue of $\Phi$.
        \item $|I_n-\Phi z|=0 \Rightarrow |z|>1$. (Mostly used).
    \end{enumerate}
\end{lemma}

\begin{definition}[Stability of $VAR(1)$]
    The $VAR(1)$ model is \textbf{stable} iff $|I_n-\Phi z|=0 \Rightarrow |z|>1$.
\end{definition}

\underline{Facts}:
\begin{enumerate}
    \item The $VAR(1)$ model admits a $VMA(\infty)$ solution of the form
    \begin{equation}
        \begin{aligned}
            Y_t=\mu+\sum_{i=0}^\infty\psi_i\epsilon_{t-i}
        \end{aligned}
        \nonumber
    \end{equation}
    iff it is stable.
    \item OLS ``works'' when the $VAR(1)$ is stable.
\end{enumerate}

\section{Spectral Analysis}
\begin{definition}[Spectral Density Function]
    If $\exists f:[-\pi, \pi] \rightarrow \mathbb{C}^{n\times n}$ such that
    \begin{equation}
        \begin{aligned}
            \Gamma(j)=\int_{-\pi}^\pi \exp\left(ij\lambda\right)f(\lambda)d \lambda,\ \forall j\in \mathbb{Z},
        \end{aligned}
        \nonumber
    \end{equation}
    then $f(\cdot)$ is called a \textbf{spectral density function}.
\end{definition}
Given the existence of a spectral density function,
\begin{equation}
    \begin{aligned}
        \Gamma(0)=\int_{-\pi}^\pi f(\lambda)d\lambda
    \end{aligned}
    \nonumber
\end{equation}

\begin{lemma}[Short Memory]
    If $\sum_{j=-\infty}^\infty \|\Gamma(j)\|<\infty$, then the spectral density function $f$ exists and
    \begin{equation}
        \begin{aligned}
            f(\lambda)=\frac{1}{2\pi}\sum_{j=-\infty}^\infty \exp\left(-ij\lambda\right)\Gamma(j),\ \lambda\in[-\pi, \pi],
        \end{aligned}
        \nonumber
    \end{equation}
\end{lemma}
Then,
\begin{equation}
    \begin{aligned}
        &f(\lambda) = f(-\lambda)^T\\
        &2\pi f(0) = \sum_{j=-\infty}^\infty \Gamma(j) = \Gamma(0) + \sum_{j=1}^\infty \left\{\Gamma(j)+\Gamma(j)^T\right\}
    \end{aligned}
    \nonumber
\end{equation}

\begin{example}
    \underline{$VMA(\infty)$ Case:} Suppose
    \begin{equation}
        \begin{aligned}
            Y_t=\mu+\sum_{i=0}^\infty\psi_i\epsilon_{t-i},\ \forall t,
        \end{aligned}
        \nonumber
    \end{equation}
    where $\epsilon_t\sim \textnormal{WN}(0,\Sigma)$ and $\sum_{i=0}^\infty\|\psi_i\|^2<\infty$. Then, $\sum_{j=-\infty}^\infty \|\Gamma(j)\|<\infty$ and
    \begin{equation}
        \begin{aligned}
            f(\lambda)&=\frac{1}{2\pi}\sum_{j=-\infty}^\infty \exp\left(-ij\lambda\right)\Gamma(j)\\
            \Gamma(j)&=\sum_{k=0}^\infty\psi_{k+j}\Sigma\psi_{k}^T
        \end{aligned}
        \nonumber
    \end{equation}
    which can be rewritten as
    \begin{equation}
        \begin{aligned}
            f(\lambda)&=\frac{1}{2\pi}\Psi(\exp\left(-i\lambda\right))\Sigma\Psi(\exp\left(-i\lambda\right))^T\\
            \Psi(z)&=\sum_{j=0}^\infty\psi_jz^j
        \end{aligned}
        \nonumber
    \end{equation}
    Then,
    \begin{equation}
        \begin{aligned}
            2\pi f(0)=\Psi(1)\Sigma\Psi(1)^T
        \end{aligned}
        \nonumber
    \end{equation}
\end{example}


\section{Estimation}
Suppose
\begin{equation}
    \begin{aligned}
        Y_t=\Phi Y_{t-1} + \epsilon_t, \ t\geq 2,
    \end{aligned}
    \nonumber
\end{equation}
where
\begin{enumerate}
    \item $\epsilon_t\sim {i.i.d.} \mathcal{N}(0,\Sigma)$.
    \item $Y_1\perp (\epsilon_2,...,\epsilon_T)$.
\end{enumerate}

\begin{claim}
    \begin{equation}
        \begin{aligned}
            \hat{\Phi}_{ML}&=\cdots=\left(\sum_{t=2}^T Y_tY_{t-1}^T\right)\left(\sum_{t=2}^T Y_tY_{t-1}^T\right)^{-1}\\
            &=\argmin_{\Phi} \sum_{t=2}^T \left(Y_t-\Phi Y_{t-1}\right)^T\left(Y_t-\Phi Y_{t-1}\right)\\
            &=\hat{\Phi}_{OLS}
        \end{aligned}
        \nonumber
    \end{equation}
\end{claim}
where
\begin{equation}
    \begin{aligned}
        \left(\hat{\Phi}_{ML},\hat{\Sigma}_{ML}\right)=\argmax_{(\Phi,\Sigma)} f_{2:T}\left(Y_2,...,Y_T\mid Y_1;\Phi,\Sigma\right)
    \end{aligned}
    \nonumber
\end{equation}

\begin{lemma}[Prediction-error Decomposition]
    $Y_t\mid Y_1,...,Y_{t-1}\sim \mathcal{N}(\Phi Y_{t-1},\Sigma)$ for $t\geq 2$. Then,
    \begin{equation}
        \begin{aligned}
            f_{2:T}\left(Y_2,...,Y_T\mid Y_1;\Phi,\Sigma\right)=\prod_{t=2}^Tf_t\left(Y_t\mid Y_1,...,Y_{t-1};\Phi,\Sigma\right),
        \end{aligned}
        \nonumber
    \end{equation}
    where $f_t\left(Y_t\mid Y_1,...,Y_{t-1};\Phi,\Sigma\right)=\frac{1}{\sqrt{2\pi}}|\Sigma|^{-\frac{1}{2}}\exp\left(-\frac{1}{2}\left(Y_t-\Phi Y_{t-1}\right)^T\Sigma^{-1}\left(Y_t-\Phi Y_{t-1}\right)\right)$
\end{lemma}
Then,
\begin{equation}
    \begin{aligned}
        \argmax_{\Phi} f_{2:T}\left(Y_2,...,Y_T\mid Y_1;\Phi,\Sigma\right)=\argmin_{\Phi} \sum_{t=2}^T \left(Y_t-\Phi Y_{t-1}\right)^T\Sigma^{-1}\left(Y_t-\Phi Y_{t-1}\right)
    \end{aligned}
    \nonumber
\end{equation}

\begin{lemma}
    $\argmin_{\Phi} \sum_{t=2}^T \left(Y_t-\Phi Y_{t-1}\right)^T\Sigma^{-1}\left(Y_t-\Phi Y_{t-1}\right)$ does not depend on $\Sigma$.
\end{lemma}
Thus,
\begin{equation}
    \begin{aligned}
        \hat{\Phi}_{ML}&=\argmin_{\Phi} \sum_{t=2}^T \left(Y_t-\Phi Y_{t-1}\right)^T\Sigma^{-1}\left(Y_t-\Phi Y_{t-1}\right)\\
        &=\argmin_{\Phi} \sum_{t=2}^T \left(Y_t-\Phi Y_{t-1}\right)^T\left(Y_t-\Phi Y_{t-1}\right)=\hat{\Phi}_{OLS}
    \end{aligned}
    \nonumber
\end{equation}

\begin{proposition}[Hamilton, Prop 11.1]
    Suppose
    \begin{equation}
        \begin{aligned}
            Y_t=\Phi Y_{t-1} + \epsilon_t, \ t\geq 2,
        \end{aligned}
        \nonumber
    \end{equation}
    where
    \begin{enumerate}
        \item $|I_n-\Phi z|=0 \Rightarrow |z|>1$.
        \item $\epsilon_t\sim{i.i.d.}(0,\Sigma)$ with $\mathbb{E}(\|\epsilon_t\|^4)<\infty$.
        \item $Y_1=\sum_{i=0}^\infty \Phi^i\epsilon_{1-i}$.
    \end{enumerate}
    Then,
    \begin{enumerate}
        \item $\hat{\Phi}_{OLS}$ is consistent.
        \item $\hat{\Phi}_{OLS}$ is asymptotically normal.
        \item OLS variance estimator ``works.''
    \end{enumerate}
\end{proposition}


\section{$VAR(p)$ Models}
\begin{definition}[$VAR(p)$ Model]
    $Y_t\sim VAR(p)$ iff
    \begin{equation}
        \begin{aligned}
            Y_t=c+\Phi_1 Y_{t-1}+\Phi_2 Y_{t-2}+\cdots+\Phi_p Y_{t-p}+\epsilon_t,\ t\geq p+1
        \end{aligned}
        \nonumber
    \end{equation}
    where $\epsilon_t\sim WN(0,\Sigma)$.
\end{definition}
\begin{lemma}
    OLS ``works'' if $\epsilon_t\sim{i.i.d.}(0,\Sigma)$ and if the $VAR(p)$ model is stable.
\end{lemma}
The OLS estimator is given by
\begin{equation}
    \begin{aligned}
        \left(\hat{c}_{OLS},\hat{\Phi}_{1,OLS},\cdots,\hat{\Phi}_{p,OLS}\right)=\argmin_{\left(c,\Phi_1,\cdots,\Phi_p\right)} \sum_{t=p+1}^T \|Y_t-c-\Phi_1 Y_{t-1}-\cdots-\Phi_p Y_{t-p}\|^2
    \end{aligned}
    \nonumber
\end{equation}

Using the \underline{Lag operator notation}, the $VAR(p)$ model can be written as
\begin{equation}
    \begin{aligned}
        \Phi(L)Y_t=c+\epsilon_t,\ t\geq p+1
    \end{aligned}
    \nonumber
\end{equation}
where
\begin{equation}
    \begin{aligned}
        \Phi(L)=I_n-\Phi_1 L-\Phi_2 L^2-\cdots-\Phi_p L^p
    \end{aligned}
    \nonumber
\end{equation}

\begin{definition}[Stability of $VAR(p)$]
    The $VAR(p)$ is \textbf{stable} iff
    \begin{equation}
        \begin{aligned}
            |\Phi(z)|=0 \Rightarrow |z|>1
        \end{aligned}
        \nonumber
    \end{equation}
    where
    \begin{equation}
        \begin{aligned}
            \Phi(z)=I_n-\Phi_1 z-\Phi_2 z^2-\cdots-\Phi_p z^p
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}

\begin{lemma}
    The $VAR(p)$ model admits an $MA(\infty)$ solution of the form
    \begin{equation}
        \begin{aligned}
            Y_t=\mu+\sum_{i=0}^\infty \psi_i\epsilon_{t-i},\ t\geq 1
        \end{aligned}
        \nonumber
    \end{equation}
    iff the $VAR(p)$ model is stable.
\end{lemma}

\begin{theorem}[Granger-Sims Causality]
    Suppose $\underbrace{Z_t}_{n\times 1}=\left(Y_t^T,X_t^T\right)^T\sim VAR(p)$:
    \begin{equation}
        \begin{aligned}
            \begin{bmatrix}
                \underbrace{Y_t}_{m\times 1}\\
                \underbrace{X_t}_{k\times 1}
            \end{bmatrix}
            =
            \begin{bmatrix}
                c_Y\\
                c_X
            \end{bmatrix}
            +
            \begin{bmatrix}
                \underbrace{\Phi_{YY,1}}_{m\times m}&  \underbrace{\Phi_{YX,1}}_{m\times k}\\
                \underbrace{\Phi_{XY,1}}_{k\times m}& \underbrace{\Phi_{XX,1}}_{k\times k}
            \end{bmatrix}
            \begin{bmatrix}
                Y_{t-1}\\
                X_{t-1}
            \end{bmatrix}
            +\cdots\\
            +
            \begin{bmatrix}
                \Phi_{YY,p}& \Phi_{YX,p}\\
                \Phi_{XY,p}& \Phi_{XX,p}
            \end{bmatrix}
            \begin{bmatrix}
                Y_{t-p}\\
                X_{t-p}
            \end{bmatrix}
            +
            \begin{bmatrix}
                \epsilon_Y\\
                \epsilon_X
            \end{bmatrix}
        \end{aligned}
        \nonumber
    \end{equation}
    Then, $X_t$ does not \textbf{Granger(-Sims) cause} $Y_t$ if and only if
    \begin{equation}
        \begin{aligned}
            \Phi_{YX,1}=\cdots=\Phi_{YX,p}=0
        \end{aligned}
        \nonumber
    \end{equation}
\end{theorem}

\section{GMM for Time Series}
\underline{Notation/Settings}:
\begin{enumerate}
    \item Data: $X_{1},...,X_T$
    \item Parameters of interests: $\theta_0\in \Theta\subseteq \mathbb{R}^k$ for some $k\in \mathbb{N}$.
    \item Model: $\mathbb{E}[h(x_t,\theta)]=0\Leftrightarrow \theta=\theta_0$ for some known $\mathbb{R}^m$-valued function $h(\cdot)$, where $m\geq k$.
    \item Estimator: $g_T(\theta):=\frac{1}{T}\sum_{t=1}^T h(X_t,\theta)=0$ at $\theta=\hat{\theta}_{GMM}$.
\end{enumerate}
\begin{definition}[GMM Estimator]
    The GMM estimator is
    $$\hat{\theta}_{GMM}=\argmin_{\theta\in \Theta}g_T(\theta)'W_Tg_T(\theta)$$ for some $m\times m$ matrix $W_T=W'_T\succeq 0$.
\end{definition}

\begin{example}[ (Sample Average)]
    \begin{enumerate}
        \item $\{Y_t\}$ is covariance stationary.
        \item Parameter of interest: $\mu=\mathbb{E}[Y_t],\forall t$.
        \item Estimator $\bar{Y}=\frac{1}{T}\sum_{t=1}^T Y_t$.
    \end{enumerate}
    \underline{GMM interpretation}: Let
    \begin{enumerate}
        \item $X_t=Y_t$
        \item $\theta_0=\mu\in \mathbb{R}=\Theta$ ($k=1$).
        \item $h(x_t,\theta)=x_t-\theta$ ($m=1$).
    \end{enumerate}
    \textbf{Claim:} $\hat{\theta}_{GMM}=\bar{Y}$ for all $W_T>0$ (e.g. $W_T=1$).
\end{example}

\begin{example}[ (OLS estimator in AR(1) without intercept)]
    \begin{enumerate}
        \item $Y_t=\phi Y_{t-1}+\epsilon_t$ where $\epsilon_t\sim WN(0,\sigma^2)$ and $Y_0$ is observed.
        \item Parameter of interest: $\phi\in \mathbb{R}$.
        \item OLS estimator: $\hat{\phi}_{OLS}=\frac{\sum_{t=1}^T Y_tY_{t-1}}{\sum_{t=1}^T Y_{t-1}^2}$.
    \end{enumerate}
    \underline{GMM interpretation}: Let
    \begin{enumerate}
        \item $X_t=(Y_t,Y_{t-1})'$
        \item $\theta_0=\phi\in \mathbb{R}\supseteq \Theta$ ($k=1$).
        \item $h(X_t,\theta)=Y_{t-1}(Y_t-\theta Y_{t-1})$ ($m=1$).
    \end{enumerate}
    \textbf{Claim:} $\hat{\theta}_{GMM}=\hat{\phi}_{OLS}$ for all $W_T>0$ (e.g. $W_T=1$) (provided $\Theta= \mathbb{R}$).
\end{example}

\begin{example}[ (Additional Examples of GMM)]
    \begin{enumerate}
        \item Any OLS estimator.
        \item Any Method of Moments (MM) estimator.
        \item Any 2SLS estimator.
        \item Any ML estimator.
    \end{enumerate}
\end{example}

\begin{lemma}[Properties of GMM Estimator]
    Let
    \begin{equation}
        \begin{aligned}
            \underbrace{G_T(\theta)}_{m\times k}= \frac{\partial }{\partial \theta'} \underbrace{g_T(\theta)}_{m\times 1},\ \theta\in \mathbb{R}^k
        \end{aligned}
        \nonumber
    \end{equation}
    Suppose
    \begin{enumerate}[(i).]
        \item $\sqrt{T}\left(\hat{\theta}_{GMM}-\theta_0\right)= -\left[G_T(\theta_0)'W_TG_T(\theta_0)\right]^{-1}G_T(\theta_0)'W_T\sqrt{T}g_T(\theta_0)+o_P(1)$.
        \item $G_T(\theta_0) \stackrel{P}{\longrightarrow} G$ for some $G\in \mathbb{R}^{m\times k}$ with rank $k$.
        \item $\sqrt{T}g_T(\theta_0)\stackrel{d}{\longrightarrow} \mathcal{N}\left(0,V\right)$ for some $V\succ 0$.
        \item $W_T\stackrel{P}{\longrightarrow} W$ for some $W\in \mathbb{R}^{m\times m}$ with $G'WG\succ 0$.
    \end{enumerate}
    Then, $\sqrt{T}\left(\hat{\theta}_{GMM}-\theta_0\right)\stackrel{d}{\longrightarrow} \mathcal{N}\left(0,\Omega\right)$, where $\Omega:=\left[G'WG\right]^{-1}G'WVWG\left[G'WG\right]^{-1}$, $$\Omega(W)\geq \Omega(V^{-1})=(G'V^{-1}G)^{-1}$$
\end{lemma}
\begin{remark}
    \begin{enumerate}
        \item (iv) is automatic when $W_T=W=I_m$ (and (ii) holds).
        \item 2SLS has $W_T\neq I_m$.
        \item ``Optimal'' matrix is choosing $W=V^{-1}$ such that $\Omega$ is minimized (when $m>k$).
        \item $\sqrt{T}g_T(\theta_0)=\frac{1}{\sqrt{T}}\sum_{t=1}^Th(X_t,\theta_0)$. Thus, if $h(X_t,\theta_0)$ satisfies CLT, then (iii) holds and ``usually''
        \begin{equation}
            \begin{aligned}
                V=\sum_{j=-\infty}^{\infty}\mathbb{E}\left[h(X_t,\theta_0)h(X_{t-j},\theta_0)'\right]
            \end{aligned}
            \nonumber
        \end{equation}
        \item $G_T(\theta_0)=\frac{1}{T}\sum_{t=1}^T \frac{\partial }{\partial \theta'}h(X_t,\theta_0)$. Thus, if $\frac{\partial }{\partial \theta'}h(X_t,\theta_0)$ satisfies LLN, then (ii) holds and $G=\mathbb{E}[\frac{\partial }{\partial \theta'}h(X_t,\theta_0)]$.
        \item Condition (i) requires additional work.
        \begin{enumerate}
            \item Condition (i) - Heuristic: GMM F.O.C. is
            \begin{equation}
                \begin{aligned}
                    \frac{1}{2}\frac{\partial }{\partial \theta}\left[g_T(\theta)'W_Tg_T(\theta)\right]\bigg|_{\theta=\hat{\theta}_{GMM}}=G_T(\hat{\theta}_{GMM})'W_Tg_T(\hat{\theta}_{GMM})=0
                \end{aligned}
                \nonumber
            \end{equation}
            Suppose $\hat{\theta}_{GMM}\approx \theta_0$ ($\hat{\theta}_{GMM}\stackrel{P}{\longrightarrow} \theta_0$) and $G_T(\cdot)$ exists and is ``smooth'' (continuous). Then,
            \begin{enumerate}
                \item $G_T(\hat{\theta}_{GMM})\approx G_T(\theta_0)$,
                \item $g_T(\hat{\theta}_{GMM})\approx g_T(\theta_0)+G_T(\hat{\theta}_{GMM})\left(\hat{\theta}_{GMM}-\theta_0\right)$
            \end{enumerate}
            Thus, $\left(\hat{\theta}_{GMM}-\theta_0\right)\approx -\left[G_T(\theta_0)'W_TG_T(\theta_0)\right]^{-1}G_T(\theta_0)'W_Tg_T(\theta_0)$.
            \item Condition (i) - Special Case:
            Suppose $g_T(\cdot)$ is affine:
            \begin{equation}
                \begin{aligned}
                    g_T(\theta)=A_T+B_T\theta (\textnormal{for some }A_T,B_T)
                \end{aligned}
                \nonumber
            \end{equation}
            Then, $G_T(\cdot)\equiv B_T$. Thus
            \begin{enumerate}
                \item $G_T(\hat{\theta}_{GMM})= B_T = G_T(\theta_0)$
                \item $g_T(\hat{\theta}_{GMM})= g_T(\theta_0)+G_T(\hat{\theta}_{GMM})\left(\hat{\theta}_{GMM}-\theta_0\right)$
            \end{enumerate}
            Given $\left[G_T(\theta_0)'W_TG_T(\theta_0)\right]^{-1}$ exists, then
            \begin{equation}
                \begin{aligned}
                    \left(\hat{\theta}_{GMM}-\theta_0\right)= -\left[G_T(\theta_0)'W_TG_T(\theta_0)\right]^{-1}G_T(\theta_0)'W_Tg_T(\theta_0)
                \end{aligned}
                \nonumber
            \end{equation}
            e.g. OLS, 2SLS.
        \end{enumerate}
    \end{enumerate}
\end{remark}

\paragraph*{Choosing $W_T$}
Steps:
\begin{enumerate}
    \item Find $W^*$ that minimizes $\Omega(W)=\left[G'WG\right]^{-1}G'WVWG\left[G'WG\right]^{-1}$.
    \item Find $W_T$ such that $W_T\stackrel{P}{\longrightarrow} W^*$.
\end{enumerate}
\begin{claim}
    $W^*=V^{-1}$.
\end{claim}
\begin{proof}
    \begin{equation}
        \begin{aligned}
            \Omega(W)-\Omega(V^{-1})=\left[G'WG\right]^{-1}\underbrace{\left[G'WVWG-(G'WG)\left[G'V^{-1}G\right]^{-1}(G'WG)\right]}_{:=D}\left[G'WG\right]^{-1}
        \end{aligned}
        \nonumber
    \end{equation}
    $\Omega(W)-\Omega(V^{-1})\succeq 0$ iff $D\succeq 0$.

    Let $Z\sim \mathcal{N}\left(0,V\right)$. Then,
    \begin{equation}
        \begin{aligned}
            \textnormal{Var}\left(G'WZ\mid G'V^{-1}Z\right)=G'WVWG-G'WG\left[G'V^{-1}G\right]^{-1}(G'WG)\succeq 0
        \end{aligned}
        \nonumber
    \end{equation}
\end{proof}
Then, we find $W_T=\hat{V}^{-1}$ such that $\hat{V}\stackrel{P}{\longrightarrow}V$. By (iii), $V=\lim_{T\rightarrow \infty}\textnormal{Var}[\sqrt{T}g_T(\theta_0)]=\Gamma_n(0)+\sum_{j=1}^\infty[\Gamma_n(j)+\Gamma_n(j)']$, where $\Gamma_n(j)=\mathbb{E}[h(X_t,\theta_0)h(X_{t-j},\theta_0)']$.

\begin{proposition}[Newey-West Estimator of $V$]
    $$\hat{V}=\hat{\Gamma}_n(0)+\sum_{j=1}^b\left(1-\frac{j}{b}\right)[\hat{\Gamma}_n(j)+\hat{\Gamma}_n(j)']$$
    where $\hat{\Gamma}_n(j)=\frac{1}{T}\sum_{t=j+1}^Th(X_t,\hat{\theta})h(X_{t-j},\hat{\theta})'$ and $\hat{\theta}$ is an estimator of $\theta_0$.\\
    $b$ is a ``tuning'' parameters ($b \rightarrow \infty$ as $T \rightarrow \infty$).
\end{proposition}

\textbf{Algorithm} (Two-Step GMM):
\begin{enumerate}
    \item Find $\hat{\theta}$. (e.g. $\hat{\theta}_{GMM}$ with $W_T=I_m$).
    \item Using $\hat{\theta}$ to find $\hat{V}$.
    \item Using $W=\hat{V}^{-1}$ to find $\hat{\theta}_{GMM}$.
\end{enumerate}
\begin{claim}
    Under ``regularity'' condition,
    \begin{equation}
        \begin{aligned}
            \sqrt{T}\left(\hat{\theta}_{GMM}-\theta_0\right) \stackrel{d}{\longrightarrow} N(0,\Omega^*)
        \end{aligned}
        \nonumber
    \end{equation}
    where $\Omega^*=(G'V^{-1}G)^{-1}$
\end{claim}

\paragraph*{Variance Estimation for Efficient GMM:} The estimator's variance is $\Omega^*=(G'V^{-1}G)^{-1}$. Its estimator is given by $$\hat{\Omega}^*=(\hat{G}'\hat{V}^{-1}\hat{G})^{-1}$$
where $\hat{G}=G_T(\hat{\theta}_{GMM})$.
\begin{claim}
    Under ``regularity'' condition, $\hat{\Omega}^* \stackrel{P}{\longrightarrow} \Omega^*$.
\end{claim}


\paragraph*{Variance Estimation for GMM:}
The estimator's variance is $\Omega:=\left[G'WG\right]^{-1}G'WVWG\left[G'WG\right]^{-1}$. Its estimator is given by
\begin{equation}
    \begin{aligned}
        \hat{\Omega}=\left[\hat{G}'\hat{W}\hat{G}\right]^{-1}\hat{G}'\hat{W}\hat{V}\hat{W}\hat{G}\left[\hat{G}'\hat{W}\hat{G}\right]^{-1}
    \end{aligned}
    \nonumber
\end{equation}
where
\begin{enumerate}
    \item $\hat{G}=G_T(\hat{\theta}_{GMM})$.
    \item $\hat{W}=W_T$.
    \item $\hat{V}...$ (why not do efficient GMM).
\end{enumerate}


\chapter{Non-stationary Time Series}
\section{}
Recall that a process $\{Y_t\}$ (with $\mathbb{E}[\|Y_t\|^2]<\infty$ for all $t$) is covariance stationary iff \eqref{v_s} and \eqref{v_ss} hold:
\begin{enumerate}
    \item[(*):] $\mathbb{E}[Y_t]=\mu,\forall t$ (some constant $\mu$).
    \item[(**):] $\textnormal{Cov}(Y_t,Y_{t-j})=\Gamma(j),\forall t,j$ (some function $\Gamma(\cdot)$).
\end{enumerate}

\begin{claim}
    Assumption \eqref{v_s} is implausible for most macroeconomic time series.
\end{claim}
\textbf{Solution:}
\begin{enumerate}
    \item \underline{Decomposition}:
    \begin{equation}
        \begin{aligned}
            Y_t=\mu_t+u_t,
        \end{aligned}
        \nonumber
    \end{equation}
    where $\mu_t=\mathbb{E}(Y_t)$ ($\Rightarrow \mathbb{E}(u_t)=0$).
    \item \underline{(Parametric) Model for $\mu_t$}:
    \begin{example}[ (Leading special case: ``linear trend'')]
        $\mu_t=\mu+\delta t$ (for some constant $\mu,\delta$).
    \end{example}
    (Reading: Chapter 16 in Hamilton.)
\end{enumerate}

\begin{theorem}[Folk Theorem]
    If $\{Y_t\}$ is a macroeconomic time series, then $\{\Delta Y_t\}$ satisfies \eqref{v_ss}, but $\{Y_t\}$ does not.
\end{theorem}
How do we test this folk theorem? -- Unit root testing.\\
If rejected, how should we model macroeconomic time series? -- Cointegration.

\subsection{Unit Root Testing}
\underline{Model}: The observable variable is assumed to follow
\begin{equation}
    \begin{aligned}
        y_t=\mu_t+u_t,\ t\geq 1
    \end{aligned}
    \nonumber
\end{equation}
where $\mu_t=\mathbb{E}[y_t]$ and $u_t\sim ARMA(1,\infty)$.

In lag operator notation,
\begin{equation}
    \begin{aligned}
        (1-\rho L)u_t=\psi(L)\epsilon_t,\ t\geq 1
    \end{aligned}
    \nonumber
\end{equation}
with
\begin{enumerate}
    \item $\|\rho\|\leq 1$.
    \item $\epsilon_t\sim{i.i.d.}(0,\sigma^2)$.
    \item $\psi(L)=\sum_{i=0}^\infty \psi_i L^i$ with $\sum_{i=0}^\infty i|\psi_i|<\infty$ and $\psi(1)=\sum_{i=0}^\infty \psi_i\neq 0$.
\end{enumerate}

\begin{remark}
    \begin{enumerate}
        \item If $\rho=1$, then $\Delta u_t\sim MA(\infty)$.
        \item If $|\rho|<1$, then $u_t\sim MA(\infty)$ iff $u_0=\sum_{i=0}^\infty \rho^i\{\psi(L)\epsilon_{-i}\}$.
    \end{enumerate}
    Thus, we can test folk theorem by testing
    \begin{equation}
        \begin{aligned}
            H_0: \rho=1 \textnormal{ vs. } H_1: |\rho|<1
        \end{aligned}
        \nonumber
    \end{equation}
\end{remark}

\textbf{Three Cases:}
\begin{enumerate}
    \item ``Canonical Model'': $\mu_t=0$, $\psi(L)=1$. $(1-\psi L)y_t=\epsilon_t$. Thus, $y_t\sim AR(1)$. It is a non-standard testing problem.
    \item ``Serial Correlation'': $\mu_t=0$, $\psi(L)=\sum_{i=0}^\infty \psi_i L^i$.
\end{enumerate}




































































\end{document}