\documentclass[11pt]{elegantbook}
\usepackage{graphicx}
\usepackage{float}
\definecolor{structurecolor}{RGB}{40,58,129}
\linespread{1.6}
\setlength{\footskip}{20pt}
\setlength{\parindent}{0pt}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\elegantnewtheorem{proof}{Proof}{}{Proof}
\elegantnewtheorem{claim}{Claim}{prostyle}{Claim}
\DeclareMathOperator{\col}{col}
\title{Statistical Inference and Learning}
\author{Wenxiao Yang}
\institute{Haas School of Business, University of California Berkeley}
\date{2023}
\setcounter{tocdepth}{2}
\extrainfo{All models are wrong, but some are useful.}

\cover{cover.png}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}

\addbibresource[location=local]{reference.bib} % bib

\begin{document}
\maketitle

\frontmatter
\tableofcontents

\mainmatter



\chapter{Information-Theoretic Functional}
\section{Definitions}
\subsection{Entropy}
\begin{definition}[Entropy]
    \textbf{Entropy} of pmf $\{p(x),x\in X\}$ $$H(p)=-\sum_{x\in X}p(x)\ln p(x)$$
    (concave in $p$)
\end{definition}


\subsection{Kullback-Leibler Divergence}
\begin{definition}[KL divergence, Relative entropy]
    The \textbf{Kullback-Leibler divergence (or relative entropy)} of two pmf's $p(x), x\in X$ and $q(x), x\in X$ is defined as
    \begin{equation}
        \begin{aligned}
            D(p\| q)=-\sum_{x\in X}p(x)\ln\frac{q(x)}{p(x)}
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}
\begin{proposition}[Positivity]
    $D(p\| q)=-\sum_{x\in X}p(x)\ln\frac{q(x)}{p(x)}\geq 0$ with equality iff $p=q$. (convex in $(p,q)$)
\end{proposition}
\begin{proof}[Using Jensen's Inequality]
    Let $g(X)=\frac{q(X)}{p(X)}$, then $D(p\| q)$ can be written as $D(p\| q)=-\sum_{x\in X}p(x)\ln g(x)$.

    Because $\log$ is concave, by the Jensen's inequality, we have
    \begin{equation}
        \begin{aligned}
            \sum_{x\in X}p(x)\ln g(x)\leq \ln \sum_{x\in X}p(x) g(x)=\ln\sum_{x\in X} q(X)=\ln 1=0
        \end{aligned}
        \nonumber
    \end{equation}
    where the inequality achieves equality if and only if $g(x)$ is constant for all $x\in X$. (i.e., $g(x)=\int_{-\infty}^\infty p(x)g(x)dx=1$)
\end{proof}
\begin{proof}[Alternative proof by $\ln x\leq x-1$]
    Because $\ln x\leq x-1$ for all $x>0$,
    \begin{equation}
        \begin{aligned}
            D(p\| q)=-\sum_{x\in X}p(x)\ln\frac{q(x)}{p(x)}\geq -\sum_{x\in X}p(x)\left(\frac{q(x)}{p(x)}-1\right)=-\sum_{x\in X}\left(q(x)-p(x)\right)=0
        \end{aligned}
        \nonumber
    \end{equation}
    where the inequality achieves equality if and only if $\frac{q(x)}{p(x)}=1$.
\end{proof}

\subsection{Cross-Entropy}
\begin{definition}[Cross-entropy]
    The \textbf{cross-entropy} of a pmf $p(x),x\in X$ relative to another pmf $q(x), x\in X$
    \begin{equation}
        \begin{aligned}
            H(p, q)&=-\sum_{x\in X}p(x)\ln q(x)\\&=H(p)+D(p\| q)
        \end{aligned}
        \nonumber
    \end{equation}
    $H(p,q)\geq H(p)$, the lower bound is achieved by $q=p$.
\end{definition}

\subsection{Mutual Information}
\begin{definition}[Mutual Information]
    Let $(x,y)$ be a pair of random variables with values over the space $X\times Y$ If their joint distribution is $p_{X,Y}(x,y)$ and the marginal distributions are $p_X(x)$ and $p_Y(y)$, the \textbf{mutual information} is defined as
    \begin{equation}
        \begin{aligned}
            I(p_{X,Y})&=\sum_{x\in X}\sum_{y\in Y}p_{X,Y}(x,y)\log\left(\frac{p_{X,Y}(x,y)}{p_X(x)p_Y(y)}\right)\\
            &=H(p_X)+H(p_Y)-H(p_X,p_Y)
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}


\chapter{Statistics Basics}
\section{Random Sampling}
\begin{definition}[Random Sample]
    \normalfont
    A \textbf{random sample} is a collection $X_1,...,X_n$ of random variables that are (mutually) independent and identical marginal distributions.

    $X_1,...,X_n$ are called "independent and identically distributed". The notation is $X_i\sim i.i.d. $
\end{definition}

\begin{definition}[Statistic]
    \normalfont
    A \textbf{statistic} (singular) or sample statistic is any quantity computed from values in a sample which is considered for a statistical purpose.\\
    If $X_1,...,X_n$ is a random sample and $T: \mathbb{R}^n \rightarrow \mathbb{R}^k$ (for some $k\in \mathbb{N}$), then $T(X_1,...,X_n)$ is called a \textbf{statistic}.
\end{definition}

\subsection{Sample Mean and Sample Variance}
\begin{definition}[Sample Mean and Sample Variance]
    \normalfont
    \begin{enumerate}
        \item The \textbf{sample mean} is $\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i$;
        \item The \textbf{sample variance} is $S^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar{X})^2=\frac{1}{n-1}(\sum_{i=1}^n X_i^2 - n\bar{X}^2)$
    \end{enumerate}
\end{definition}

\begin{note}
    We use "$X_i\sim \text{i.i.d}(\mu,\sigma^2)$" to denote a random sample from a distribution with mean $\mu$ and variance $\sigma^2$.
\end{note}

\begin{theorem}[$\mathbb{E}(\bar{X}), \text{Var}(\bar{X}), \mathbb{E}(S^2)$]
    Suppose $X_1,...,X_n$ is a random sample from a distribution with mean $\mu$ and variance $\sigma^2$ (denoted by $X_i\sim \text{i.i.d}(\mu,\sigma^2)$). Then,
    \begin{enumerate}[(a).]
        \item $\mathbb{E}(\bar{X})=\mu$;
        \item $\text{Var}(\bar{X})=\frac{\sigma^2}{n}$;
        \item $\mathbb{E}(S^2)=\sigma^2$.
    \end{enumerate}
\end{theorem}


\subsection{Distributional Properties}
\begin{theorem}
    If $X_i\sim \text{i.i.d. } N(\mu,\sigma^2)$, then
    \begin{enumerate}[(a).]
        \item $\bar{X}\sim \mathcal{N}(\mu,\frac{\sigma^2}{n})$
        \item $\frac{n-1}{\sigma^2}S^2\sim \chi^2_{n-1}$
        \item $\bar{X}\perp S^2$
    \end{enumerate}
\end{theorem}

\begin{theorem}["Asymptotics"]
    If $X_i\sim \text{i.i.d. } (\mu,\sigma^2)$ and if $n$ is "large", then
    \begin{enumerate}[(a).]
        \item $\bar{X}\sim \mathcal{N}(\mu,\frac{\sigma^2}{n})$ (converges in distribution) by CLT \ref{CLT};
        \item $S^2=\sigma^2$ by LLN;
    \end{enumerate}
\end{theorem}

\subsection{Order Statistics}
\begin{definition}[Order Statistics]
    \normalfont
    If $X_1,...,X_n$ is a random sample, then the \textbf{characteristics} are the sample values placed in ascending order.
    \underline{Notation:}
    \begin{equation}
        \begin{aligned}
            X_{(1)}\leq X_{(2)}\leq ... \leq X_{(n)}
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}

\begin{proposition}[Distribution of $X_{n}=\max_{i=1,...,n}X_i$]
    If $X_1,...,X_n$ is a random sample form a distribution with cdf $F$ (denoted by "$X_i\sim \text{i.i.d. } F$"), then
    \begin{equation}
        \begin{aligned}
            F_{X_{(n)}}(x)=P(X_{(n)}\leq x)=F^n(x)
        \end{aligned}
        \nonumber
    \end{equation}
\end{proposition}
\begin{proposition}[cdf and pdf]
    More generally,
    \begin{equation}
        \begin{aligned}
            F_{X_{(r)}}(x)&=\sum _{j=r}^{n}{\binom {n}{j}}[F_{X}(x)]^{j}[1-F_{X}(x)]^{n-j}\\
            f_{X_{(r)}}(x)&={\frac {n!}{(r-1)!(n-r)!}}f_{X}(x)[F_{X}(x)]^{r-1}[1-F_{X}(x)]^{n-r}
        \end{aligned}
        \nonumber
    \end{equation}
\end{proposition}
\begin{example}\quad
    \begin{enumerate}
        \item \textbf{Order statistics sampled from a uniform distribution on unit interval ($\text{Unif}[0,1]$):} Consider a random sample $U_1,...,U_n$ from the standard uniform distribution. Then,
        \begin{equation}
            \begin{aligned}
                f_{X_{(k)}}(x)&={\frac {n!}{(k-1)!(n-k)!}}u^{k-1}(1-u)^{n-k}
            \end{aligned}
            \nonumber
        \end{equation}
        The $k^{th}$ order statistic of the uniform distribution is a beta-distributed random variable. $$U_{(k)}\sim \text{Beta}(k,n+1-k)$$
        which has mean $\mathbb{E}[U_{(k)}]=\frac{k}{n+1}$.
        \item \textbf{The joint distribution of the order statistics of the uniform distribution on unit interval ($\text{Unif}[0,1]$):}
        Similarly, for $i < j$, the joint probability density function of the two order statistics $U_{(i)} < U_{(j)}$ can be shown to be
        \begin{equation}
            \begin{aligned}
                {\displaystyle f_{U_{(i)},U_{(j)}}(u,v)=n!{u^{i-1} \over (i-1)!}{(v-u)^{j-i-1} \over (j-i-1)!}{(1-v)^{n-j} \over (n-j)!}}
            \end{aligned}
            \nonumber
        \end{equation}
        The joint density of the $n$ order statistics turns out to be constant:
        \begin{equation}
            \begin{aligned}
                {\displaystyle f_{U_{(1)},U_{(2)},\ldots ,U_{(n)}}(u_{1},u_{2},\ldots ,u_{n})=n!}
            \end{aligned}
            \nonumber
        \end{equation}
        For $n\geq k>j\geq 1$, $U_{(k)}-U_{(j)}$ also has a beta distribution:
        \begin{equation}
            \begin{aligned}
                {\displaystyle U_{(k)}-U_{(j)}\sim \operatorname {Beta} (k-j,n-(k-j)+1)}
            \end{aligned}
            \nonumber
        \end{equation}
        which has mean $\mathbb{E}[U_{(k)}-U_{(j)}]=\frac{k-j}{n+1}$
    \end{enumerate}
\end{example}


\section{Basic Ideas of  Statistics}
\subsection{Fundamental Assumption}
An statistical model is a family of probability distributions over the data.

In statistics, we define \textit{data} be a vector $x=(x_1,...,x_n)'\in\Omega$ of numbers, where $x_i\in \mathbb{R}^d$. $x$ is the realization of a random vector $X=(X_1,...,X_n)'$. The $X$ follows a distribution $P_0$, which is the \textit{True Probability Generating Data (DGP)}. If $P_0$ is i.i.d., we have $P_0(X)=P_0(x_1)P(x_2)\cdots P_0(x_n)$.

\begin{definition}[Model]
    \normalfont
    A model $P\subseteq \{\text{Probabilities over }\Omega\}$ and a i.i.d. model $P\subseteq \{\text{Probabilities over }\mathbb{R}^d\}$.
\end{definition}

\begin{definition}[Well-Specified Model]
    \normalfont
    A model is \textbf{well-specified} if $P\ni P_0$.
\end{definition}

\subsection{Parametric Model}
\begin{definition}[Parametric Model]
    \normalfont
    A \underline{non-parametric} model $\bar{P}\cong \{\text{Probabilities over }\mathbb{R}^d\}$.\\
    A \underline{parametric} model $P=\{P_\theta:\theta\in\Theta\subseteq \mathbb{R}^v\}$.\\
    A \underline{semi-parametric} model: not parametric / non-parametric.
\end{definition}

\begin{example}
    \begin{enumerate}
        \item \underline{Parametric} model: $P=\{\Phi(\theta,1):\theta\in \mathbb{R}\}$, where $\Phi$ is the Gaussian c.d.f.
        \item Regression Models. $Z:=(Y,X)$. $P$ belongs to the model iff $\mathbb{E}_P[y^2]<\infty$ and $\mathbb{E}_P[XX^T]$ is non-singular and finite. The model gives $\mathbb{E}_P[Y|X]=h(X)$.
        \begin{enumerate}[(A).]
            \item \underline{Semi-parametric} model: $h\in\{\text{linear functions}\}$ i.e., $h(X)=\beta^TX$ for some $\beta\in \mathbb{R}^d$.
            \item \underline{Non-parametric} model: $h\in\{f:\mathbb{E}_p[f(x)^2]<\infty\}$.
        \end{enumerate}
    \end{enumerate}
\end{example}

\subsection{Parameter}
\textbf{Potential Outcome Model}: $Z:=(Y,D,X)$, where $Y$ is the outcome, $D\in\{0,1\}$ is the treatment, and $X$ is the covariates. $P$ belongs to the model iff $(y_{(0)},y_{(1)})$ represents the potential outcome given different treatment $D\in\{0,1\}$, $y=Dy_{(1)}+(1-D)y_{(0)}$, and we study $e(x):=P(D=1|x)$.

Average Treatment Effect (ATE) is given by $\text{ATE}_{P_0}:=\mathbb{E}_{P_0}[y_{(1)}-y_{(0)}]$, where $P_0$ is the DGP. It is impossible to estimate the ATE even if we have enough data. We need to link it to something we can estimate.

\begin{definition}[Parameter]
    \normalfont
    A parameter is a ``feature'' of $P_0$: $v(P), P\in \mathcal{P}$. Specifically, $v(P_0)$ is the true parameter of the DGP.
\end{definition}

\begin{example}
    Let's consider a linear regression model, $\mathbb{E}_{P_0}[Y|X]=\beta^T_0 X$. We solve $\beta$ by $\min_\beta \mathbb{E}_{P_0}[(y-\beta^Tx)^2]$. The F.O.C. gives $\mathbb{E}_{P_0}[YX^T]=\beta^T \mathbb{E}_{P_0}[XX^T]$. $\beta_0$ solves this.\\
    \underline{Linear instrumental Variable Model}: $\mathbb{E}_P[(Y-\beta^T_0X)|W]=0$, where $W$ is the instrumental variable. Look at $\mathbb{E}_{P_0}[(y-\beta^TX)W]=0$. Suppose $\hat{\beta}\neq\beta_0$, $\mathbb{E}_{P_0}[(\hat{\beta}-\beta_0)^TXW]=\underbrace{(\hat{\beta}-\beta_0)^T}_{1\times m}\underbrace{\mathbb{E}_{P_0}[XW]}_{m\times k}=0$, which holds iff $\hat{\beta}=\beta_0$ given $\mathbb{E}_{P_0}[XW]$ has full rank.
\end{example}

To identify the ATE, we give two assumptions:
\begin{enumerate}
    \item A1 (overlap): $e(X):=P(D=1|X)\in(0,1)$
    \item A2 (Unconfoundedness): $(y(0),y(1))\perp D|X$.
\end{enumerate}

$\text{ATE}=\mathbb{E}[y(1)-y(0)]=\mathbb{E}[\mathbb{E}[y(1)|X]-\mathbb{E}[y(0)|X]]$. $\mathbb{E}[y|D=1,X]=\mathbb{E}[y(1)|D=1,X]$. Given Assumption A1: $y(1)\perp D|X$, $\mathbb{E}[y|D=1,X]=\mathbb{E}[y(1)|D=1,X]=\mathbb{E}[y(1)|X]$.

\begin{example}
    For a parameter $\theta(P_0)$, we have an estimate $\hat{\theta}_m$ (with sample size $m$), which has C.D.F. $v(P_0)$. For all $t\in \mathbb{R}$, the C.D.F. is given by
    \begin{equation}
        \begin{aligned}
            v(P_0)(t)=\text{Pr}_{P_0}(\hat{\theta}_m-\theta(P_0)\leq t)
        \end{aligned}
        \nonumber
    \end{equation}
\end{example}


\subsection{Plug-In Estimation}
For a model $P$, we have ``identification'' $v(P_0):=\theta_0$. How to estimate unknown $P_0$?

\begin{definition}[Empirical Probability/CDF]
    \normalfont
    Empirical probability/CDF: $P_m(A)=\frac{1}{m}\sum_{i=1}^m \mathbf{1}\{Z_i\in A\}$. By the LLN, $P_m(A) \stackrel{P_0}{\longrightarrow} P_0(A)$.
\end{definition}

\begin{definition}[Plug-in estimator]
    \normalfont
    A \textbf{Plug-in estimator} is an estimator based on the empirical CDF, which is given by
    \begin{equation}
        \begin{aligned}
            \hat{\theta}_m=v(P_m)
        \end{aligned}
        \nonumber
    \end{equation}
    \underbar{Note:} The domain of $v$ is $\mathcal{P}$. Is $v(P_m)$ well-defined? It might be $P_m\notin \mathcal{P}$.
\end{definition}

\begin{example}
    \begin{enumerate}
        \item $\mathcal{P}=\{\text{all pdf with finite first moments}\}$. $v(P_0)=\mathbb{E}_{P_0}[Z]$, $v(P_m)=\frac{1}{m}\sum_{i=1}^m Z_i$.
        \item $\mathcal{P}$ is the set of linear regression models. $v(P_0)=\argmin_b \mathbb{E}_{P_0}[(Y-b^TX)^2]=\mathbb{E}_{P_0}[XX^T]^{-1}\mathbb{E}_{P_0}[XY]$,
        \begin{equation}
            \begin{aligned}
                v(P_m)=\mathbb{E}_{P_m}[(Y-b^TX)^2]=\argmin_b\frac{1}{m}\sum_{i=1}^m\left(Y_i-b^TX_i\right)^2=\left(\frac{1}{m}\sum_{i=1}^mX_iX_i^T\right)^{-1}\left(\frac{1}{m}\sum_{i=1}^mX_iY_i\right)
            \end{aligned}
            \nonumber
        \end{equation}
        where $v(P_m)$ is OLS.
        \item \textbf{GMM}. $\forall P\in \mathcal{P}: \mathbb{E}_P[g(Z,v(p))]=0$, where $g$ is a known moment function. $$v(P_0)=\argmin_\theta \mathbb{E}_{P_0}[g(Z,\theta)]^TW\mathbb{E}_{P_0}[g(Z,\theta)]$$
        where $W$ is a weighted matrix.
        \begin{equation}
            \begin{aligned}
                v(P_m)=\argmin_\theta \left(\frac{1}{m}\sum_{i=1}^m g(Z_i,\theta)\right)^TW\left(\frac{1}{m}\sum_{i=1}^m g(Z_i,\theta)\right)
            \end{aligned}
            \nonumber
        \end{equation}
        The $v(P_m)$ is the \textbf{Gaussian Estimator}.
        \item (When it doesn't work.) For the linear regression case, $v(P_m)=\underbrace{\left(\frac{1}{m}\sum_{i=1}^mX_iX_i^T\right)^{-1}}_\text{well-defined?}\left(\frac{1}{m}\sum_{i=1}^mX_iY_i\right)$. If the $\#$ of Covariates $> m$, the estimator is not well-defined.
        \item (When it doesn't work.) $\mathcal{P}$ is the potential outcome model. $\text{ATE}=v(P_0)=\mathbb{E}_{P_0}[\mu_1(x)-\mu_0(x)]$ where $\mu_d(x):=\mathbb{E}_{P_0}[y|D=d,x],d=0,1$.
        \begin{equation}
            \begin{aligned}
                v(P_m)=\frac{1}{m}\sum_{i=1}^m\left(\underbrace{\mathbb{E}_{P_m}[y|D=1,X_i]}_\text{well-defined?}-\mathbb{E}_{P_m}[y|D=0,X_i]\right)
            \end{aligned}
            \nonumber
        \end{equation}
        $\mathbb{E}_{P_m}[y|D=d,x]$ is ``too complex'' to define, (consider the example that $x$ is continuous).
    \end{enumerate}
\end{example}
\underline{What is the solution when the Plug-in estimation doesn't work?} 1. Propose a functional form restriction $\mu_d$. 2. ``Regularization''.


\subsection{Bootstrap}
Let $v(P_0): \text{ CDF of }\theta(P_m)-\theta(P_0)=:C(P_m,P_0)$
\begin{equation}
    \begin{aligned}
        v(P_0)(t)=\text{Pr}_{P_0}\left(C(P_m,P_0)\leq t\right),\forall t
    \end{aligned}
    \nonumber
\end{equation}
Here, the data $\{Z_i\}_i$ is generated from $P_0$, which forms $P_m$.\\ Sometimes, we may study $v_A(P_0)(t)=\text{Pr}_{P_0}\left(\underbrace{\frac{C(P_m,P_0)}{\sqrt{\text{Var}_{P_0}(\theta(P_m))}}}_{T(P_m,P_0)}\leq t\right),\forall t$ instead.

The Plug-in estimator $v(P_m)$ is a.t.a. the Bootstrap estimator. Now, we generate new data i.i.d. from $P_m$, $\{Z_i^*\}_i$, which forms $P_m^*$.
\begin{equation}
    \begin{aligned}
        v(P_m)=\text{Pr}_{P_m}\left(\theta(P_m^*)-\theta(P_m)\leq t\right)
    \end{aligned}
    \nonumber
\end{equation}
\paragraph*{Computation of $v(P_m)$}
\begin{enumerate}[(1).]
    \item Draw $\{Z_i^*\}_i$ from $P_m$ and forms $P_m^*$.
    \item Based on the new $P_m^*$, compute $C^{(b)}(P_m^*,P_m)=\theta(P_m^*)-\theta(P_m)$.
    \item Repeat (1) and (2):
    \begin{equation}
        \begin{aligned}
            \frac{1}{B}\sum_{b=1}^B \mathbf{1}\{C^{(b)}(P_m^*,P_m)\leq t\}\stackrel{B \rightarrow \infty}{\longrightarrow}v(P_m)(t)
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}


\begin{example}
    $\theta(P_0)=\mathbb{E}_{P_0}(Z)$, $\theta(P_m)=\bar{Z}_m=\frac{1}{m}\sum_{i=1}^m Z_i$. $v(P_0)(t)=\text{Pr}_{P_0}\left(\frac{1}{m}\sum_{i=1}^m (Z_i-\mathbb{E}_{P_0}(Z))\leq t\right)$. The Bootstrap estimator is given by
    \begin{equation}
        \begin{aligned}
            v(P_m)=\text{Pr}_{P_0}\left(\frac{1}{m}\sum_{i=1}^m (Z_i^*-\underbrace{\mathbb{E}_{P_m}(Z)}_{\bar{Z}_m})\leq t\right)
        \end{aligned}
        \nonumber
    \end{equation}
    or
    \begin{equation}
        \begin{aligned}
            v_A(P_m)=\text{Pr}_{P_0}\left(\sqrt{m}\frac{\frac{1}{m}\sum_{i=1}^m (Z_i^*-\underbrace{\mathbb{E}_{P_m}(Z)}_{\bar{Z}_m})}{\sqrt{\text{Var}_{P_0}(\theta(P_m))}}\leq t\right)
        \end{aligned}
        \nonumber
    \end{equation}
    where $Z_i^*\sim_{i.i.d.} P_m$, $Z_i^*\in\{Z_1,...,Z_m\},\forall i\in\{1,...,m\}$. It is equivalent to give a weight to each $Z_i$, $\sum_{i=1}^m Z_i^*=\sum_{i=1}^m W_{i,m} Z_i$, where $$\left(W_{1,m},...,W_{m,m}\right)\sim\text{Multinomial}\left(\frac{1}{m},....,\frac{1}{m},m\right),\ W_{i,m}\in\{0,1,...,m\}$$
    Based on this, the Bootstrap estimator can be rewritten as
    \begin{equation}
        \begin{aligned}
            v(P_m)=\text{Pr}_{P_0}\left(\frac{1}{m}\sum_{i=1}^m (W_{i,m}-1)Z_i\leq t\right)
        \end{aligned}
        \nonumber
    \end{equation}
    (Other Bootstrap procedure, $W_{i,m}$ is not restricted to be multinomial, $\mathbb{E}[W_{i,m}]=1$.)

    For the $v_A(P_0)$, $\text{Var}_{P_0}(\theta(P_m))=\frac{1}{m}\sigma^2_{P_0}(Z)$ and $\text{Var}_{P_m}(\theta(P_m^*))=\frac{1}{m}\sigma^2_{P_m}(Z)=\frac{1}{m}\underbrace{S^2_Z}_\text{sample variance of $Z$}$
    So,
    \begin{equation}
        \begin{aligned}
            v_A(P_m)=\text{Pr}_{P_0}\left(\sqrt{m}\frac{\bar{Z}_m-\mathbb{E}_{P_0}[Z]}{S_Z}\leq t\right)
        \end{aligned}
        \nonumber
    \end{equation}
\end{example}


\paragraph*{Consistency}
The consistency of the estimator is $v(P_m)(t)$ is given by
\begin{equation}
    \begin{aligned}
        \sup_t|v(P_m)(t)-v(P_0)(t)|=\underbrace{0_{P_0}(1)}_\text{Goes to zero in probability}
    \end{aligned}
    \nonumber
\end{equation}


\subsection{}
\textbf{\underline{Objective}:} Using $x$ to give (data-based) answers to questions about the distribution of $X$, i.e., $P_0$.

\textbf{\underline{Probability vs. Statistics}:}
\begin{enumerate}[$\circ$]
    \item Probability: Distribution known, outcome unknown;
    \item Statistics: Distribution unknown, outcome known.
\end{enumerate}

\textbf{\underline{Setting:}} $X_1,...,X_n$ is a random sample from a discrete/continuous distribution with pmf/pdf $f(\cdot\mid \theta)$, where $\theta\in\Theta$ is unknown.

\textbf{\underbar{Types of Statistical Inference}:}
\begin{enumerate}[$\circ$]
    \item Point estimation $\Rightarrow$ "What is $\theta$?";
    \item Hypothesis testing $\Rightarrow$ "Is $\theta=\theta_0$?";
    \item Interval estimation $\Rightarrow$ "Which values of $\theta$ are 'plausible'?".
\end{enumerate}

\begin{example}
    \underline{Examples of Statistical Models}
    \begin{enumerate}[(1).]
        \item $x_i\sim \text{i.i.d. } \text{Bernoulli}(p)$, where $p$ is unknown.
        \item $x_i\sim \text{i.i.d. } U(0,\theta)$, where $\theta>0$ is unknown.
        \item $x_i\sim \text{i.i.d. } N(\mu,\sigma^2)$, where $\mu\in \mathbb{R}$ and $\sigma^2>0$ are unknown.
    \end{enumerate}
\end{example}


\section{Point Estimation}
Suppose $X_1,...,X_n$ is a random sample from a discrete/continuous distribution with pmf/pdf $f(\cdot\mid \theta)$, where $\theta\in\Theta$ is unknown.

\begin{definition}[Point Estimator]
    \normalfont
    A \textbf{point estimator} (of $\theta$) is a function of $(X_1,...,X_n)$.\\
    \underline{Notation}: $\hat{\theta}=\hat{\theta}(X_1,...,X_n)$.
\end{definition}

\underline{Agenda}
\begin{enumerate}[(1).]
    \item Constructing point estimators
    \begin{enumerate}[$\circ$]
        \item Method of moments;
        \item Maximum likelihood.
    \end{enumerate}
    \item Comparing estimators
    \begin{enumerate}[$\circ$]
        \item Pairwise comparisons;
        \item Finding 'optimal' estimators.
    \end{enumerate}
\end{enumerate}


\subsection{Method of Moments (MM)}
\begin{definition}[Method of Moments in $\mathbb{R}^1$]
    \normalfont
    Suppose $\Theta\subseteq \mathbb{R}^1$. A \textbf{method of moments} estimator $\hat{\theta}_{MM}$ solves
    \begin{equation}
        \begin{aligned}
            \mu(\hat{\theta}_{MM})=\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i
        \end{aligned}
        \nonumber
    \end{equation}
    where $\mu: \Theta \rightarrow \mathbb{R}$ is given by
    \begin{equation}
        \begin{aligned}
            \mu(\theta)=\left\{\begin{matrix}
                \sum_{x\in \mathbb{R}}x f(x\mid \theta),& \text{ if $X_i$ are discrete}\\
                \int_{-\infty}^\infty x f(x\mid \theta) dx,& \text{ if $X_i$ are continuous}
            \end{matrix}\right.
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}
\begin{remark}
    Existence of $\mu(\cdot)$ is assumed;
    Existence (and uniqueness) of $\hat{\theta}_{MM}$ is assumed.
\end{remark}

\begin{example}\quad
    \begin{enumerate}
        \item Suppose $X_i\sim \text{i.i.d. Ber}(p)$ where $p\in[0,1]$ is unknown. The \underline{moment function} is
        \begin{equation}
            \begin{aligned}
                \mu(p)=p
            \end{aligned}
            \nonumber
        \end{equation}
        Then, the \underline{estimator} is
        \begin{equation}
            \begin{aligned}
                \hat{p}_{MM}=\mu(\hat{p}_{MM})=\bar{X}
            \end{aligned}
            \nonumber
        \end{equation}
        \begin{remark}
            $\hat{p}_{MM}=\bar{X}$ is the 'best' estimator of $p$.
        \end{remark}
        \item Suppose $X_i\sim \text{i.i.d.}U(0,\theta)$ where $\theta>0$ is unknown.\\
        \begin{remark}
            Non-regular statistical model: parameter dependent support, where $\text{supp}X=[0,\theta]$.
        \end{remark}
        The \underline{moment function} is
        \begin{equation}
            \begin{aligned}
                \mu(\theta)=\frac{\theta}{2}
            \end{aligned}
            \nonumber
        \end{equation}
        Then, the \underline{estimator} is
        \begin{equation}
            \begin{aligned}
                \hat{\theta}_{MM}=2\mu(\hat{\theta}_{MM})=2\bar{X}
            \end{aligned}
            \nonumber
        \end{equation}
        \begin{remark}
            $\hat{\theta}_{MM}$ is not a very good estimator of $\theta$. Concern $X_i>\hat{\theta}_{MM}$ could happen. So, $\max\{\hat{\theta}_{MM},X_{(n)}\}$ can be better.
        \end{remark}
    \end{enumerate}
\end{example}

\begin{definition}[Method of Moments in $\mathbb{R}^k$]
    \normalfont
    Suppose $\Theta\subseteq \mathbb{R}^k$. A \textbf{method of moments} estimator $\hat{\theta}_{MM}$ solves
    \begin{equation}
        \begin{aligned}
            \mu'_j(\hat{\theta}_{MM})=\frac{1}{n}\sum_{i=1}^n X_i^j,\quad (j=1,...,k)
        \end{aligned}
        \nonumber
    \end{equation}
    where $\mu'_j: \Theta \rightarrow \mathbb{R}$ is given by
    \begin{equation}
        \begin{aligned}
            \mu'_j(\theta)=\left\{\begin{matrix}
                \sum_{x\in \mathbb{R}}x^j f(x\mid \theta),& \text{ if $X_i$ are discrete}\\
                \int_{-\infty}^\infty x^j f(x\mid \theta) dx,& \text{ if $X_i$ are continuous}
            \end{matrix}\right.
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}
\begin{example}\quad\\
    Suppose $X_i\sim \text{i.i.d.}N(\mu,\sigma^2)$ where $\mu\in \mathbb{R}$ and $\sigma^2>0$ are unknown. The \underline{moment function} is
    \begin{equation}
        \begin{aligned}
            \mu'_1(\mu,\sigma^2)&=\mu\\
            \mu'_2(\mu,\sigma^2)&=\mu^2+\sigma^2
        \end{aligned}
        \nonumber
    \end{equation}
    Then, the \underline{estimator} is
    \begin{equation}
        \begin{aligned}
            \mu'_1(\hat{\mu}_{MM},\hat{\sigma}^2_{MM})&=\hat{\mu}_{MM}=\frac{1}{n}\sum_{i=1}^n X_i\\
            \mu'_2(\hat{\mu}_{MM},\hat{\sigma}^2_{MM})&=\hat{\mu}_{MM}+\hat{\sigma}^2_{MM}=\frac{1}{n}\sum_{i=1}^n X_i^2\\
            \Rightarrow \hat{\mu}_{MM}&=\bar{X}\\
            \hat{\sigma}^2_{MM}&=\frac{1}{n}\sum_{i=1}^n (X_i-\bar{X})^2
        \end{aligned}
        \nonumber
    \end{equation}
    \begin{remark}
        $\bar{X}$ is the 'best' estimator of $\mu$; An alternative better estimator of $\sigma^2$ is $\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar{X})^2$.
    \end{remark}
\end{example}


\subsection{Maximum Likelihood (ML)}
\begin{definition}[Maximum Likelihood]
    \normalfont
    A \textbf{maximum likelihood estimator} $\hat{\theta}_{ML}$ solves
    \begin{equation}
        \begin{aligned}
            L(\hat{\theta}_{ML}\mid X_1,...,X_n)=\max_{\theta\in\Theta} L(\theta\mid X_1,...,X_n)
        \end{aligned}
        \nonumber
    \end{equation}
    where $L(\cdot\mid X_1,...,X_n):\Theta \rightarrow \mathbb{R}_+$ is given by $$L(\theta\mid X_1,...,X_n)=\prod_{i=1}^n f_{X_i}(X_i\mid\theta),\ \theta\in\Theta$$
\end{definition}
\begin{remark}
    $L(\cdot\mid X_1,...,X_n)$ is called the \underline{likelihood} function.
\end{remark}
\begin{definition}[Log-Likelihood]
    \normalfont
    The \textbf{log-likelihood} function is
    \begin{equation}
        \begin{aligned}
            l(\theta\mid X_1,...,X_n)=\log L(\theta\mid X_1,...,X_n)=\sum_{i=1}^n \log f_{X_i}(X_i\mid\theta),\ \theta\in\Theta
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}

\begin{example}\quad
\begin{enumerate}
    \item Suppose $X_i\sim \text{i.i.d. Ber}(p)$ where $p\in[0,1]$ is unknown. The \underline{marginal pmf} is
    \begin{equation}
        \begin{aligned}
            f(x\mid p)=\left\{\begin{matrix}
                p,&x=1\\
                1-p,&x=0\\
                0,&\text{ otherwise}
            \end{matrix}\right.=p^x(1-p)^{1-x}\mathbf{1}_{\{x\in\{0,1\}\}}
        \end{aligned}
        \nonumber
    \end{equation}
    Then, the \underline{likelihood function} is
    \begin{equation}
        \begin{aligned}
            L(p\mid X_1,...,X_n)&=\prod_{i=1}^n\left\{ p^{X_i}(1-p)^{1-X_i}\underbrace{\mathbf{1}_{\{X_i\in\{0,1\}\}}}_{=1}\right\}\\
            &=p^{\sum_{i=1}^n X_i}(1-p)^{n-\sum_{i=1}^n X_i}, \ p\in[0,1]
        \end{aligned}
        \nonumber
    \end{equation}
    and the \underline{log-likelihood function} is
    \begin{equation}
        \begin{aligned}
            l(p\mid X_1,...,X_n)&=(\sum_{i=1}^n X_i)\log p + (n-\sum_{i=1}^n X_i)\log (1-p),\ p\in (0,1)
        \end{aligned}
        \nonumber
    \end{equation}
    \underline{Maximization:}
    \begin{enumerate}
        \item Suppose $0<\sum_{i=1}^n X_i<n$, we can give the first-order condition:
        \begin{equation}
            \begin{aligned}
                \frac{\partial l(p\mid X_1,...,X_n)}{\partial p}\big|_{p=\hat{p}_{ML}}=\frac{\sum_{i=1}^n X_i}{\hat{p}_{ML}}-\frac{n-\sum_{i=1}^n X_i}{n-\hat{p}_{ML}}=0\\
                \Rightarrow \hat{p}_{ML}=\frac{\sum_{i=1}^n X_i}{n}=\bar{X}
            \end{aligned}
            \nonumber
        \end{equation}
        \item Suppose $\sum_{i=1}^n X_i=0$, then
        \begin{equation}
            \begin{aligned}
                l(p\mid X_1,...,X_n)=n\log (1-p),\ p\in [0,1)
                \Rightarrow \hat{p}_{ML}=0
            \end{aligned}
            \nonumber
        \end{equation}
        \item Suppose $\sum_{i=1}^n X_i=n$, then
        \begin{equation}
            \begin{aligned}
                l(p\mid X_1,...,X_n)=n\log p,\ p\in (0,1]
                \Rightarrow \hat{p}_{ML}=1
            \end{aligned}
            \nonumber
        \end{equation}
    \end{enumerate}
    All in all, $$\hat{p}_{ML}=\bar{X}$$
    \begin{remark}
        $\hat{p}_{ML}=\bar{X}=\hat{p}_{MM}$ is the 'best' estimator of $p$.
    \end{remark}
    \item Suppose $X_i\sim \text{i.i.d. } U[0,\theta]$ where $\theta>0$ is unknown. The \underline{marginal pdf} is
    \begin{equation}
        \begin{aligned}
            f(x\mid \theta)=\left\{\begin{matrix}
                \frac{1}{\theta},&x\in[0,\theta]\\
                0,&\text{otherwise}
            \end{matrix}\right.=\frac{1}{\theta}\mathbf{1}_{\{x\in[0,\theta]\}}
        \end{aligned}
        \nonumber
    \end{equation}
    and the \underline{likelihood function} is
    \begin{equation}
        \begin{aligned}
            L(\theta\mid X_1,...,X_n)
            =\prod_{i=1}^n\left\{ \frac{1}{\theta}\mathbf{1}_{\{x\in[0,\theta]\}}\right\}
            =\left\{\begin{matrix}
                \frac{1}{\theta^n},&\theta\geq X_{(n)}\\
                0,&\text{otherwise}
            \end{matrix}\right.\\
            \Rightarrow \hat{\theta}_{ML}=X_{(n)}
        \end{aligned}
        \nonumber
    \end{equation}
    \begin{remark}
        $\hat{\theta}_{ML}=X_{(n)}\neq 2\bar{X}=\hat{\theta}_{MM}$; $\hat{\theta}_{ML}<X_i$ can't occur, which is good news; $\hat{\theta}_{ML}\leq \theta$ (low) must occur, which is bad news.
    \end{remark}
    \item Suppose $X_i\sim \text{i.i.d.}N(\mu,\sigma^2)$ where $\mu\in \mathbb{R}$ and $\sigma^2>0$ are unknown. Then,
    \begin{equation}
        \begin{aligned}
            \hat{\mu}_{ML}=\hat{\mu}_{MM}=\bar{X},\ \hat{\sigma}^2_{ML}=\hat{\sigma}^2_{MM}=\frac{1}{n}\sum_{i=1}^n (X_i-\bar{X})^2
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}
\end{example}

\subsection{Comparing Estimators: Mean Squared Error}
\textbf{\underline{General Approach}}
\begin{enumerate}[$\circ$]
    \item Statistical Decision Theory
    \subitem \underline{Leading Special Case}: Mean Squared Error.
\end{enumerate}

\begin{definition}[Mean Squared Error]
    \normalfont
    The \textbf{mean squared error} (MSE) of one estimator $\hat{\theta}$ of $\theta$ is defined as
    \begin{equation}
        \begin{aligned}
            \text{MSE}_\theta(\hat{\theta})=\mathbb{E}_\theta[(\hat{\theta}-\theta)^2],\ \theta\in \Theta\subseteq \mathbb{R}
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}

\begin{definition}[Bias]
    \normalfont
    The \textbf{bias} of $\hat{\theta}$ is (the function of $\theta$) given by
    \begin{equation}
        \begin{aligned}
            \text{Bias}_\theta(\hat{\theta})=\mathbb{E}_\theta(\hat{\theta})-\theta,\ \theta\in\Theta
        \end{aligned}
        \nonumber
    \end{equation}
    $\hat{\theta}$ is \textbf{unbiased} iff $\text{Bias}_\theta(\hat{\theta})=0$ ($\forall \theta\in \Theta$)
\end{definition}
\textbf{\underline{Decomposition:}}
\begin{equation}
    \begin{aligned}
        \text{MSE}_\theta(\hat{\theta})=\text{Bias}_\theta(\hat{\theta})^2+\text{Var}_\theta(\hat{\theta})
    \end{aligned}
    \nonumber
\end{equation}
which is given by $\mathbb{E}[X^2]=\mathbb{E}[X]^2+\text{Var}(X)$. Hence, if $\hat{\theta}$ is unbiased ($\text{Bias}_\theta(\hat{\theta})=0$), $\text{MSE}_\theta(\hat{\theta})=\text{Var}_\theta(\hat{\theta})$.

\begin{definition}[Uniform Minimum Variance Unbiased (UMVU)]
    \normalfont
    An unbiased estimator $\hat{\theta}$ is a \textbf{uniform minimum variance unbiased (UMVU)} estimator (of $\theta$) iff
    \begin{equation}
        \begin{aligned}
            \text{MSE}_\theta(\hat{\theta})=\text{Var}_\theta(\hat{\theta})\leq \text{Var}_\theta(\tilde{\theta})=\text{MSE}_\theta(\tilde{\theta})
        \end{aligned}
        \nonumber
    \end{equation}
    whenever $\tilde{\theta}$ is an unbiased estimator of $\theta$.
\end{definition}
\begin{remark}
    UMVU estimators often exist; UMVU estimators are based on \underline{sufficient} statistics.
\end{remark}

\subsection{Sufficient Statistic}
\begin{definition}[Sufficient Statistic]
    \normalfont
    A statistic $T=T(X_1,...,X_n)$ is \textbf{sufficient} iff the conditional distribution of $(X_1,...,X_n)$ given $T$, $(X_1,...,X_n)|T$, doesn't depend on $\theta$.
    $$f_X(x\mid T(X_1,...,X_n)=t;\theta)=f_X(x\mid T(X_1,...,X_n)=t),\ \forall x$$
    That is, the mutual information between $\theta$ and $T(X_1,...,X_n)$ equals the mutual information between $\theta$ and $\{X_1,...,X_n\}$,
    \begin{equation}
        \begin{aligned}
            I(\theta;T(X_1,...,X_n))=I(\theta;\{X_1,...,X_n\})
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}

\begin{theorem}[Rao-Blackwell Theorem]
    \normalfont
    Suppose $\tilde{\theta}$ is an unbiased estimator of $\theta$ and suppose $T$ is sufficient (for $\theta$). Then,
    \begin{enumerate}[(a).]
        \item $\hat{\theta}=\mathbb{E}[\tilde{\theta}|T]$ is an unbiased estimator of $\theta$.
        \item $\text{Var}_\theta(\hat{\theta})\leq \text{Var}_\theta(\tilde{\theta})$, $\forall \theta\in\Theta$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}[(a).]
        \item \underline{Estimator:} $\hat{\theta}=\mathbb{E}[\tilde{\theta}\mid T]$ doesn't depend on $\theta$ because $T$ is sufficient. By the Law of Iterative Expectation, we have
        \begin{equation}
            \begin{aligned}
                \mathbb{E}_\theta(\hat{\theta})=\mathbb{E}_\theta[\mathbb{E}[\tilde{\theta}\mid T]]=\mathbb{E}_\theta[\tilde{\theta}]=\theta
            \end{aligned}
            \nonumber
        \end{equation}
        \item \underline{Variance Reduction:} By the Law of Total Variance
        \begin{equation}
            \begin{aligned}
                \text{Var}(\hat{\theta})=\text{Var}_\theta[\mathbb{E}[\tilde{\theta}\mid T]]\leq \text{Var}_\theta(\tilde{\theta}),\ \forall \theta\in \Theta
            \end{aligned}
            \nonumber
        \end{equation}
        with strict inequality unless $\text{Var}(\hat{\theta}|T)=0$ (which also makes $\hat{\theta}=\tilde{\theta}$).
    \end{enumerate}
\end{proof}


\textbf{\underline{Finding sufficient statistics}}
\begin{enumerate}[$\circ$]
    \item Apply "definition";
    \item Apply \underline{factorization criterion}.
\end{enumerate}

\begin{proposition}[Fisher-Neyman Factorization Criterion]
    A statistic $T=T(X_1,...,X_n)$ is sufficient \underline{if and only if} $\exists g(\cdot|\cdot)$ and $h(\cdot)$ such that
    \begin{equation}
        \begin{aligned}
            f_X((X_1,...,X_n)\mid \theta)&=\prod_{i=1}^n f(X_i\mid \theta)\\
            &=g[T(X_1,...,X_n)| \theta] h(X_1,...,X_n)
        \end{aligned}
        \nonumber
    \end{equation}
\end{proposition}

\begin{example}\quad
    \begin{enumerate}
        \item Suppose $\{X_i\}_{i=1}^n$ be a random sample from $Poisson(\theta)$. Then, show $T(X_1,...,X_n)=\sum_{i=1}^n X_i$ is a sufficient statistic.
        \begin{enumerate}
            \item \textbf{Prove by Definition:} The sum of independent Poisson random variables are Poisson random variable, so we have $T=\sum_{i=1}^n X_i\sim Pois(n\theta)$. Then the conditional distribution of $X_1,...,X_n$ given $T$ is
            \begin{equation}
                \begin{aligned}
                    f(X_1,...,X_n\mid T)=\frac{\prod_{i=1}^n\frac{\theta^{X_i} e^{-\theta}}{X_i!}}{\frac{(n\theta)^{T} e^{-n\theta}}{T!}}=\frac{T!}{n^T\prod_{i=1}^nX_i!}
                \end{aligned}
                \nonumber
            \end{equation}
            which is independent of $\theta$. So, $T(X_1,...,X_n)$ is sufficient statistic by definition.
            \item \textbf{Prove by Factorization Theorem:}
            \begin{equation}
                \begin{aligned}
                    \prod_{i=1}^n f(X_i\mid\theta)=\prod_{i=1}^n\frac{\theta^{X_i} e^{-\theta}}{X_i!}=\frac{\theta^{T(X_1,...,X_n)} e^{-n\theta}}{\prod_{i=1}^n X_i!}=g(T(X_1,...,X_n)\mid\theta)h(X_1,...,X_n)
                \end{aligned}
                \nonumber
            \end{equation}
            where $g(T(X_1,...,X_n)\mid\theta)=\theta^{T(X_1,...,X_n)} e^{-n\theta}$ and $h(X_1,...,X_n)=\frac{1}{\prod_{i=1}^n X_i!}$. Hence, $T(X_1,...,X_n)$ is sufficient statistic by Fisher-Neyman Factorization Criterion.
            \item \textbf{Prove by Exponential Family:}
            \begin{equation}
                \begin{aligned}
                    f(X\mid\theta)=\frac{\theta^X e^{-\theta}}{X!}=\frac{e^{-\theta+X\ln\theta}}{ X!}
                \end{aligned}
                \nonumber
            \end{equation}
            Hence, the distribution is a member of the exponential family, where $c(\theta)=1,h(X)=\frac{1}{X!}, w_1(\theta)=-\theta,w_2(\theta)=\ln\theta,t_1(X)=1,t_2(X)=X$. By theorem \ref{comp_exp_fam}, $\sum_{i=1}^n X_i$ is sufficient because $\{w_1(\theta)=-\theta,w_2(\theta)=\ln\theta\}$ is non-empty.
        \end{enumerate}
        \item Suppose $X_i\sim \text{i.i.d. } U[0,\theta]$ where $\theta>0$ is unknown. The \underline{marginal pdf} is
        \begin{equation}
            \begin{aligned}
                f(x\mid \theta)=\left\{\begin{matrix}
                    \frac{1}{\theta},&x\in[0,\theta]\\
                    0,&\text{otherwise}
                \end{matrix}\right.=\frac{1}{\theta}\mathbf{1}_{\{x\in[0,\theta]\}}
            \end{aligned}
            \nonumber
        \end{equation}
        \underline{Factorization}:
        \begin{equation}
            \begin{aligned}
                \prod_{i=1}^n f(X_i\mid \theta)=\underbrace{\frac{1}{\theta^n}\mathbf{1}_{\{X_{(n)}\leq \theta\}}}_{g(X_{(n)}|\theta)} \underbrace{\mathbf{1}_{\{X_{(1)}\geq 0\}}}_{h(X_1,...,X_n)}
            \end{aligned}
            \nonumber
        \end{equation}
        Hence, we have shown that $X_{(n)}$ is sufficient $\Rightarrow$ $\hat{\theta}_{MM}=2\bar{X}$ cannot be UMVU and $\hat{\theta}_{RB}=\mathbb{E}[\hat{\theta}_{MM}|X_{(n)}]$ is better.
    \end{enumerate}
\end{example}

\subsection{Minimal Sufficient Statistic}
\begin{definition}[Minimal Sufficient Statistic]
    \normalfont
    A sufficient statistic $T(X_1,...,X_n)$ is called a \textbf{minimal sufficient statistic} if, for any other sufficient statistic $T'(X_1,...,X_n)$, $T(X_1,...,X_n)$ is a function of $T'(X_1,...,X_n)$.
\end{definition}

\begin{theorem}[Theorem to Check Minimal Sufficient Statistic]
    Let $f(\vec{X})$ be the pmf or pdf of a sample $\vec{X}$. Suppose there exists a function $T(\vec{X})$ such that,
    \begin{center}
        "for every sample points $\vec{X}$ and $\vec{Y}$, the ratio $\frac{f(\vec{X}\mid\theta)}{f(\vec{Y}\mid\theta)}$ is constant for any $\theta$ \underline{if and only if} $T(\vec{X}) = T(\vec{Y})$".
    \end{center}
    Then $T(\vec{X})$ is a \textbf{minimal sufficient statistic} for $\theta$.
\end{theorem}

\begin{example}
    Let $X_1,...,X_n\sim \text{i.i.d. }U[\theta -\frac{1}{2},\theta + \frac{1}{2}]$, with $\theta \in \mathbb{R}$ unknown.

    By $f(X\mid\theta)=\mathbf{1}_{\{X\in[\theta-\frac{1}{2},\theta+\frac{1}{2}]\}}$, we have
    \begin{equation}
        \begin{aligned}
            \prod_{i=1}^n f(X_i\mid\theta)&=\underbrace{\mathbf{1}_{\{X_{(1)}\geq\theta-\frac{1}{2}\}}\mathbf{1}_{\{X_{(n)}\leq\theta+\frac{1}{2}\}}}_{g[T(X_1,...,X_n)\mid\theta]}\underbrace{1}_{h(X_1,...,X_n)}
        \end{aligned}
        \nonumber
    \end{equation}
    By the Fisher-Neyman Factorization Criterion, $T(X_1,...,X_n)=\{X_{(1)},X_{(n)}\}$ is a sufficient statistic.

    We can prove $T(X_1,...,X_n)=\{X_{(1)},X_{(n)}\}$ is a minimal sufficient statistic by proving "for every sample points $(X_1,...,X_n)$ and $(Y_1,...,Y_n)$, $\frac{f(X_1,...,X_n\mid\theta)}{f(Y_1,...,Y_n\mid\theta)}$ is constant as a function of $\theta$ if and only if $T(X_1,...,X_n)=T(Y_1,...,Y_n)$."
    \begin{equation}
        \begin{aligned}
            \frac{f(X_1,...,X_n\mid\theta)}{f(Y_1,...,Y_n\mid\theta)}=\frac{\mathbf{1}_{\{X_{(1)}\geq\theta-\frac{1}{2}\}}\mathbf{1}_{\{X_{(n)}\leq\theta+\frac{1}{2}\}}}{\mathbf{1}_{\{Y_{(1)}\geq\theta-\frac{1}{2}\}}\mathbf{1}_{\{Y_{(n)}\leq\theta+\frac{1}{2}\}}}
        \end{aligned}
        \nonumber
    \end{equation}
    Hence, for every sample points $(X_1,...,X_n)$ and $(Y_1,...,Y_n)$, $\frac{f(X_1,...,X_n\mid\theta)}{f(Y_1,...,Y_n\mid\theta)}$ is constant for all $\theta$ if and only if $X_{(1)}=Y_{(1)}$ and $X_{(n)}=Y_{(n)}$. That is, $T(X_1,...,X_n)=T(Y_1,...,Y_n)$. Hence, $T(X_1,...,X_n)=\{X_{(1)},X_{(n)}\}$ is a \textbf{minimal sufficient statistic}.

    Consider $g(T)=X_{(n)}-X_{(1)}-\frac{n-1}{n+1}$, it has $\mathbb{E}[g(T)]=0$ but $P_\theta[g(T)=0]<1$. Hence, $T$ is not a complete statistic by definition.
\end{example}



\subsection{Complete Statistic}
Suppose $T$ is sufficient and then $\hat{\theta}=\hat{\theta}(T)$ is unbiased. Under what conditions (on $T$) is $\hat{\theta}$ UMVU?\\
\textbf{Answers:} If "only one" estimator based on $T$ is unbiased. ($T$ is complete.)

\begin{definition}[Complete Statistic]
    \normalfont
    A statistic $T$ is \textbf{complete} \underline{if and only if}
    \begin{equation}
        \begin{aligned}
            P_\theta[g(T)=0]=1, \ \forall \theta\in \Theta
        \end{aligned}
        \nonumber
    \end{equation}
    \underline{whenever} $g(\cdot)$ is such that
    \begin{equation}
        \begin{aligned}
            \mathbb{E}_\theta[g(T)]=0,\ \forall \theta\in \Theta
        \end{aligned}
        \nonumber
    \end{equation}
    (whenever the mean is zero, it can only equal to zero).
\end{definition}

Recall: A matrix $A_{m\times k}$ has rank $k$ iff $Ax=0 \Rightarrow x=0$.

\begin{theorem}[Lehmann-Scheffé Theorem]
    If $T$ is complete and if $\hat{\theta}=\hat{\theta}(T)$ and $\tilde{\theta}=\tilde{\theta}(T)$ are unbiased, then $$\mathbb{E}_\theta[\hat{\theta}-\tilde{\theta}]=0 \Rightarrow P(\hat{\theta}-\tilde{\theta}=0)=P(\hat{\theta}=\tilde{\theta})=1$$
\end{theorem}

\textbf{\underline{Implication:}}
\begin{corollary}[Unbiased $\hat{\theta}(T)$ with sufficient and complete $T$]
    If $T$ is sufficient and complete and if $\hat{\theta}=\hat{\theta}(T)$ is unbiased, then $\hat{\theta}$ is UMVU (let $\tilde{\theta}$ be an UMVU).
\end{corollary}

\begin{example}\quad
    Suppose $X_i\sim \text{i.i.d. } U[0,\theta]$ where $\theta>0$ is unknown.\\
    \textbf{\underline{Facts:}}
    \begin{enumerate}[$\bullet$]
        \item $X_{(n)}$ is sufficient and complete $\Rightarrow$
        Any unbiased estimator given $X_{(n)}$ is UMVU, e.g. $\hat{\theta}_{RB}=\mathbb{E}[\hat{\theta}_{MM}|X_{(n)}]$;
        \item $\mathbb{E}_\theta(X_{(n)})=\frac{n}{n+1}\theta$ $\Rightarrow$ unbiased $\frac{n+1}{n}X_{(n)}$ is UMVU ($=\hat{\theta}_{RB}$).
    \end{enumerate}
    \begin{remark}
        The cdf of $X_{(n)}$ is
        \begin{equation}
            \begin{aligned}
                F_{X_{(n)}}(x\mid\theta)=F(x\mid\theta)^n=\left\{\begin{matrix}
                    0,&\text{ if }x<0\\
                    \left(\frac{x}{\theta}\right)^n&\text{ if }0\leq x\leq \theta\\
                    1,&\text{ if }x>\theta
                \end{matrix}\right.
            \end{aligned}
            \nonumber
        \end{equation}
        so $X_{(n)}$ is continuous with pdf
        \begin{equation}
            \begin{aligned}
                f_{X_{(n)}}(x\mid\theta)=\left\{\begin{matrix}
                    \frac{n}{\theta^n}x^{n-1}&\text{ if }x\in[0,\theta]\\
                    0,& \text{ otherwise}
                \end{matrix}\right.
            \end{aligned}
            \nonumber
        \end{equation}
        Hence, $\mathbb{E}_\theta X_{(n)}=\int_0^\theta \frac{n}{\theta^n}x^{n-1} x dx = \frac{n}{n+1}\theta$.
    \end{remark}
\end{example}

\textbf{\underline{Verifying Completeness}}
\begin{enumerate}[$\bullet$]
    \item Apply definition:
    \begin{enumerate}[$\circ$]
        \item Example: $\sum_{i=1}^n X_i$ is complete when $X_i\sim i.i.d. \text{ Ber}(p)$ - compute rank of the matrix to check completeness
    \end{enumerate}
    \item Show that $\{f(\cdot|\theta):\theta\in\Theta\}$ is on exponential family and apply theorem \ref{comp_exp_fam}.
\end{enumerate}

\begin{theorem}[Sufficient and Complete Statistic for Exponential Family]\label{comp_exp_fam}
    If the distribution is a member of the exponential family, that is, $$f(x|\theta)=c(\theta)h(x)\text{exp}\left\{\sum_{j=1}^k w_j(\theta) t_j(x)\right\}$$ then $$T=\left(\sum_{i=1}^n t_1(x_i),...,\sum_{i=1}^n t_k(x_i)\right)$$ is sufficient and complete if $\{\{w_1(\theta),...,w_k(\theta)\}:\theta\in\Theta\}$ contains an open set.
\end{theorem}
\begin{example}
    Suppose $X\sim \mathcal{N}(\mu,\sigma^2)$ for some $\mu\in \mathbb{R}$ and some $\sigma^2>0$. Then, $\theta=(\mu,\sigma^2)$ and $\Theta=\mathbb{R}\times \mathbb{R}_{++}$. The pdf can be written as
    \begin{equation}
        \begin{aligned}
            f(x\mid \mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{\mu^2}{2\sigma^2}}e^{\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}x^2}
        \end{aligned}
        \nonumber
    \end{equation}
    We can have $h(x)=1, c(\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{\mu^2}{2\sigma^2}}, t_1(x)=x, w_1(\mu,\sigma^2)=\frac{\mu}{\sigma^2}, t_2(x)=x^2, w_2(\mu,\sigma^2)=-\frac{1}{2\sigma^2}$.\\
    That is, $T=\left(\sum_{i=1}^n X_i, \sum_{i=1}^n X_i^2\right)$ is sufficient and complete.\\
    And $\left(\bar{X},S^2\right)=\left(\frac{1}{n}\sum_{i=1}^n X_i,\frac{1}{n-1}\sum_{i=1}^n\left[X_i^2-\frac{(\sum_{i=1}^n X_i^2)^2}{n}\right]\right)$ is UMVU estimator of $(\mu,\sigma^2)$.
\end{example}


\subsection{Cramér-Rao Lower Bound}
The score function is the derivative of the log likelihood function with respect to $\theta$.
\begin{definition}[Score Function]
    \normalfont
    The \textbf{score function} is $$u(\theta,\vec{X})=\frac{\partial}{\partial \theta}\log f_{\vec{X}}(\vec{X}\mid\theta)=\frac{1}{f_{\vec{X}}(\vec{X}\mid\theta)}\frac{\partial}{\partial \theta}f_{\vec{X}}(\vec{X}\mid\theta)$$
    where $f_{\vec{X}}(\vec{X}\mid\theta)=L(\theta\mid X_1,...,X_n)=\prod_{i=1}^n f_{X_i}(X_i\mid\theta)$.
\end{definition}


\begin{definition}["Regularity" Condition]
    \normalfont
    The regularity conditions are as follows:
    \begin{enumerate}
        \item The partial derivative of $f_{\vec{X}}(\vec{X}\mid\theta)$ with respect to $\theta$ exists almost everywhere. (It can fail to exist on a null set, as long as this set does not depend on $\theta$.)
        \item The integral of $f_{\vec{X}}(\vec{X}\mid\theta)$ can be differentiated under the integral sign with respect to $\theta$.
        \item The support of $f_{\vec{X}}(\vec{X}\mid\theta)$ does not depend on $\theta$.
    \end{enumerate}
\end{definition}
\begin{lemma}
    [Mean of Score Function is Zero]
    \normalfont
    Under "Regularity" condition and $X$ are continuous, the mean of score function is zero:
    \begin{equation}
        \begin{aligned}
            \mathbb{E}_\theta\left[u(\theta,\vec{X})\right]&=\int_{\vec{X}} \left[\frac{\partial}{\partial \theta}\log f_{\vec{X}}(\vec{X}\mid\theta)\right]f_{\vec{X}}(\vec{X}\mid\theta) d\vec{X}\\
            &=\int_{\vec{X}}  \left[\frac{\partial}{\partial \theta}f_{\vec{X}}(\vec{X}\mid\theta)\right]d\vec{X}\\
            (*)\quad &=\frac{\partial}{\partial \theta}\underbrace{\int_{\vec{X}}  f_{\vec{X}}(\vec{X}\mid\theta)d\vec{X}}_{=1}=0
        \end{aligned}
        \nonumber
    \end{equation}
    (*): Moving the derivative outside the integral can be done as long as the limits of integration are fixed, i.e. they do not depend on $\theta$.
\end{lemma}

\begin{definition}[Fisher Information]
    \normalfont
    Under "regularity" conditions, the \textbf{Fisher information} is defined to be the variance of the score function.
    \begin{equation}
        \begin{aligned}
            \mathcal{I}(\theta)=\text{Var}_\theta(u(\theta,\vec{X}))=\mathbb{E}_\theta[u(\theta,\vec{X})u(\theta,\vec{X})^T]
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}
\begin{lemma}[Fisher Information]
    For $\theta\in\Theta\subseteq \mathbb{R}$, the Fisher information can be written as
    \begin{equation}
        \begin{aligned}
            \mathcal{I}(\theta)&=\text{Var}_\theta\left[\frac{\partial}{\partial \theta}\log f_{\vec{X}}(\vec{X}\mid\theta)\right]\\
            &=\mathbb{E}_\theta\left[\left(\frac{\partial}{\partial \theta}\log f_{\vec{X}}(\vec{X}\mid\theta)\right)^2\right]=n\mathbb{E}_\theta\left[\left(\frac{\partial}{\partial \theta}\log f_X(X_i\mid\theta)\right)^2\right]\\
            &=-\mathbb{E}_\theta\left[\frac{\partial^2}{\partial \theta^2}\log f_{\vec{X}}(\vec{X}\mid\theta)\right]=-n\mathbb{E}_\theta\left[\frac{\partial^2}{\partial \theta^2}\log f_X(X_i\mid\theta)\right]
        \end{aligned}
        \nonumber
    \end{equation}
\end{lemma}

\begin{proposition}[Cramér-Rao Lower Bound]
    Under "regularity" conditions, for every estimator $\hat{\theta}$
    \begin{equation}
        \begin{aligned}
            \text{Var}_\theta[\hat{\theta}(\vec{X})]\geq \frac{\left(\frac{d}{d\theta}\mathbb{E}_\theta[\hat{\theta}(\vec{X})]\right)^2}{\mathcal{I}(\theta)}\equiv \text{CRLB}(\theta)
        \end{aligned}
        \nonumber
    \end{equation}
    Specifically, if the estimator $\hat{\theta}$ is unbiased, $$\text{CRLB}(\theta)=\mathcal{I}(\theta)^{-1}$$
\end{proposition}
\begin{remark}
    $\mathcal{I}(\theta)$ is called the \textbf{Fisher Information}; "Regularity" conditions are satisfied by "smooth" exponential families; Proof uses Cauchy-Schwarz inequality.
\end{remark}
\textbf{\underline{3 Possibilities}}
\begin{enumerate}[(1).]
    \item CR inequality is applicable and attainable:
    \begin{enumerate}
        \item Estimating $p$ when $X\sim \text{i.i.d. Ber}(p)$;
        \item Estimating $\mu$ when $X\sim \text{i.i.d. } N(\mu,\sigma^2)$.
    \end{enumerate}
    \item CR inequality is applicable, but not attainable:
    \begin{enumerate}
        \item Estimating $\sigma^2$ when $X\sim \text{i.i.d. } N(\mu,\sigma^2)$: $\text{Var}(S^2)=\frac{2\sigma^4}{n-1}>\frac{2\sigma^4}{n}=\mathcal{I}(\theta)^{-1}$ (CR bound).
    \end{enumerate}
    \item CR inequality is not applicable:
    \begin{enumerate}
        \item Estimating $\theta$ when $X\sim \text{i.i.d. }U[0,\theta]$: CR bound $\mathcal{I}(\theta)^{-1}=\frac{\theta^2}{n}$ and $\text{Var}(\hat{\theta}_{UMVU})=\frac{\theta^2}{n(n+2)}$
    \end{enumerate}
\end{enumerate}

\begin{theorem}[MLE Covariance $\stackrel{n \rightarrow \infty}{\longrightarrow}$ Cramér-Rao Lower Bound]
    Suppose the sample $\{X_i\}_{i=1}^n$ is i.i.d. The Maximum likelihood estimator (MLE) $\hat{\theta}=\arg\max_\theta L(\theta\mid X_1,...,X_n)$, under "regularity" conditions, as $n \rightarrow \infty$
    \begin{equation}
        \begin{aligned}
            \sqrt{n}\left(\hat{\theta}-\theta\right) \rightarrow N(0,\mathcal{I}(\theta)^{-1})
        \end{aligned}
        \nonumber
    \end{equation}
\end{theorem}
\begin{proposition}[Approximation of MLE Covariance Matrix]
    When the sample $x$ is made up of i.i.d. observations, the covariance matrix of the maximum likelihood estimator $\hat{\theta}$ is approximately equal to the inverse of the information matrix. $$\text{Cov}(\hat{\theta})\approx(\mathcal{I}(\theta))^{-1}$$
\end{proposition}
Hence, the covariance matrix can be estimated as $(\mathcal{I}(\hat{\theta}))^{-1}$. Similarly, \textit{SE} is estimated by $\sqrt{(\mathcal{I}(\hat{\theta}))^{-1}}$.


\section{Hypothesis Testing}
$X_1,...,X_n$ is a random sample from a discrete/continuous distribution with pmf/pdf $f(\cdot\mid \theta)$, where $\theta\in\Theta$ is unknown.

\textbf{\underline{Ingredients of Hypothesis Test}}
\begin{enumerate}[(1).]
    \item Formulation of Testing Problem:
    \begin{enumerate}[$\circ$]
        \item Partioning of $\Theta$ into \underline{two} disjoint subsets $\Theta_0$ and $\Theta_1$.
    \end{enumerate}
    \item Testing Procedure:
    \begin{enumerate}[$\circ$]
        \item Rule for choosing the two subsets specified in (1).
    \end{enumerate}
\end{enumerate}

\subsection{Formulation of Testing Problem}
\textbf{\underline{Formulating a Testing Procedure}}
\begin{enumerate}[$\bullet$]
    \item Terminology:
    \begin{definition}[Hypothesis]
        \normalfont
        \begin{enumerate}
            \item A \underline{hypothesis} is a statement about $\theta$;
            \item \underline{Null hypothesis}: $H_0: \theta\in \Theta_0$;
            \item \underline{Alternative hypothesis}: $H_1: \theta\in \Theta_1=\Theta\backslash\Theta_0$;
            \item \underline{Maintained hypothesis}: $\theta\in \Theta$ (always correct).
            \item \textit{Typical Formulation}: $$H_0: \theta\in \Theta_0 \text{ vs. } H_1: \theta\in \Theta_1$$
        \end{enumerate}
    \end{definition}
    \begin{example}
        Suppose $X\sim \text{i.i.d. } N(\mu,1)$, where $\mu\geq 0$ is unknown.\\
        \underline{Objective}: Determine whether $\mu=0$.\\
        \underline{Two possible formulation}: $H_0:\mu=0$ vs. $H_1:\mu> 0$ (or vice versa).
    \end{example}
    \item Testing Procedure:
    \begin{enumerate}
        \item[] Consider the problem of testing $H_0:\theta\in \Theta_0$ vs. $H_1:\theta\in \Theta_1$.
        \item[]
        \begin{definition}[Testing Procedure with Critical Region]
            \normalfont
            A \underline{testing procedure} is a (data-based) rule for choosing between $H_0$ and $H_1$.\\
            The rule: $$\text{"Reject $H_0$ iff $(X_1,...,X_n)\in C$" (for some $C\in \mathbb{R}^n$)}$$ is a testing procedure with \underline{critical region} $C$.
        \end{definition}
    \end{enumerate}
    \begin{example}
        Suppose $X\sim \text{i.i.d. } N(\mu,1)$, where $\mu\geq 0$ is unknown. The decision rule "Reject $H_0$ iff $\frac{\sum_{i=1}^n X_i}{n}=\bar{X}\geq \frac{1.645}{\sqrt{n}}$", where the critical region is $C=\{(X_1,...,X_n):\frac{\sum_{i=1}^n X_i}{n}\geq \frac{1.645}{\sqrt{n}}\}$
    \end{example}
    \begin{proposition}[Critical Region $\Leftrightarrow$ Test Statistic and Critical Value]
        Any set $C\in \mathbb{R}^n$ can be written as $$C=\{(X_1,...,X_n): T(X_1,...,X_n)>c\}$$ for some $T: \mathbb{R}^n \rightarrow \mathbb{R}$ and some $c\in \mathbb{R}$.
    \end{proposition}
    \begin{definition}[Test Statistic and Critical Value]
        \normalfont
        $T(X_1,...,X_n)$ is called a \underline{test statistic} and $c$ is called the \underline{critical value} (of the test).
    \end{definition}
\end{enumerate}

\subsection{Errors, Power Function, and Agenda}
\textbf{\underline{Agenda}}
\begin{enumerate}
    \item Choosing critical value (given test statistic).
    \item Choosing test statistic.
\end{enumerate}
\begin{definition}[Type I and Type II Errors]
    \normalfont
    \begin{center}
        \begin{tabular}{ccc}
            \hline
                Decision vs. Truth&$H_0$ (True) &$H_1$ (False)\\
            \hline
                $H_0$ (Fail to Reject)& &Type II Error\\
                $H_1$ (Reject)& Type I Error &\\
            \hline
        \end{tabular}
    \end{center}
    where
    \begin{enumerate}
        \item Type I Error: mistaken rejection of a null hypothesis that is actually true;
        \item Type II Error: failure to reject a null hypothesis that is actually false.
    \end{enumerate}
\end{definition}
There is a trade-off between Type I and Type II errors. The \underline{general approach} is \textit{statistical decision theory}.
\begin{example}
    \underline{Heading Special Case:} Making $P_\theta[\text{Type I Error}]$ "small".
\end{example}

\begin{definition}[Power Function]
    \normalfont
    The \textbf{power function} of a test unit critical region $C\subseteq \mathbb{R}^n$ is the function $\beta:\Theta \rightarrow [0,1]$ given by
    \begin{equation}
        \begin{aligned}
            \beta(\theta)&=P_\theta[\text{Reject $H_0$}]\\
            &=P_\theta[(X_1,...,X_n)'\in C]\\
            \text{(equivalently) }&=P_\theta[T(X_1,...,X_n)>c]
        \end{aligned}
        \nonumber
    \end{equation}
    for corresponding statistic $T$ and critical value $c$.
\end{definition}
\begin{enumerate}[$\circ$]
    \item For $\theta\in\Theta_0$: $P_\theta[\text{Type I Error}]=P_\theta[\text{Reject $H_0$}]=\beta(\theta)$;
    \item For $\theta\in\Theta_1$: $P_\theta[\text{Type II Error}]=1-P_\theta[\text{Reject $H_0$}]=1-\beta(\theta)$;
    \item Hence, the \underline{ideal power function} is $\beta(\theta)=\left\{\begin{matrix}
        1,&\theta\in\Theta_1\\
        0,&\theta\in\Theta_0
    \end{matrix}\right.$;
    \item \underline{"Good" Power Function}: $\beta(\theta)$ is "low" ("high") when $\theta\in\Theta_0$ ($\theta\in\Theta_1$).
\end{enumerate}
\textbf{\underline{Standard:}}
\begin{enumerate}[(1).]
    \item Given $T(\cdot)$, choose critical value $c$ such that $\beta(\theta)=P_\theta[T(X_1,...,X_n)>c]\leq 5\%$ when $\theta\in\Theta_0$ (i.e., $\sup_{\theta\in\Theta_0}\beta(\theta)\leq 5\%$);
    \item Choose test statistic such that $\beta(\theta)=P_\theta[T(X_1,...,X_n)>c(T)]$ is "large" for $\theta\in\Theta_1$. (Main Tool: Neyman-Pearson Lemma).
\end{enumerate}


\subsection{Choice of Critical Value}
Given $T(\cdot)$, choose critical value $c$ such that $\beta(\theta)=P_\theta[T(X_1,...,X_n)>c]\leq 5\%$ when $\theta\in\Theta_0$ (i.e., $\sup_{\theta\in\Theta_0}\beta(\theta)\leq 5\%$).
\begin{definition}[Test Size and Level $\alpha$]
    \normalfont
    The \textbf{size} of a test (with power function $\beta$) is $\sup_{\theta\in\Theta_0}\beta(\theta)$.\\
    A test is of \textbf{level $\alpha$ ($\in[0,1]$)} \underline{if and only if} its size is $\leq \alpha$. (Standard choice $\alpha=0.05$).
\end{definition}

\begin{example}
    Suppose $X\sim \text{i.i.d. } N(\mu,1)$, where $\mu\geq 0$ is unknown.\\
    Consider the decision rule "Reject $H_0$ iff $\frac{\sum_{i=1}^n X_i}{n}=\bar{X}\geq \frac{1.645}{\sqrt{n}}$". The power function is $\beta(\mu)=P_\mu[\text{Reject $H_0$}]=P_\mu(\bar{X}\geq\frac{1.645}{\sqrt{n}})$\\
    \underline{Recall}: $\bar{X}\sim \mathcal{N}(\mu,\frac{1}{n}) \Rightarrow \sqrt{n}(\bar{X}-\mu)\sim \mathcal{N}(0,1)$.
    \begin{equation}
        \begin{aligned}
            \beta(\mu)=P_\mu[\text{Reject $H_0$}]&=P_\mu(\bar{X}\geq\frac{1.645}{\sqrt{n}})\\
            &=P_\mu(\sqrt{n}(\bar{X}-\mu)\geq 1.645-\sqrt{n}\mu)\\
            &=1-\Phi(1.645-\sqrt{n}\mu)
        \end{aligned}
        \nonumber
    \end{equation}
    where $\Phi$ is the standard normal cdf.\\
    \underline{Size}$=\beta(0)=1-\Phi(1.645)\approx 0.05$.
\end{example}

\subsection{Choice of Test Statistic: Uniformly Most Powerful (UMP) Level $\alpha$ Test}
Choose test statistic such that $\beta(\theta)=P_\theta[T(X_1,...,X_n)>c(T)]$ is "large" for $\theta\in\Theta_1$. (Main Tool: Neyman-Pearson Lemma).

\begin{definition}[Uniformly Most Powerful (UMP) Level $\alpha$ Test]
    \normalfont
    A test with level $\alpha$ and power function $\beta$ is a \textbf{\underline{uniformly most powerful} (UMP) level $\alpha$ test} iff
    \begin{equation}
        \begin{aligned}
            \beta(\theta)\geq \tilde{\beta}(\theta),\ \forall \theta\in\Theta_1
        \end{aligned}
        \nonumber
    \end{equation}
    where $\tilde{\beta}$ is the power function of some (other) level $\alpha$ test.
\end{definition}

Consider the problem of testing $H_0:\theta=\theta_0\in \mathbb{R}$
\begin{enumerate}[$\circ$]
    \item UMP level $\alpha$ test always $\exists$ if $H_1:\theta=\theta_1$ (Proven by Neyman-Pearson Lemma);
    \item UMP level $\alpha$ test often $\exists$ if $H_1:\theta>\theta_0$ or $H_1:\theta<\theta_0$ (Proven by Karlin-Rubin Theorem);
    \item UMP level $\alpha$ test often $\nexists$ if $H_1:\theta\neq \theta_0$; UMP "unbiased" level $\alpha$ test often $\exists$.
\end{enumerate}
\begin{theorem}[Neyman-Pearson Lemma]\label{NP_lemma}
    Consider the problem of testing,
    \begin{equation}
        \begin{aligned}
            H_0:\theta=\theta_0 \text{ vs. }H_1:\theta=\theta_1
        \end{aligned}
        \nonumber
    \end{equation}
    For any $k\geq 0$, the test which
    \begin{equation}
        \begin{aligned}
            \text{Rejects $H_0$ iff }L(\theta_1\mid X_1,...,X_n)\geq k L(\theta_0\mid X_1,...,X_n)
        \end{aligned}
        \nonumber
    \end{equation}
    is a UMP level $\alpha$ test, where
    \begin{equation}
        \begin{aligned}
            \alpha=P_{\theta_0}[L(\theta_1\mid X_1,...,X_n)\geq k L(\theta_0\mid X_1,...,X_n)]
        \end{aligned}
        \nonumber
    \end{equation}
    and where $L(\theta\mid X_1,...,X_n)=\prod_{i=1}^n f(X_i\mid\theta)$.
\end{theorem}
\begin{remark}
    \begin{enumerate}[$\circ$]
        \item UMP level $\alpha$ test exists if $\alpha\in \{P_{\theta_0}[L(\theta_1\mid X_1,...,X_n)\geq k L(\theta_0\mid X_1,...,X_n)]:k\geq 0\}$.
        \item The Neyman-Pearson Lemma rejects the $H_0$ iff
        \begin{equation}
            \begin{aligned}
                L(\theta_1\mid X_1,...,X_n)\geq k L(\theta_0\mid X_1,...,X_n)\Leftrightarrow \frac{L(\theta_1\mid X_1,...,X_n)}{L(\theta_0\mid X_1,...,X_n)}\geq k
            \end{aligned}
            \nonumber
        \end{equation}
        ($L(\theta_0\mid X_1,...,X_n)\neq 0$)
        \item Hence, it is called \textbf{"Likelihood Ratio" test}.
        \item Converse: Any UMP level $\alpha$ test is of "NP type."
    \end{enumerate}
\end{remark}

\subsubsection*{Example of Using NP Lemma}
\begin{example}
    Suppose $X\sim \text{i.i.d. } N(\mu,1)$, where $\mu\geq 0$ is unknown.\\
    Let $\mu_1=0$ be given and consider the problem of testing $$H_0:\mu=0 \text{ vs. }H_1:\mu=\mu_1>0$$
    We have $L(\mu\mid X_1,...,X_n)=\prod_{i=1}^n \left(\frac{1}{\sqrt{2\pi}}e^{-\frac{(X_i-\mu)^2}{2}}\right)=\left(2\pi\right)^{-\frac{n}{2}}e^{-\frac{1}{2}\sum_{i=1}^n X_i^2} e^{\mu\sum_{i=1}^n X_i}e^{-\frac{n\mu^2}{2}}$. Then,
    \begin{equation}
        \begin{aligned}
            \frac{L(\mu=\mu_1\mid X_1,...,X_n)}{L(\mu=0\mid X_1,...,X_n)}=e^{\mu_1\sum_{i=1}^n X_i}e^{-\frac{n\mu_1^2}{2}}
        \end{aligned}
        \nonumber
    \end{equation}
    \underline{Decision Rule:}
    Reject $H_0$ iff
    \begin{equation}
        \begin{aligned}
            \frac{L(\mu=\mu_1\mid X_1,...,X_n)}{L(\mu=0\mid X_1,...,X_n)}&=&e^{\mu_1\sum_{i=1}^n X_i}e^{-\frac{n\mu_1^2}{2}}\geq k\\
            &\Leftrightarrow& -\frac{n\mu_1^2}{2}+\mu_1\sum_{i=1}^n X_i&\geq \log k\\
            &\Leftrightarrow& \bar{X}\geq \frac{\log k }{n\mu_1}+\frac{\mu_1}{2}
        \end{aligned}
        \nonumber
    \end{equation}
    The NP test reject for large values of $\bar{X}$.
\end{example}

\subsubsection*{Optimality Theorem for One-sided Testing Problem}
Consider $$H_0:\mu=\mu_0 \text{ vs. }H_1:\mu>\mu_0$$
For any $\theta_1>\theta_0$, use NP Lemma to find optimal test of $H_0:\mu=\theta_0 \text{ vs. }H_1:\mu=\mu_1$.
\begin{enumerate}[$\circ$]
    \item If the NP tests coincide, then the test is the UMP test of $H_0:\mu=\mu_0 \text{ vs. }H_1:\mu>\mu_0$;
    \item Otherwise, $\nexists$ UMP (level $\alpha$) test of the $H_0:\mu=\mu_0 \text{ vs. }H_1:\mu>\mu_0$.
\end{enumerate}
\textbf{\underline{Implications}:} (The previous $N(\mu,1)$ example)
\begin{enumerate}[(i).]
    \item The UMP $5\%$ test of $H_0:\mu=0 \text{ vs. }H_1:\mu>0$ rejects $H_0$ iff $\bar{X}>\frac{1.645}{\sqrt{n}}$.
    \item The UMP $5\%$ test of $H_0:\mu=0 \text{ vs. }H_1:\mu<0$ rejects $H_0$ iff $-\bar{X}>\frac{1.645}{\sqrt{n}}$.
    \item $\nexists$ UMP $5\%$ test of $H_0:\mu=0 \text{ vs. }H_1:\mu\neq 0$.
\end{enumerate}

\begin{definition}[Unbiased Test]
    \normalfont
    A test of $$H_0:\theta\in\Theta_0 \text{ vs. }H_1:\theta\in\Theta_1$$
    is \textbf{unbiased} iff its power function $\beta(\cdot)$ satisfies $\sup_{\theta\in\Theta_0}\beta(\theta)\leq \inf_{\theta\in\Theta_1}\beta(\theta)$
\end{definition}
\begin{claim}
    The UMP \underline{unbiased} 5\% test of $H_0:\mu=0 \text{ vs. }H_1:\mu\neq 0$: Rejects $H_0$ iff $|\bar{X}|>\frac{1.96}{\sqrt{n}}$.
\end{claim}
\begin{corollary}
    Suppose $X_i\sim \text{i.i.d. }N(\mu,\sigma^2)$, where $\sigma^2$ is known. Then, the UMP unbiased $5\%$ test of the $H_0:\mu=\mu_0 \text{ vs. }H_1:\mu\neq \mu_0$: Rejects $H_0$ if $|\frac{\bar{X}-\mu_0}{\sigma}|>\frac{1.96}{\sqrt{n}}$.
\end{corollary}

\begin{claim}
    "In general", "Natural" test statistics are (approximately) optimal and critical values can be find.
\end{claim}


\subsection{Generalized Neyman-Pearson Lemma}
\underline{NP Lemma:} $\max\beta(\theta_1)$ s.t. $\beta(\theta_0)\leq \alpha$;\\
\underline{Generalized NP Lemma:} How to optimize a function with infinity constraints.

\underline{Observation:} If $\beta$ is differentiable, then an unbiased test of the $H_0:\theta=\theta_0 \text{ vs. }H_1:\theta\neq \theta_0$ satisfies $\beta'(\theta_0)=0$
\begin{theorem}[Generalized Neyman-Pearson Lemma]
    \normalfont
\end{theorem}


\section{Trinity of Classical Tests}
\begin{enumerate}[$\circ$]
    \item Likelihood Ratio Test
    \item Lagrangian Multiplier Test (Score Test)
    \item Wald Test
\end{enumerate}
\underline{Properties:} Deliver optimal test in motivating example; closely related (and "approximately" optimal) in general.

\subsection{Test Statistics}
\underline{Settings:} $X_1,...,X_n$ is a random sample from a discrete/continuous distribution with pmf/pdf $f(\cdot\mid\epsilon)$, where $\theta\in\Theta\subseteq \mathbb{R}$ is unknown.

\underline{Testing Problem:} $H_0:\theta=\theta_0 \text{ vs. }H_1:\theta\neq \theta_0$ for some $\theta_0\in\Theta$.

Recall the log likelihood function is given by
\begin{equation}
    \begin{aligned}
        l(\theta\mid X_1,...,X_n)=\sum_{i=1}^n \log f(X_i\mid \theta)
    \end{aligned}
    \nonumber
\end{equation}
The (sample) score function is
\begin{equation}
    \begin{aligned}
        u(\theta\mid X_1,...,X_n)=\frac{\partial}{\partial \theta}l(\theta\mid X_1,...,X_n)
    \end{aligned}
    \nonumber
\end{equation}
and the (sample) fisher information is
\begin{equation}
    \begin{aligned}
        \mathcal{I}(\theta\mid X_1,...,X_n)=-\frac{\partial^2}{\partial \theta^2}l(\theta_0\mid X_1,...,X_n)
    \end{aligned}
    \nonumber
\end{equation}

\begin{enumerate}[$\bullet$]
    \item \textbf{Likelihood Ratio Test Statistic}:
    \begin{equation}
        \begin{aligned}
            T_{LR}\left(X_1,...,X_n\right)&=2\left\{\max_{\theta\in\Theta}l(\theta\mid X_1,...,X_n)-\max_{\theta\in\Theta_0}l(\theta\mid X_1,...,X_n)\right\} \text{ (general form)}\\
            &=2\left\{l(\hat{\theta}_{ML}\mid X_1,...,X_n)-l(\theta_0\mid X_1,...,X_n)\right\}\\
            &=2\log\left\{\frac{l(\hat{\theta}_{ML}\mid X_1,...,X_n)}{l(\theta_0\mid X_1,...,X_n)}\right\}
        \end{aligned}
        \nonumber
    \end{equation}
    Motivation: Neyman-Pearson Lemma (\ref{NP_lemma})
    \item \textbf{Lagrangian Multiplier Test Statistic}:
    \begin{equation}
        \begin{aligned}
            T_{LM}\left(X_1,...,X_n\right)&=\frac{\left(\frac{\partial}{\partial \theta}l(\theta_0\mid X_1,...,X_n)\right)^2}{-\frac{\partial^2}{\partial \theta^2}l(\theta_0\mid X_1,...,X_n)}=\frac{\left(u(\theta_0\mid X_1,...,X_n)\right)^2}{\mathcal{I}(\theta_0\mid X_1,...,X_n)}
        \end{aligned}
        \nonumber
    \end{equation}
    Motivation: $T_{LM}$ is approximate to $T_{LR}$; No estimation required.
    \item \textbf{Wald Test Statistic}:
    \begin{equation}
        \begin{aligned}
            T_W\left(X_1,...,X_n\right)=\frac{(\hat{\theta}_{ML}-\theta_0)^2}{\left\{-\frac{\partial^2}{\partial \theta^2}l(\hat{\theta}_{ML}\mid X_1,...,X_n)\right\}^{-1}}=\frac{(\hat{\theta}_{ML}-\theta_0)^2}{\left(\mathcal{I}(\hat{\theta}_{ML}\mid X_1,...,X_n)\right)^{-1}}
        \end{aligned}
        \nonumber
    \end{equation}
    Motivation: $T_W$ is approximate to $T_{LR}$;\\
    Generalization: Reject the $H_0:\theta=\theta_0$ if $|\hat{\theta}-\theta_0|$ is "large", when $\hat{\theta}$ is some estimator of $\theta$.
\end{enumerate}
\begin{claim}
    In general, for "large" $n$, $$T_{LR}\approx T_{LM}\approx T_W\sim \chi^2(1)=N(0,1)^2 \text{ under }H_0:\theta=\theta_0$$
    \begin{enumerate}[$\circ$]
        \item Approximate $5\%$critical value is $(1.96)^2=3.84$.
        \item $T_{LR}=T_{LM}=T_W\sim \chi^2(1)=N(0,1)^2 \text{ under }H_0:\theta=\theta_0$ when $X_i\sim \text{i.i.d. }N(\mu,1)$.
    \end{enumerate}
\end{claim}

\subsection{Approximation to $T_{LR}$}
In this part as $n \rightarrow \infty$, we use $l(\theta), l'(\theta), l''(\theta)$ to denote $l(\theta\mid X_1,...,X_n), l'(\theta\mid X_1,...,X_n)\triangleq u(\theta\mid X_1,...,X_n), l''(\theta\mid X_1,...,X_n)\triangleq - \mathcal{I}(\theta\mid X_1,...,X_n)$.
\begin{enumerate}[(1).]
    \item $T_{LM}$:\\
    Suppose $$l(\theta)\approx l(\theta_0)+l'(\theta_0)(\theta-\theta_0)+\frac{1}{2}l''(\theta_0)(\theta-\theta_0)^2\triangleq\tilde{l}(\theta)$$
    Then $$\hat{\theta}_{ML}=\argmax_{\theta}l (\theta)\approx \argmax_{\theta}\tilde{l}(\theta)=\theta_0-\frac{l'(\theta_0)}{l''(\theta_0)}\triangleq\tilde{\theta}_{ML}$$
    Hence, $$T_{LR}=2\left\{l(\hat{\theta}_{ML})-l(\theta_0)\right\}\approx 2\left\{\tilde{l}(\tilde{\theta}_{ML})-\tilde{l}(\theta_0)\right\}=-\frac{l'(\theta_0)^2}{l''(\theta_0)}=T_{LM}$$
    \item $T_W$:\\
    Suppose $$l(\theta)\approx l(\hat{\theta}_{ML})+l'(\hat{\theta}_{ML})(\theta-\hat{\theta}_{ML})+\frac{1}{2}l''(\hat{\theta}_{ML})(\theta-\hat{\theta}_{ML})^2\triangleq\hat{l}(\theta)$$
    Then,
    $$T_{LR}=2\left\{l(\hat{\theta}_{ML})-l(\theta_0)\right\}\approx 2\left\{\tilde{l}(\hat{\theta}_{ML})-\hat{l}(\theta_0)\right\}=\frac{(\hat{\theta}_{ML}-\theta_0)^2}{(-l''(\hat{\theta}_ML))^{-1}}=T_W$$
\end{enumerate}

\section{Interval Estimation}
\begin{definition}
    \normalfont
    Suupose $\theta\in \mathbb{R}$.
    \begin{enumerate}
        \item An \underline{interval estimator} of $\theta$ is an interval $\left[L(X_1,...,X_n),U(X_1,...,X_n)\right]$, where $L(X_1,...,X_n)$ and $U(X_1,...,X_n)$ are statistics.
        \item The \underline{converge probability} (of the interval estimator) is the function (of $\theta$) given by $$P_\theta\left[L(X_1,...,X_n)\leq\theta\leq U(X_1,...,X_n)\right]$$
        \item The \underline{confidence coefficient} is $\inf_{\theta}P_\theta\left[L(X_1,...,X_n)\leq\theta\leq U(X_1,...,X_n)\right]$
    \end{enumerate}
\end{definition}
\begin{example}
    Suppose $X_i\sim \text{i.i.d. }N(\mu,1)$, where $\mu$ is unknown.\\
    \underline{Interval estimator:} $\left[\bar{X}-\frac{1.96}{\sqrt{n}},\bar{X}+\frac{1.96}{\sqrt{n}}\right]$.\\
    \underline{Converge probability:} $P_\mu\left[\bar{X}-\frac{1.96}{\sqrt{n}}\leq\mu\leq\bar{X}+\frac{1.96}{\sqrt{n}}\right]=P_\mu\left[-1.96\leq\sqrt{n}(\bar{X}-\mu)\leq 1.96\right]=\Phi(1.96)-\Phi(-1.96)\approx 0.95$.
\end{example}
\underline{Interpretation:}
\begin{enumerate}[(I).]
    \item Recall
    \begin{enumerate}[(i).]
        \item $\bar{X}=\hat{\mu}_{MM}=\hat{\mu}_{ML}=\hat{\mu}_{UMVU}$;
        \item $\bar{X}\sim \mathcal{N}(\mu,\frac{1}{n}) \Rightarrow \frac{1}{\sqrt{n}}=\sqrt{\text{Var}(\bar{x})}$.
    \end{enumerate}
    Hence, $\left[\bar{X}-\frac{1.96}{\sqrt{n}},\bar{X}+\frac{1.96}{\sqrt{n}}\right]=\left[\bar{X}-1.96\sqrt{\text{Var}(\bar{x})},\bar{X}+1.96\sqrt{\text{Var}(\bar{x})}\right]$. $\frac{\bar{X}-\mu}{\text{Var}(\bar{x})}\sim \mathcal{N}(0,1)$.
    \item Recall: The "optimal" two-sided $5\%$ of the $\mu=\mu_0$ rejects \underline{iff} $|\bar{X}-\mu_0|>\frac{1.96}{\sqrt{n}}$\\
    $\Leftrightarrow$ $\bar{X}-\mu_0>\frac{1.96}{\sqrt{n}}$ or $\bar{X}-\mu_0<-\frac{1.96}{\sqrt{n}}$\\
    $\Leftrightarrow$ $\mu_0<\bar{X}-\frac{1.96}{\sqrt{n}}$ or $\mu_0>\bar{X}+\frac{1.96}{\sqrt{n}}$\\
    Hence, the test "accepts" $H_0$ iff
    \begin{equation}
        \begin{aligned}
            \bar{X}-\frac{1.96}{\sqrt{n}}\leq \mu_0\leq \bar{X}+\frac{1.96}{\sqrt{n}}
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}


\chapter{Decision Rule Based Statistical Inference}
\section{Decision Rule}
Given an observation $x\in X$, we want to estimate an unknown state $\theta \in S$ (not necessarily random). The $\theta$ can form $x$ with $P_\theta(x)$. We use decision rule $\delta (x)$ to form an action (estimation of $\theta$) $a=\hat{\theta}$.

\textbf{Example:}
\begin{enumerate}[(1)]
    \item Binary hypothesis testing (detection) when $S=\{0,1\}$ e.g. $P_0\sim \mathcal{N}(0,\sigma^2), P_1\sim \mathcal{N}(\mu,\sigma^2)$
    \item Multiple hypothesis testing (classification) when $S=\{1,2,...,n\}$
    \item (Estimation) when $S=\mathbb{R}$ e.g. $P_\theta\in N(\theta,\sigma^2)$
\end{enumerate}
\begin{example}[ (Binary HT)]
    For the example Binary HT, $P_0\sim \mathcal{N}(0,\sigma^2), P_1\sim \mathcal{N}(\mu,\sigma^2)$: decision rule $\delta: \mathbb{R} \rightarrow \{0,1\}$

    We can find a $\tau$ such that $\delta(x)=\left\{\begin{matrix}
        1,&x\ge \tau\\
        0,& else
    \end{matrix}\right.=\mathbf{1}_{x\geq \tau}$. Howe to choose $\tau$?
    
    Type-I error probability: probability that $\theta$ is $0$ but receive $\delta(x)=1$. $$P_I=P_0\{\delta(x)=1\}=P_0\{x\geq \tau\}=Q\left(\frac{\tau}{\sigma}\right)$$
    Type-II error probability: probability that $\theta$ is $1$ but receive $\delta(x)=0$. $$P_{II}=P_1\{\delta(x)=0\}=P_1(x<\tau)=Q(\frac{\mu-\tau}{\sigma})$$
    
    Both $P_I$ and $P_{II}$ depends on $\tau$. $Q(t)=\int_t^\infty\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}dx$
    
    For $\tau=\frac{\mu}{2}$, $P_I=P_{II}=Q\left(\frac{\mu}{2\sigma}\right)$
\end{example}

\begin{example}[ (Multiple HT)]
    Consider three state $S=\{1,2,3\}$.
    We can find a $\tau$ such that $\delta(x)=\left\{\begin{matrix}
        1,&x< \tau_1\\
        2,& \tau_1\leq x\leq \tau_2\\
        3,& x>\tau_2
    \end{matrix}\right.=\mathbf{1}_{x\geq \tau}$.
    
    \textit{Conditional Error Probabilities:} probability that $\theta$ is $i$ but receive $\delta(x)=j$ (6 types in this example) $$P_i\{\delta(x)=j\}, \forall i\neq j$$
\end{example}

\section{Maximum-Likelihood Principle (state is norandom)}
Maximum-Likelihood Principle $$\hat{\theta}=\argmax_{\theta\in S}P_{\theta}(x)=\argmax_{\theta\in S}\ln P_{\theta}(x)$$
Applied to the binary example: $P_0\sim \mathcal{N}(0,\sigma^2), P_1\sim \mathcal{N}(\mu,\sigma^2)$.

$P_0(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^2}{2\sigma^2}}, P_1(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$. $\ln P_0(x)=c-\frac{x^2}{2\sigma^2}$, $\ln P_1(x)=c-\frac{(x-\mu)^2}{2\sigma^2}$.

Then, the rule can become $$\hat{\theta}=\left\{\begin{matrix}
    0,&x^2<(x-\mu)^2\\
    1,&else
\end{matrix}\right.=\mathbf{1}_{x^2\geq (x-\mu)^2}=\mathbf{1}_{x\geq \frac{\mu}{2}}$$

\subsection*{Vector Observations}
Observations $X=\left(x_1,x_2,...,x_n\right)$, where i.i.d. $x_i\sim P_\theta$. Then $$P_\theta(X)=\prod_{i=1}^n P_\theta(x_i),\ \ln P_\theta(X)=\sum_{i=1}^n\ln P_\theta(x_i)$$
$\ln P_0(x)=cn-\frac{\sum_{i=1}^n x_i^2}{2\sigma^2}$, $\ln P_1(x)=cn-\frac{\sum_{i=1}^n(x_i-\mu)^2}{2\sigma^2}$.

Then, the rule can become $$\hat{\theta}=\left\{\begin{matrix}
    0,&\sum_{i=1}^nx_i^2<\sum_{i=1}^n(x_i-\mu)^2\\
    1,&else
\end{matrix}\right.=\mathbf{1}_{\sum_{i=1}^nx_i^2\geq \sum_{i=1}^n(x_i-\mu)^2}=\mathbf{1}_{\bar{x}\geq \frac{\mu}{2}}$$
where $\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i$. Under both $H_0$ and $H_1$, $\bar{x}\sim \mathcal{N}(0,\frac{\sigma^2}{n})$.

Then, type I error prob and type II error prob are the same $$P_I=P_0\{\bar{x}\geq \frac{\mu}{2}\}=P_{II}=P_1\{\bar{x}< \frac{\mu}{2}\}=Q\left(\frac{\mu\sqrt{n}}{2\sigma}\right)$$

\subsection*{Estimation $S=\mathbb{R}$}
To estimate $\theta$ when $S=\mathbb{R}$
\begin{equation}
    \begin{aligned}
        &\max_{\theta\in \mathbb{R}}\sum_{i=1}^n\ln P_\theta(x_i)\\
        &\Leftrightarrow \max_{\theta\in \mathbb{R}} \left[cn-\frac{\sum_{i=1}^n(x_i-\theta)^2}{2\sigma^2}\right]\\
        &\Leftrightarrow \max_{\theta\in \mathbb{R}} \sum_{i=1}^n(x_i-\theta)^2 \Rightarrow \hat{\theta}=\bar{x}
    \end{aligned}
    \nonumber
\end{equation}

Then, with $\bar{x}\sim \mathcal{N}(\theta,\frac{\sigma^2}{n})$, the $$MSE_\theta=\mathbb{E}_\theta\left(\bar{x}-\theta\right)^2=\frac{\sigma^2}{n}$$

\section{Bayesian Decision Rule (state is random)}
\subsection{Rules}
Prior probability distribution of $\theta$ is $\pi(\theta)$.

\underline{Loss/cost function} with action (estimation) $a$ is $l(a,\theta)$. e.g.
\begin{enumerate}
    \item (binary HT) Hamming/zero-one loss $l(a=\hat{\theta},\theta)=\mathbf{1}_{a\neq \theta}$
    \item (estimation) Squared error loss $l(a=\hat{\theta},\theta)=(a-\theta)^2$; Absolute error loss $l(a,\theta)=|a-\theta|$.
\end{enumerate}

\begin{definition}[Risk]
    \normalfont
    \textbf{Risk of decision rule $\delta$ on $\theta$:} $$R(\delta,\theta)=\mathbb{E}_{X\sim \pi(\theta)}\left[l(\delta(X),\theta)\right]$$ where $X$ are random with prob $P(\cdot|\theta)$.\\
    \textbf{Risk of decision rule $\delta$:}
        \begin{equation}
            \begin{aligned}
                R(\delta)&=\mathbb{E}_{\theta\sim P_\theta}[R(\delta,\theta)]\\
                &=\mathbb{E}_{\theta\sim P_\theta}\mathbb{E}_{X\sim \pi(\theta)}\left[l(\delta(X),\theta)\right]
            \end{aligned}
            \nonumber
        \end{equation}
        where $(X,\theta)$ are random with joint probability distribution $$P(X,\theta)=P(X)\pi(\theta\mid X)$$
\end{definition}
\begin{note}
    In machine learning, we normally use $y$ to substitute $\theta$.
\end{note}


\begin{example}[ (Hamming/zero-one Loss)]
    The risk of decision $\delta(x)$ in Hamming/zero-one loss $l(a=\hat{y},y)=\mathbf{1}_{a\neq y}$
    \begin{equation}
        \begin{aligned}
            R(\delta)&=\mathbb{E}(\mathbf{1}_{\delta(x)\neq y})=\mathbb{E}[\delta(x)\neq y]\\
            &=P(y=0)P[\delta(x)\neq 0|y=0]+P(y=1)P[\delta(x)\neq 1|y=1]\\
            &=P(y=0)P[\delta(x)=1|y=0]+P(y=1)P[\delta(x)=0|y=1]
        \end{aligned}
        \nonumber
    \end{equation}
\end{example}

\subsection{Optimization Problem in Bayes Form}
We want to compute the optimal rule that minimizes the risk:
$$\delta_B=\argmin_\delta R(\delta)$$
\underline{Derive Bayes rule}
\begin{equation}
    \begin{aligned}
        R(\delta)&=\int_x\int_\theta P(x,\theta) l(\delta(x),\theta) d\theta dx\\
        &=\int_x P(x)\int_\theta \pi(\theta|x) l(\delta(x),\theta) d\theta dx\\
    \end{aligned}
    \nonumber
\end{equation}
$\delta_B$ is given by solving the optimization problem:
\begin{equation}
    \begin{aligned}
        \min_\delta \int_x P(x)\int_\theta \pi(\theta|x) l(\delta(x),\theta) d\theta dx
    \end{aligned}
    \nonumber
\end{equation}
Hence,
\begin{proposition}
    $\delta_B=\argmin_\delta R(\delta)$ can be transformed into optimization problems for each $x\in S$
    \begin{equation}
        \begin{aligned}
            \min_{\delta(x)} \int_\theta \pi(\theta\mid x) l(\delta(x),\theta) d\theta
        \end{aligned}
        \nonumber
    \end{equation}
\end{proposition}
\underline{The problem becomes to compute $\pi(\theta|x)$}, which is computed by $$\pi(\theta\mid x)=\frac{\pi(\theta)P(x\mid \theta)}{P(x)}$$



\begin{example}[ (Square Loss)]\label{ex:square_loss}
    Consider a Bernoulli Distribution with parameter $\theta$. We have data $\{Z_1,...,Z_n\}$ and we want to predict the next sample $Z$.\\
    Note that $Z$ is what we want to predict, so the Sqaure error loss given estimation $t$ is
    \begin{equation}
        \begin{aligned}
            R(t,Z;\theta)&=\mathbb{E}_\theta[(t-Z)^2]\\
            &=(t-\theta)^2+\theta(1-\theta)
        \end{aligned}
        \nonumber
    \end{equation}
    Then, the optimal estimation is $t^*=\theta$. And we say the minimial risk is \textbf{oracle risk} $$R(t^*,Z\mid\theta)=\theta(1-\theta)$$\\
    However, we don't have $\theta$. What if we use the MLE $t(\{Z_1,...,Z_n\})=\hat{\theta}_\text{MLE}=\bar{Z}=\frac{1}{n}\sum_{i=1}{n}Z_i$ instead?
    \begin{equation}
        \begin{aligned}
            R(\hat{\theta}_\text{MLE},Z\mid \theta)&=\mathbb{E}_\theta[(\bar{Z}-Z)^2]\\
            &=\mathbb{E}_\theta[(\bar{Z}-\theta)^2]+\theta(1-\theta)\\
            &=\underbrace{\frac{\theta(1-\theta)}{n}}_\text{Sample Uncertainty}+\underbrace{\theta(1-\theta)}_\text{Oracle Risk}
        \end{aligned}
        \nonumber
    \end{equation}
    The averge risk with prior belief $P(\theta)=1_{\{\theta\in[0,1]\}}$ ($\theta\sim \text{Beta}(1,1)=\text{Unif}[0,1]$) is
    \begin{equation}
        \begin{aligned}
            R(t)=\int_\theta R(t,Z\mid \theta) \pi(\theta| \{Z_1,...,Z_n\}) d\theta
        \end{aligned}
        \nonumber
    \end{equation}
    $\pi(\theta| \{Z_1,...,Z_n\})$ is the posterior beliefs about the $\theta$:
    \begin{equation}
        \begin{aligned}
            \pi(\theta\mid \{Z_1,...,Z_n\})&=\frac{f(\{Z_1,...,Z_n\}\mid \theta)P(\theta)}{\int f(\{Z_1,...,Z_n\}\mid \theta')P(\theta')d\theta'}
        \end{aligned}
        \nonumber
    \end{equation}
    As $Z_i\sim \text{Bernoulli}(\theta)$, we have $$\theta\mid \{Z_1,...,Z_n\}\sim \text{Beta}\left(\sum_{i=1}^n Z_i+1, n- \sum_{i=1}^n Z_i+1\right)$$
    which has mean $\mathbb{E}[\theta\mid \{Z_1,...,Z_n\}]=\frac{\sum_{i=1}^n Z_i+1}{n+2}=\frac{\hat{\theta}_{MLE}+\frac{1}{2}}{1+\frac{2}{n}}$.
    Then,
    \begin{equation}
        \begin{aligned}
            R^*(t)&=\min_t\int_\theta R(t,Z\mid \theta) \pi(\theta| \{Z_1,...,Z_n\}) d\theta\\
            &=\min_t \int_\theta (t-\theta)^2 \pi(\theta| \{Z_1,...,Z_n\}) d\theta+\int_\theta \theta(1-\theta)\pi(\theta| \{Z_1,...,Z_n\}) d\theta\\
            \Rightarrow t^*&=\mathbb{E}[\theta\mid \{Z_1,...,Z_n\}]
        \end{aligned}
        \nonumber
    \end{equation}
\end{example}


\subsection{Maximum A Posteriori (MAP) Decision Rule (Binary example)}
\begin{example}
    Hamming/zero-one loss $l(a,y)=\mathbf{1}_{a\neq y}$
\end{example}

\textbf{Maximum A Posteriori (MAP) Decision Rule}:

Optimization problem is $$\delta(x)=\argmin_{a} \sum_{y=0,1} \pi(y|x) \mathbf{1}_{a\neq y} dy=\argmax_{y\in\{0,1\}}\pi(y|x)$$
$$\Rightarrow \sum_{y=0,1} \pi(y|x) \mathbf{1}_{\delta(x)\neq y} dx=\min_{a} \sum_{y=0,1} \pi(y|x) \mathbf{1}_{a\neq y} dy=\min\{\pi(1|x),\pi(0|x)\}$$

Likelihood ratio: $L(x)=\frac{P_1(x)}{P_0(x)}$

Likelihood ratio test: threshold $\tau=\frac{\pi(0)}{\pi(1)}$. If $L(x)>\tau$ accept $H_1$ (equivalent to $P_1(x)\pi(1)>P_0(x)\pi(0)$ which is also equivalent to comparing $\pi(y|x)$).

In this rule the whole optimization problem also goes to
\begin{equation}
    \begin{aligned}
        R(\delta_{MAP})&=\int_x P(x)\sum_{y=0,1} \pi(y|x) \mathbf{1}_{\delta(x)\neq y} dx\\
        &=\int_x P(x)\min\{\pi(1|x),\pi(0|x)\}dx
    \end{aligned}
    \nonumber
\end{equation}

\subsection{Minimum Mean Squared Error (MMSE) Rule ($\mathbb{R}^n$ example)}
\begin{example}[ (Estimation)]
    Squared error loss $l(a,y)=(a-y)^2$.
\end{example}
\subsection*{Minimum Mean Squared Error (MMSE) Rule:}
Optimization problem is $\delta(x)=\argmin_{a} \int_{y} \pi(y|x) (a-y)^2 dy$
\begin{equation}
    \begin{aligned}
        0=\int_{y} \pi(y|x) (\delta_B(x)-y) dy=\delta_B(x)-\mathbb{E}\left[Y|X=x\right]
    \end{aligned}
    \nonumber
\end{equation}
\begin{equation}
    \begin{aligned}
        \Rightarrow \delta_B(x)=\mathbb{E}\left[Y|X=x\right]
    \end{aligned}
    \nonumber
\end{equation}
which is called \textbf{conditional mean estimation}.

In this rule the whole optimization problem also goes to
$$R(\delta_{MMSE})=\int_x P(x)\int_y \pi(y|x) (y-\mathbb{E}\left[Y|X=x\right])^2 dy dx=\mathbb{E}_X Var\left[Y|X=x\right]$$

\textbf{Gaussian case:}
If $X\in \mathbb{R}^n$ and $(Y,X)$ are jointly Gaussian, then the conditional mean is a linear function of x, also called linear MMSE estimator.
\begin{equation}
    \begin{aligned}
        \mathbb{E}\left[Y|X=x\right]=\mathbb{E}[Y]+Cov(Y,X)Cov(X)^{-1}(x-\mathbb{E}[X])
    \end{aligned}
    \nonumber
\end{equation}
and the posterior risk is independent of $x$:
\begin{equation}
    \begin{aligned}
        Var\left[Y|X=x\right]=Var[Y]-Cov(Y,X)Cov(X)^{-1}Cov(X,Y)
    \end{aligned}
    \nonumber
\end{equation}
\underline{\textbf{Note:}} MMSE estimator coincides with the MAP estimator for Gaussian Variables.

\section{Comparison}
Maximum-Likelihood Principle (state is nonrandom): $$\delta_{ML}(x)=\argmax_y P_y(x)$$

Maximum A Posteriori (MAP) Decision Rule (state is random): $$\delta_{MAP}(x)=\argmax_y \pi(y|x)=\argmax_y \{\pi(y|x),P_y(x)\}$$



\chapter{Bootstrap}
Bootstrap is a procedure to compute properties of an estimator by random re-sampling with replacement from the data. It was first introduced by Efron (1979).

Suppose we have i.i.d. sample $\vec{Y}=(Y_1,Y_2,...,Y_n)$ taken i.i.d. from a distribution with cdf $F$ and we want to compute a statistic $\theta$ of the distribution using an estimator $\hat{\theta}_n(\vec{Y})$. The distribution of the statistic $\theta$ has cdf $G$.
%\begin{example}
    %$P_{\theta}=N(0,1),\hat{\theta}_n=\frac{1}{n}\sum_{i=1}^nY_i$.
%\end{example}

While the estimator $\hat{\theta}_n(\vec{Y})$ may not be optimal in any sense, it is often the case that $\hat{\theta}_n(\vec{Y})$ is consistent in probability, i.e., $\hat{\theta}_n(\vec{Y}) \stackrel{p}{\longrightarrow} \theta$ as $n \rightarrow \infty$. We want to analyze the performance of the estimartor $\hat{\theta}_n(\vec{Y})$ in terms of the following quantities:
\begin{enumerate}[(1).]
    \item Bias: $$\text{Bias}(\hat{\theta}_n)= \mathbb{E}_{\theta}[\hat{\theta}_n(\vec{Y})]-\theta$$
    \item Variance: $$\text{Var}(\hat{\theta}_n)=\mathbb{E}_{\theta}[\hat{\theta}_n^2(\vec{Y})]-\mathbb{E}_{\theta}^2[\hat{\theta}_n(\vec{Y})]$$
    \item CDF: $$G_{n}(t)=P(\hat{\theta}_n(\vec{Y})<t),\forall t$$
\end{enumerate}

\section{Traditional Monte-Carlo Approach}
Generate $k$ vectors $\vec{Y}^{(i)},i=1,2,...,k$ (total $kn$ random variables)
\begin{enumerate}[(1).]
    \item Bias: $$\widehat{\text{Bias}}(\hat{\theta}_n)=\frac{1}{k}\sum_{j=1}^k \hat{\theta}_n(\vec{Y}^{(j)})-\theta$$
    By the strong law of large number, the mean $\frac{1}{k}\sum_{j=1}^k \hat{\theta}_n(\vec{Y}^{(j)})$ converges almost surely to the expected value $\mathbb{E}_{\theta}[\hat{\theta}_n(\vec{Y})]$, so $\widehat{\text{Bias}}(\hat{\theta}_n) \stackrel{a.s.}{\longrightarrow} \text{Bias}(\hat{\theta}_n)$.
    \item Variance: $$\widehat{\text{Var}}(\hat{\theta}_n)=\frac{1}{k}\sum_{j=1}^k \hat{\theta}^2_n(\vec{Y}^{(j)})-\left(\frac{1}{k}\sum_{j=1}^k \hat{\theta}_n(\vec{Y}^{(j)})\right)^2$$
    Still by the strong law of large number, the mean $\frac{1}{k}\sum_{j=1}^k \hat{\theta}_n(\vec{Y}^{(j)})$ converges almost surely to the expected value $\mathbb{E}_{\theta}[\hat{\theta}_n(\vec{Y})]$ and the mean $\frac{1}{k}\sum_{j=1}^k \hat{\theta}^2_n(\vec{Y}^{(j)})$ converges almost surely to the expected value $\mathbb{E}_{\theta}[\hat{\theta}^2_n(\vec{Y})]$, so $\widehat{\text{Var}}(\hat{\theta}_n) \stackrel{a.s.}{\longrightarrow} \text{Var}(\hat{\theta}_n)$.
    \item Empirical Distribution Function (CDF): $$\hat{G}_{n}(t)=\frac{1}{k}\sum_{j=1}^k \mathbf{1}\{\hat{\theta}_n(\vec{Y}^{(j)})<t\},\forall t$$
    By law of large numbers, we have $\hat{G}_n(x) \stackrel{a.s.}{\longrightarrow} G_n(x),\forall t\in \mathbb{R}$ as $k \rightarrow \infty$.\\
    By Glivenko-Cantelli Theorem, we have $\sup_{t\in \mathbb{R} |\mathbb{R}}\hat{G}_n(x) - G_n(x)| \stackrel{a.s.}{\longrightarrow} 0$ as $k \rightarrow \infty$. (Stronger result).
\end{enumerate}


\section{Bootstrap (When data is not enough)}
Suppose we only have data $\vec{Y}=(Y_1,...,Y_n)$ and we can't draw new samples from the real distribution anymore. We reuse $Y_1,...Y_n$ to obtain resamples $\vec{Y}^*=(Y_1^*,...,Y_n^*)$ (drawing from $\{Y_1,...Y_n\}$ uniformly, equivalently drawing from the empirical distribution with cdf $F_n(y)\triangleq \frac{1}{n}\sum_{i=1}^n\mathbf{1}\{Y_i=y\}$). We get $k$ resamples, denoted by ${\vec{Y^*}}^{(1)},...,{\vec{Y^*}}^{(k)}$.
\begin{enumerate}
    \item Bias: $$\text{Bias}^*(\hat{\theta}_n)\triangleq\frac{1}{k}\sum_{j=1}^k \hat{\theta}_n(\vec{Y^*}^{(j)})-\theta$$
    \item Variance: $$\text{Var}^*(\hat{\theta}_n)\triangleq\frac{1}{k}\sum_{j=1}^k \hat{\theta}^2_n(\vec{Y^*}^{(j)})-\left(\frac{1}{k}\sum_{j=1}^k \hat{\theta}_n(\vec{Y^*}^{(j)})\right)^2$$
    \item CDF: $$\hat{G}^*_{n}(t)=\frac{1}{k}\sum_{j=1}^k \mathbf{1}_{\hat{\theta}_n(\vec{Y^*}^{(j)})<t},\forall t$$
\end{enumerate}
\begin{note}
    $\hat{G}^*_{n}(t)$ may not always converges to $G_n$ as $n \rightarrow \infty$.
\end{note}
\begin{example}[ (Bootstrap Fail Example)]
    Suppose $Y\sim \text{i.i.d. }[0,\theta]$ and consider the estimator $\hat{\theta}_n(\vec{Y})=\max_i Y_i\triangleq Y_{(n)}$. Then, for all $t\geq 0$, $$G_n(t) \rightarrow 1-e^{-\frac{t}{\theta_F}}\text{ as }n \rightarrow \infty$$
    But for all $t\geq 0$, $$\hat{G}^*_{n}(t)\geq P_{F_n}(Y_{(n)}=Y^*_{(n)})=1-(1-\frac{1}{n})^n \rightarrow  1-e^{-1} \text{ as }n \rightarrow \infty$$
\end{example}


\section{Residual Bootstrap (for problem with not i.i.d. data)}
The bootstrap principle is quite general and may also be used in problems where the data $Y_i$, $1\leq i\leq n$, \textbf{are not i.i.d}.

\subsection{Example: Linear}
Consider the model $$Y_i=a+bs_i+Z_i,\ i=1,2,...,n$$
where $\theta=(a,b)$ is the parameter to be estimated, $\vec{s}=(s_1,...,s_n)$ is a known signal, and $Z_i\sim \mathcal{N}(0,\sigma^2)$ (i.i.d.).

The Linear Least Square Estimator is
$$(\hat{a}_n,\hat{b}_n)=\argmin_{(a,b)}\sum_{i=1}^n(Y_i-a-bs_i)^2$$
Given $\vec{Y}$ and estimator $\hat{\theta}_n=(\hat{a}_n,\hat{b}_n)$, define the residual errors (not i.i.d.)$$E_i=Y_i-\hat{a}_n-\hat{b}_ns_i\approx Z_i$$
Then, we use bootstrap to generate $k$ resamples of $\vec{E}=(E_1,E_2,...,E_n)$.\\
For $j=1,...,k$, do the following:
\begin{enumerate}
    \item Obtain $\vec{E^*}^{(j)}$ by uniformly resampling from $\vec{E}$.
    \item Compute pseudo-data ${Y_i^*}^{(j)}=\hat{a}_n+\hat{b}_ns_i+{E_i^*}^{(j)}$ for $1\leq i\leq n$.
    \item Compute LS estimator to the pseudo-data $$\hat{\theta}_n^{(j)}=(\hat{a}_n^{(j)},\hat{b}_n^{(j)})=\argmin_{(a,b)}\sum_{i=1}^n({Y_i^*}^{(j)}-a-bs_i)^2$$
\end{enumerate}
Then, we can evaluate bias $$\widehat{Bias}=\frac{1}{k}\sum_{j=1}^k \hat{\theta}_n^{(j)}-\theta$$

\subsection{Example: Nonlinear Markov Process}
Consider the model $Y_i=F_{\theta}(Y_{i-1})+Z_i$, where $Z_i\sim \mathcal{N}(0,\sigma^2)$ (i.i.d.) for $i=1,2,...,n$

Parameter $\theta=(a,b)$. Linear Least Square Estimator:
$$\hat{\theta}_n(\vec{Y})=\argmin_{\theta}\sum_{i=1}^n(Y_i-F_{\theta}(Y_{i-1}))^2$$
Given $\vec{Y}$, the residual (not i.i.d.)$$E_i=Y_i=\hat{a}_n-F_{\hat{\theta}_n}(Y_{i-1})\approx Z_i$$
Generate $k$ resamples of $\vec{E}=(E_1,E_2,...,E_n)$\\
$\Rightarrow$ obtain $\vec{E^*}^{(1)},\vec{E^*}^{(2)},...,\vec{E^*}^{(k)}$ by resampling\\
$\Rightarrow$ Fix ${Y_0^*}^{(j)}=Y_0$, compute pseudo-data ${Y_i^*}^{(j)}=F_{\hat{\theta}_n}({Y^*_{i-1}}^{(j)})+{E_i^*}^{(j)}$\\
$\Rightarrow$ Compute LS estimator $$\hat{\theta}_n^{(j)}=\argmin_{(a,b)}\sum_{i=1}^n({Y_i^*}^{(j)}-F_{\hat{\theta}_n}({Y^*_{i-1}}^{(j)}))^2$$
$\Rightarrow$ Evaluate bias $$\widehat{Bias}=\frac{1}{k}\sum_{j=1}^k \hat{\theta}_n^{(j)}-\theta$$


\section{Posterior Simulation / Bayesian (Weighted) Bootstrap}
\begin{assumption}
    Bootstrap makes a strong assumption: The data is discrete and values not seen in the data are impossible.
\end{assumption}

Consider $Z\in \mathbb{Z}=\{z_1,....,z_J\}$ with parameter $\vec{\theta}=\{\theta_1,...,\theta_J\}\in \Theta= \mathbb{S}^{J-1}=\{\vec{\theta}\in \mathbb{R}^J:\sum_{j=1}^J\theta_j=1,\theta_j\geq 0, j=1,...,J\}$ such that $P(Z=z_j\mid\vec{\theta})=\theta_j$.

Given a sample $\vec{Z}=\left(Z_1,...,Z_N\right)$. Define $N_j=\sum_{i=1}^N \mathbf{1}\{Z_i=z_j\}, j=1,2,...,J$, the number of observations that have value $z_j$. Then, the conditional pmf of $\vec{Z}\mid\vec{\theta}$ is $$f(\vec{Z}\mid\vec{\theta})=\prod_{j=1}^J\theta_j^{N_j}$$

\begin{definition}[Steps to estimate $\beta$ by Bayesian Bootstrap]
\normalfont
\begin{enumerate}[(1).]
    \item We have prior $\pi(\vec{\theta})$.
    \item Given $\vec{Z}$, calculate posterior distribution $\pi(\vec{\theta}\mid \vec{Z})$.
    \item Draw samples $\vec{\theta}^{(t)},t=1,...,T$ from $\pi(\vec{\theta}\mid \vec{Z})$.
    \item Then compute $\frac{1}{T}\sum_{t=1}^T\hat{\beta}(\vec{\theta}^{(t)})$.
\end{enumerate}
\end{definition}
\subsection{Dirichlet Distribution Prior}
A convenient way to assign the prior distribution of $\vec{\theta}$ over $\Theta$ is to use Dirichlet distribution.
\begin{definition}[Dirichlet Distribution]
    \normalfont
    A \textbf{Dirichlet distribution} with parameters $\vec{\alpha}=(\alpha_1,...,\alpha_J)$, $J\geq 2$.
    It allocates mass on $\vec{\theta}$ over $\Theta$,
    $$\pi(\vec{\theta})=\frac{\Gamma(\sum_{j=1}^J \alpha_j)}{\sum_{j=1}^N\Gamma(\alpha_j)}\prod_{j=1}^J\theta_j^{\alpha_j-1}$$
    where $\Gamma(z)\triangleq\int_0^\infty t^{z-1}e^{-t}dt$ is Gamma function (if $z$ is positive integer, $\Gamma(z)=(z-1)!$).
    \begin{note}
        Dirichlet distribution generalizes Beta distribution.
    \end{note}
\end{definition}
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.4]{dirichlet_plot.png}
    \caption{Dirichlet Distribution Examples}
    \label{}
\end{figure}

Now let's use Dirichlet distribution with parameters $\vec{\alpha}=(\alpha_1,...,\alpha_J)$ to estimate $\mathbb{E}[\vec{\theta}\mid \vec{Z}]$.

As $f(\vec{Z}\mid\vec{\theta})=\prod_{j=1}^J\theta_j^{N_j}$, we can compute the posterior beliefs
\begin{equation}
    \begin{aligned}
        \pi(\vec{\theta}\mid \vec{Z})&=\frac{f(\vec{Z}\mid \vec{\theta})P(\vec{\theta})}{\int f(\vec{Z}\mid \vec{\theta}')P(\vec{\theta}')d\vec{\theta}'}=\frac{\Gamma(\sum_{j=1}^J (N_j+\alpha_j))}{\sum_{j=1}^N\Gamma(N_j+\alpha_j)}\prod_{j=1}^J\theta_j^{N_j+\alpha_j-1}
    \end{aligned}
    \nonumber
\end{equation}
That is
\begin{equation}
    \begin{aligned}
        \theta\mid\vec{Z}\sim \text{Dirichlet}(\bar{\alpha}), \text{ where }\bar{\alpha}_j=\alpha_j+N_j, \forall j
    \end{aligned}
    \nonumber
\end{equation}
\subsubsection*{Simulate samples from Dirichlet distribution}
\begin{definition}[Simulate samples from $\text{Dirichlet}(\vec{\alpha})$]
    \normalfont
    \begin{enumerate}[1.]
        \item Consider a series of independent Gamma random variable $w_i\sim \text{Gamma}(\alpha_i,1), i=1,...,J$;
        \item Define $v_i=\frac{w_i}{\sum_{j=1}^J w_j}$;
        \item We have $(v_1,...,v_J)\sim \text{Dirichlet}(\alpha_1,...,\alpha_J)$.
    \end{enumerate}
\end{definition}

\subsection{Haldane Prior}
We may also begin with an uninformative prior, an improper prior, $\text{Dirichlet}(\vec{\alpha})$, where $\vec{\alpha} \rightarrow 0$. $\pi(\theta)\varpropto \frac{1}{\theta_1\theta_2\cdots\theta_J}$.

Under this prior, the posterior is $\text{Dirichlet}(N_1,...,N_J)$, where $N_j=\sum_{i=1}^N \mathbf{1}\{Z_i=z_j\}$.

\subsection{Linear Model Case}
Each sample is $Z_i=(1,X_{1,i},X_{2,i},X_{3,i},X_{4,i})$. The linear regression coefficient is $\beta=\mathbb{E}[XX']^{-1}\mathbb{E}[XY]$, and $\mathbb{E}^*[Y\mid X=x]=x'\beta$.

\subsection{Bernoulli Case}
Consdier the problem of Example \ref{ex:square_loss}. Given $N$ random sample $\{Z_1,...,Z_N\}$ from a Bernoulli distribution with parameter $\theta$ and the sum $\sum_{i=1}^N Z_i=S$.

Consider a series of Gamma random variable $w_i^{(t)}\sim \text{Gamma}(1,1)$ from time $t=1,...,T$. Then, we have
\begin{equation}
    \begin{aligned}
        \sum_{i=1}^N w_i^{(t)} \mathbf{1}_{\{Z_i=1\}}&\sim \text{Gamma}(S,1)\\
        \sum_{i=1}^N w_i^{(t)} \mathbf{1}_{\{Z_i=0\}}&\sim \text{Gamma}(N-S,1)
    \end{aligned}
    \nonumber
\end{equation}
Define $v_i^{(t)}=\frac{w_i^{(t)}}{\sum_{j=1}^N w_j^{(t)}}$. Based on the property of Gamma distribution, we have $\mathbb{E}[w_i^{(t)}]=\text{Var}[w_i^{(t)}]=1$ and $\mathbb{E}[v_i^{(t)}]=\frac{1}{N}$.

As the relation between Gamma distribution and Beta distribution, we have
\begin{equation}
    \begin{aligned}
        \frac{\text{Gamma}(S,1)}{\text{Gamma}(S,1)+\text{Gamma}(N-S,1)}\sim \text{Beta}(S,N-S)
    \end{aligned}
    \nonumber
\end{equation}
Hence, we can define
\begin{equation}
    \begin{aligned}
        \hat{\theta}^{(t)}&=\sum_{i=1}^N v_i^{(t)}Z_i\\
        &=\sum_{i=1^N}\frac{w_i^{(t)}Z_i}{\sum_{j=1}^N w_j^{(t)}}\sim \text{Beta}(S,N-S)
    \end{aligned}
    \nonumber
\end{equation}
which is close to the posterior beliefs in Example \ref{ex:square_loss} and can be seen as the posterior beliefs drawn from an \underline{improper prior}: $\theta\sim \text{Beta}(\epsilon,\epsilon), \epsilon \rightarrow 0$, which has p.d.f. $\pi(\theta)=\frac{1}{\theta(1-\theta)}$.

We use $$\frac{1}{T}\sum_{t=1}^T\hat{\theta}^{(t)}\approx \mathbb{E}[\theta^{(t)}|\{Z_1,...,Z_n\}]$$ to estimate $\mathbb{E}[\theta^{(t)}|\{Z_1,...,Z_n\}]$.





\chapter{Nonparameteric Prediction Probelm}
\subsection*{Problem}
There are $J$ non-stochastic treats $X\in \mathbb{X}\subseteq \mathbb{R}^J$, and we want to predict a related outcome $Y\in \mathbb{Y}\subseteq \mathbb{R}$.

Given a sample $X_i$,
\begin{equation}
    \begin{aligned}
        Y_i=m(X_i)+\sigma u_i
    \end{aligned}
    \nonumber
\end{equation}
where $m(\cdot)$ is an unknown function and $u_i|X_i\sim \mathcal{N}(0,1)$.

\underline{Goal}:
\begin{enumerate}[$\circ$]
    \item Predict $Y$ given a new $X$;
    \item Learn $m(\cdot)$.
\end{enumerate}

\subsection*{Decision Rule}
Given $\vec{X}=\{X_1,...,X_N\}$, we want to derive a decision rule $d(\vec{Y})$ given corresponding $\vec{Y}$ of $\vec{X}$.

Define $\boldsymbol{m}\triangleq \left[m(X_1),...,m(X_N)\right]'$, its estimation is denoted by $\hat{\boldsymbol{m}}$, which is based on the decision rule.

\subsection*{Sum of Squared Residual}
\begin{proposition}[Sum of Squared Residual]
    \textbf{Sum of Squared Residual} (SSR) of an estimation $\hat{\boldsymbol{m}}$ is given by
    \begin{equation}
        \begin{aligned}
            \mathbb{E}\left[\|\vec{Y}-\hat{\boldsymbol{m}}\|^2\right]=N\sigma^2+\sum_{i=1}^N\left(\hat{m}(X_i)-m(X_i)\right)^2-2\sigma^2 df(\hat{\boldsymbol{m}})
        \end{aligned}
        \tag{SSR}
        \label{SSR}
    \end{equation}
    where the norm is $\|\vec{X}\|=\left[\sum_{i=1}^N X_i^2\right]^{1/2}$, and the degree of freedom $df(\hat{\boldsymbol{m}})=\frac{\sum_{i=1}^N \text{Cov}(Y_i,\hat{m_i})}{\sigma^2}$.
\end{proposition}
\begin{proof}
    \begin{equation}
        \begin{aligned}
            \mathbb{E}\left[\|\vec{Y}-\hat{\boldsymbol{m}}\|^2\right]&=\mathbb{E}\left[\|(\vec{Y}-\boldsymbol{m})+(\boldsymbol{m}-\hat{\boldsymbol{m}})\|^2\right]\\
            &=\mathbb{E}\left[\|\vec{Y}-\boldsymbol{m}\|^2\right]+\mathbb{E}\left[\|\hat{\boldsymbol{m}}-\boldsymbol{m}\|^2\right]-2 \mathbb{E}\left[(\vec{Y}-\boldsymbol{m})'(\hat{\boldsymbol{m}}-\boldsymbol{m})\right]\\
            &=N\sigma^2+\sum_{i=1}^N\left(\hat{m}(X_i)-m(X_i)\right)^2-2\sigma^2 df(\hat{\boldsymbol{m}})
        \end{aligned}
        \nonumber
    \end{equation}
    The second equality is because
    \begin{equation}
        \begin{aligned}
            \mathbb{E}\left[(\vec{Y}-\boldsymbol{m})'(\hat{\boldsymbol{m}}-\boldsymbol{m})\right]&=\sum_{i=1}^N \mathbb{E}\left[(Y_i-m_i)(\hat{m_i}-m_i)\right]\\
            &=\sum_{i=1}^N \mathbb{E}\left[(Y_i-m_i)\hat{m_i}\right]\\
            &=\sum_{i=1}^N \mathbb{E}\left[(Y_i-m_i)(\hat{m_i}-\mathbb{E}m_i)\right]\\
            &=\sum_{i=1}^N \text{Cov}(Y_i,\hat{m_i})
        \end{aligned}
        \nonumber
    \end{equation}
\end{proof}

We can represent the \underline{risk} of estimation $\hat{\boldsymbol{m}}$ by rewriting \ref{SSR}
\begin{equation}
    \begin{aligned}
        \mathbb{E}\left[\|\hat{\boldsymbol{m}}-\boldsymbol{m}\|^2\right]=\mathbb{E}\left[\|\vec{Y}-\hat{\boldsymbol{m}}\|^2\right]-N\sigma^2+2\sigma^2 df(\hat{\boldsymbol{m}})
    \end{aligned}
    \nonumber
\end{equation}

\section{$K$-normal Means Probelm}
\subsection{Assumptions}
\begin{assumption}
    \begin{enumerate}
        \item Linear Combination: Suppose the $m(\cdot)$ can be wrritten as a liner combination of bases functions:
        $$\boldsymbol{m}(X)=\sum_{k=1}^K \alpha_k g_k(X)$$
        \item Gram-Schmidt Orthonormalization (How to processe raw data):
        \begin{definition}[Gram-Schmidt Orthonormalization]\label{GSO}
            \normalfont
            $\phi_{k}(X), k=1,...,K$ such that
            \begin{enumerate}[(1).]
                \item $\frac{1}{N}\sum_{i=1}^N\phi_{k}^2(X_i)=1$ and
                \item $\frac{1}{N}\sum_{i=1}^N\phi_{k}(X_i)\phi_{k}(X_j)=0$.
            \end{enumerate}
        \end{definition}
    \end{enumerate}
    Suppose the $\boldsymbol{m}(\cdot)$ is a liner combination of Gram-Schmidt orthonormalizations:
    \begin{equation}
        \begin{aligned}
            \boldsymbol{m}(X)&=\sum_{k=1}^K \theta_k \phi_{k}(X)
            &\triangleq W \boldsymbol{\theta}
        \end{aligned}
        \nonumber
    \end{equation}
    where $W=\left(w(X_1),...,w(X_N)\right)^T\in \mathbb{R}^{N\times K}$, $w(X_i)=\left(\phi_1(X_i),...,\phi_K(X_i)\right)^T\in \mathbb{R}^{K\times 1}$, and $\boldsymbol{\theta}=\left(\theta_1,...,\theta_K\right)^T$.\\
    That is, given $X$, let
    \begin{equation}
        \begin{aligned}
            \underbrace{\boldsymbol{Y}}_{N\times 1}=\underbrace{W \boldsymbol{\theta}}_{(N\times K) (K\times 1)}+\underbrace{\sigma^2 U}_{N\times 1}, \text{ where }U\sim \mathcal{N}(0,I_N)
        \end{aligned}
        \nonumber
    \end{equation}
\end{assumption}

\subsection{Maximum Likelihood Estimator}
Based on these assumptions, the conditional distribution is
\begin{equation}
    \begin{aligned}
        \boldsymbol{Y}\mid X\sim \mathcal{N}(W \boldsymbol{\theta},\sigma^2 I_N)
    \end{aligned}
    \nonumber
\end{equation}
and the log-likelihood function is
\begin{equation}
    \begin{aligned}
        l(Y\mid X,\boldsymbol{\theta})=-\frac{N}{2}\ln 2\pi-N\ln\sigma-\frac{1}{2\sigma^2}\sum_{i=1}^N\left(Y_i-w(X_i)^T \boldsymbol{\theta}\right)^2
    \end{aligned}
    \nonumber
\end{equation}
Then, we get the \underline{maximum likelihood estimator (MLE)},
\begin{equation}
    \begin{aligned}
        \hat{\theta}_{MLE}=\left[\frac{1}{N}\sum_{i=1}^N w(X_i)w(X_i)^T\right]^{-1}\left[\frac{1}{N}\sum_{i=1}^N w(X_i)Y_i\right]
    \end{aligned}
    \nonumber
\end{equation}
By the orthonormalization assumption \ref{GSO}, $\frac{1}{N}\sum_{i=1}^N w(X_i)w(X_i)^T=I_{K}$.
Hence,
\begin{equation}
    \begin{aligned}
        \hat{\theta}_{MLE}=\frac{1}{N}\sum_{i=1}^N w(X_i)Y_i=\frac{W^T \boldsymbol{Y}}{N}\in \mathbb{R}^{K\times 1}
    \end{aligned}
    \nonumber
\end{equation}
We can observe that it is a \textbf{conditionally unbiased} estimate of $\boldsymbol{\theta}$:
\begin{equation}
    \begin{aligned}
        \mathbb{E}[\hat{\theta}_{MLE}\mid X]&=\frac{1}{N}\sum_{i=1}^N w(X_i)\mathbb{E}[Y_i\mid X]
        &=\left(\frac{1}{N}\sum_{i=1}^N w_iw_i^T\right)\boldsymbol{\theta}=\boldsymbol{\theta}
    \end{aligned}
    \nonumber
\end{equation}
The \textbf{variance} of $k^\text{th}$ item of $\hat{\theta}_{MLE}$, $Z_k\triangleq\frac{1}{N}\sum_{i=1}^N\phi_k(X_i)Y_i\mid X$, is
\begin{equation}
    \begin{aligned}
        \text{Var}(Z_k)=
        \text{Var}\left(\frac{1}{N}\sum_{i=1}^N\phi_k(X_i)Y_i\mid X\right)=\frac{1}{N^2}\sum_{i=1}^N\phi^2_k(X_i)\text{Var}\left(Y_i\mid X\right)=\frac{\sigma^2}{N}
    \end{aligned}
    \nonumber
\end{equation}
Hence,
\begin{equation}
    \begin{aligned}
        \hat{\theta}_{MLE}\mid X\sim \mathcal{N}\left(\boldsymbol{\theta},\frac{\sigma^2}{N}I_K\right)
    \end{aligned}
    \nonumber
\end{equation}

\subsection{Risk of MLE}
All in all, we estimate the $\boldsymbol{m}(\cdot)$ by the unbiased estimator
\begin{equation}
    \begin{aligned}
        \hat{\boldsymbol{m}}=W\hat{\theta}_{MLE}
    \end{aligned}
    \nonumber
\end{equation}
and then the loss is given by
\begin{equation}
    \begin{aligned}
        \|\hat{\boldsymbol{m}}-\boldsymbol{m}\|^2=\|W(\hat{\theta}_{MLE}-\boldsymbol{\theta})\|^2=\sum_{k=1}^K(Z_k-\theta_k)^2\triangleq L(\hat{\theta}_{MLE},\boldsymbol{\theta})
    \end{aligned}
    \nonumber
\end{equation}
where we consider the square loss. Hence, the risk of MLE estimation is
\begin{equation}
    \begin{aligned}
        R(d_{MLE},\boldsymbol{\theta})&=\mathbb{E}[\|\hat{\boldsymbol{m}}-\boldsymbol{m}\|^2]\\
        &=\sum_{k=1}^K\mathbb{E}[(Z_k-\theta_k)^2]=\frac{K}{N}\sigma^2
    \end{aligned}
    \nonumber
\end{equation}

\subsection{James-Stein Type Estimator}
MLE is a member of the class of estimators $\mathcal{L}=\{C \mathbb{Z}:C=\{c_1,...,c_K\},c_k\in[0,1]\}$. Here, we consider a estimartor in the class:
\begin{equation}
    \begin{aligned}
        \mathbb{E}\left[\sum_{k=1}^K\left(c_kZ_k-\theta_k\right)^2\right]&=\mathbb{E}\left[\sum_{k=1}^K\left(c_k(Z_k-\theta_k)-(1-c_k)\theta_k\right)^2\right]\\
        &=\frac{\sigma^2}{N}\sum_{i=1}^K c_k^2+\sum_{i=1}^K(1-c_k)^2\theta_k^2
    \end{aligned}
    \nonumber
\end{equation}
By F.O.C., the optimal estimator minimizing the risk is
\begin{equation}
    \begin{aligned}
        c^*_k=\frac{\theta_k^2}{\frac{\sigma^2}{N}+\theta_k^2},\ k=1,...,K
    \end{aligned}
    \nonumber
\end{equation}
Then, the oracle bound is
\begin{equation}
    \begin{aligned}
        \inf_{d_\mathcal{L}}R(d_\mathcal{L},\boldsymbol{\theta})=\underbrace{\frac{\sigma^2}{N}\left(\sum_{k=1}^K\frac{\theta_k^2}{\frac{\sigma^2}{N}+\theta_k^2}\right)}_{\text{oracle bound}}<\frac{K}{N}\sigma^2
    \end{aligned}
    \nonumber
\end{equation}
(we can't achieve it as we don't know $\boldsymbol{\theta}$).

\textit{Is there a feasible estimator which uniformly improves upon MLE? Yes!}

\subsubsection{Stein's Unbiased Risk Estimate (SURE)}
Consider sample $\mathcal{Z}\sim \mathcal{N}(\theta,\sigma^2 I_K)$, and a estimator based on $\mathcal{Z}$, $\hat{\theta}=\hat{\theta}(\mathcal{Z})$. Let $g(\hat{\theta})=\hat{\theta}-\mathcal{Z}$. Then, the estimate of risk is given by
\begin{equation}
    \begin{aligned}
        \hat{R}_\text{SURE}(\mathcal{Z})=K\sigma^2+2\sigma^2\sum_{k=1}^K \frac{\partial g_k(\mathcal{Z})}{\partial \mathcal{Z}_k}+\underbrace{\sum_{k=1}^K\left(\hat{\theta}_k- \mathcal{Z}_k\right)^2}_{\|g(\hat{\theta})\|_\text{Frobenius}^2}
    \end{aligned}
    \nonumber
\end{equation}
It is an unbiased estimate of the mean-squared error:
\begin{equation}
    \begin{aligned}
        \mathbb{E}[\hat{R}_\text{SURE}]=\mathbb{E}[\|\hat{\theta}-\theta\|^2]
    \end{aligned}
    \nonumber
\end{equation}
\begin{proof}[for the unbiased property]
    According to \eqref{SSR},
    \begin{equation}
        \begin{aligned}
            \mathbb{E}[\|\hat{\theta}-Z\|^2]&=\mathbb{E}[\|\hat{\theta}-\theta\|^2]+K\sigma^2-2\sum_{k=1}^K \text{Cov}(\mathcal{Z}_k,\hat{\theta}_k)\\
            \Rightarrow\ \mathbb{E}[\|\hat{\theta}-\theta\|^2]&=-K\sigma^2+2\sum_{k=1}^K \text{Cov}(\mathcal{Z}_k,\hat{\theta}_k)+\mathbb{E}[\|\hat{\theta}-\mathcal{Z}\|^2]
        \end{aligned}
        \nonumber
    \end{equation}
    where
    \begin{equation}
        \begin{aligned}
            \text{Cov}(\mathcal{Z}_k,\hat{\theta}_k)&=\mathbb{E}[\hat{\theta}_k(\mathcal{Z}_k-\theta_k)]\\
            &=\mathbb{E}[(\hat{\theta}_k-Z_k)(\mathcal{Z}_k-\theta_k)]+\mathbb{E}[(\mathcal{Z}_k-\theta_k)^2]\\
            &=\mathbb{E}[g_k(\mathcal{Z})(\mathcal{Z}_k-\theta_k)]+\sigma^2
        \end{aligned}
        \nonumber
    \end{equation}
    Note $g(\hat{\theta})=\hat{\theta}-\mathcal{Z}$.
    \begin{claim}
        $\mathbb{E}[g_k(\mathcal{Z})(\mathcal{Z}_k-\theta_k)]=\sigma^2 \mathbb{E}[\frac{\partial g_k(\mathcal{Z})}{\partial \mathcal{Z}_k}]$
    \end{claim}
    \begin{equation}
        \begin{aligned}
            \sigma^2\mathbb{E}[\nabla_Z g(\mathbf{Z})]&=\sigma^2\int_z f_Z(z)\nabla_z g(z) dz\\
            &=\sigma^2\int_z f_Z(z)d g(z)\\
            &=\sigma^2\left(f_Z(z)g(z)\bigg|_{\partial\mathbb{R}^K}-\int_z g(z)\frac{\partial  f_Z(z)}{\partial z}dz\right)\\
            &=\sigma^2\int_z g(z)\left(\frac{1}{\sigma^2}(z-\theta)'f_Z(z)\right)dz\\
            &=\int_z f_Z(z)g(z)(z-\theta)' dz\\
            &=\mathbb{E}[g(\mathbf{Z})(\mathbf{Z}-\theta)']
        \end{aligned}
        \nonumber
    \end{equation}
    Hence,
    \begin{equation}
        \begin{aligned}
            \mathbb{E}[\|\hat{\theta}-\theta\|^2]&=K\sigma^2+2\sigma^2\sum_{k=1}^K \mathbb{E}[\frac{\partial g_k(\mathcal{Z})}{\partial \mathcal{Z}_k}]+\mathbb{E}[\|\hat{\theta}-\mathcal{Z}\|^2]=\mathbb{E}[\hat{R}_\text{SURE}]
        \end{aligned}
        \nonumber
    \end{equation}
\end{proof}

\subsubsection{James and Stein Estimator}
\textbf{Note}: Here, we consider $\mathcal{Z}\sim \mathcal{N}(\theta,\frac{\sigma^2}{N}I_K)$
\begin{theorem}[James and Stein (1961)]
    $$\hat{\theta}_{JS}(\mathcal{Z})=\left(1-\frac{(K-2)}{\mathcal{Z}'\mathcal{Z}}\frac{\sigma^2}{N}\right)\mathcal{Z}$$
\end{theorem}
We have, $g_{JS}(\mathcal{Z})=-\frac{(K-2)}{\mathcal{Z}'\mathcal{Z}}\frac{\sigma^2}{N}\mathcal{Z}$ and
\begin{equation}
    \begin{aligned}
        \sum_{k=1}^K \frac{\partial g_k(\mathcal{Z})}{\partial \mathcal{Z}_k}=-\frac{(K-2)}{\mathcal{Z}'\mathcal{Z}}\frac{\sigma^2}{N}\sum_{k=1}^K\left(1-\frac{2 \mathcal{Z}_k^2}{\mathcal{Z}'\mathcal{Z}}\right)=-\frac{(K-2)^2}{\mathcal{Z}'\mathcal{Z}}\frac{\sigma^2}{N}
    \end{aligned}
    \nonumber
\end{equation}
Hence, the corresponding SURE is
\begin{equation}
    \begin{aligned}
        \hat{R}_\text{SURE}(\mathcal{Z})&=\frac{K}{N}\sigma^2-\frac{2\sigma^2}{N}\frac{(K-2)^2}{\mathcal{Z}'\mathcal{Z}}\frac{\sigma^2}{N}+\frac{(K-2)^2}{(\mathcal{Z}'\mathcal{Z})^2}\left(\frac{\sigma^2}{N}\right)^2\sum_{k=1}^K \mathcal{Z}_k^2\\
        &=\frac{K}{N}\sigma^2-\frac{(K-2)^2}{\mathcal{Z}'\mathcal{Z}}\frac{\sigma^4}{N^2}
    \end{aligned}
    \nonumber
\end{equation}
Then,
\begin{equation}
    \begin{aligned}
        R(\hat{\theta}_{JS},\theta)&=\mathbb{E}[\hat{R}_\text{SURE}(\mathcal{Z})]\\
        &=\frac{K}{N}\sigma^2-(K-2)^2\frac{\sigma^4}{N^2}\mathbb{E}\left[\frac{1}{\mathcal{Z}'\mathcal{Z}}\right]
    \end{aligned}
    \tag{RJS}
    \label{RJS}
\end{equation}
As $\mathcal{Z}_k\sim \mathcal{N}(\theta_k,\frac{\sigma^2}{N})$, $\mathcal{Z}'\mathcal{Z}=\sum_{k=1}^K \mathcal{Z}_k^2\sim \frac{\sigma^2}{N}V$ such that $V\sim \chi^2_{K+2W}$ where $W\sim \text{Poisson}(\frac{\rho}{2})$ and $\rho=N\sum_{k=1}^K \frac{\theta_K^2}{\sigma^2}$. So,
\begin{equation}
    \begin{aligned}
        \mathbb{E}\left[\frac{1}{\mathcal{Z}'\mathcal{Z}}\right]&=\frac{N}{\sigma^2}\mathbb{E}\left[\frac{1}{V}\right]\\
        &=\frac{N}{\sigma^2}\mathbb{E}\left[\frac{1}{K-2+2W}\right] \text{ (by the identity of chi-square distribution)}\\
        &\geq \frac{N}{\sigma^2}\frac{1}{K-2+\rho} \text{ (by Jensen's inequality)}\\
        &=\frac{1}{(K-2)\frac{\sigma^2}{N}+\|\theta\|_2^2}
    \end{aligned}
    \nonumber
\end{equation}
Substitute it into \ref{RJS},
\begin{equation}
    \begin{aligned}
        R(\hat{\theta}_{JS},\theta)&\leq \frac{K}{N}\sigma^2-\frac{(K-2)^2\frac{\sigma^4}{N^2}}{(K-2)\frac{\sigma^2}{N}+\|\theta\|_2^2}
    \end{aligned}
    \nonumber
\end{equation}
Hence,
\begin{equation}
    \begin{aligned}
        R(\hat{\theta}_{MLE},\theta)-R(\hat{\theta}_{JS},\theta)\geq \frac{(K-2)^2\frac{\sigma^4}{N^2}}{(K-2)\frac{\sigma^2}{N}+\|\theta\|_2^2}\geq 0
    \end{aligned}
    \nonumber
\end{equation}
which shows that \underline{$\hat{\theta}_{JS}$ works better than $\hat{\theta}_{MLE}$} (ML is inadmissable) under squared error loss.


\subsubsection{A more general form of estimator $\mathcal{L}=\{C \mathbf{Z}:C=\text{diag}\vec{c},\vec{c}\in[0,1]^K\}$}
Consider a new estimator $\hat{\theta}\in \mathcal{L}=\{C \mathbf{Z}:C=\text{diag}\vec{c},\vec{c}\in[0,1]^K\}$. The SURE is
\begin{equation}
    \begin{aligned}
        \hat{R}_\text{SURE}\left(\mathbf{Z},\vec{c}\right)=\frac{\sigma^2}{N}\sum_{k=1}^K c_k^2+\sum_{k=1}^K\left(\mathcal{Z}_k^2-\frac{\sigma^2}{N}\right)(1-c_k)^2
    \end{aligned}
    \nonumber
\end{equation}
Empirical Risk Minimization: taking F.O.C.
\begin{equation}
    \begin{aligned}
        \hat{c}_k=\left(1-\frac{\sigma^2}{N}\frac{1}{\mathcal{Z}_k^2}\right),\ k=1,...,K
    \end{aligned}
    \nonumber
\end{equation}



\chapter{Linear Predictors / Regression}
\section{Best Linear Predictor}
Consider a prediction problem that the distribution $F_{X,Y}$ is known, we observe $X=\begin{pmatrix}
    1\\
    R
\end{pmatrix}\in \mathbb{R}^{K\times 1}$ and predict $Y\in \mathbb{R}$. Only linear functions of $X$ are allowed $\mathcal{L}=\{X'b:b\in \mathbb{R}^K\}$. We use square experience loss $(Y-X'b)^2$. We want to minimze Risk (mean squared error) $$\mathbb{E}_{X,Y}[(Y-X'b)^2]=\int_{x,y}(y-x'b)^2f_{x,y}(x,y)dxdy$$

\begin{assumption}
    Following inference is based on assumptions:
    \begin{enumerate}[(i).]
        \item $\mathbb{E}[Y^2]<\infty$;
        \item $\mathbb{E}[\|X\|^2]<\infty$ (Frobenius norm);
        \item $\mathbb{E}[(\alpha'X)^2]>0$ for any non-zero $\alpha\in \mathbb{R}^K$.
    \end{enumerate}
\end{assumption}

Let $\beta_0=\arg\min_{b\in \mathbb{R}^k}\mathbb{E}_{X,Y}[(Y-X'b)^2]$. By the F.O.C.
\begin{equation}
    \begin{aligned}
        \mathbb{E}[X(Y-X'\beta_0)]=0\\
        \mathbb{E}[XY]-\mathbb{E}[XX']\beta_0=0\\
        \mathbb{E}[XY]=\underbrace{\mathbb{E}[XX']}_{non-singular}\beta_0\\
        \beta_0=\mathbb{E}[XX']^{-1}\mathbb{E}[XY]
    \end{aligned}
    \nonumber
\end{equation}
\begin{proposition}[Best Linear Predictor]
    Hence, the mean-squared error minimizing linear predictor of $Y$ given $X$ is
    \begin{equation}
        \begin{aligned}
            \mathbb{E}^*[Y|X]=X'\beta_0, \text{ where }\beta_0=\mathbb{E}[XX']^{-1}\mathbb{E}[XY]
        \end{aligned}
        \nonumber
    \end{equation}
\end{proposition}
\begin{equation}
    \begin{aligned}
        \mathbb{E}_{X,Y}[X(\underbrace{Y-X'\beta_0}_{\triangleq u})]=\begin{pmatrix}
            \mathbb{E}[u]\\
            \mathbb{E}[uR]
        \end{pmatrix}=\boldsymbol{0}
    \end{aligned}
    \nonumber
\end{equation}
Hence, we have $\mathbb{E}[u]=0$, then $\mathbb{E}[uR]=0=\text{Cov}(u,R)$.
\begin{lemma}
    $\mathbb{E}[u]=\mathbb{E}[uR]=\text{Cov}(u,R)=0$, where $u=Y-\mathbb{E}^*[Y|X]$.
\end{lemma}
If $u>0$, it is underpredicting and if $u<0$, it is overpredicting.

\subsection*{Result 1 (ure Partitioned Inverse Formula)}
When we separate the constant term from other variables, we can write the \underline{Best Linear Predictor} as:
\begin{proposition}[Best Linear Predictor (ure Partitioned Inverse Formula)]
    $X=\begin{pmatrix}
        1\\
        R
    \end{pmatrix}$, $\beta_0=\begin{pmatrix}
        \alpha_0\\
        \beta_*
    \end{pmatrix}$, $\mathbb{E}[XX']^{-1}=\begin{bmatrix}
        1	& \mathbb{E}[R]'\\
        \mathbb{E}[R]	&\mathbb{E}[RR']
    \end{bmatrix}^{-1}$, $\mathbb{E}[XY]=\begin{pmatrix}
        \mathbb{E}[Y]\\
        \mathbb{E}[RY]
    \end{pmatrix}$. Then,
    \begin{equation}
        \begin{aligned}
            \alpha_0=\mathbb{E}[Y]-\mathbb{E}[R]'\beta_*\\
            \beta_*=\underbrace{\text{Var}(R)^{-1}}_{(K-1)\times(K-1)}\times \underbrace{\text{Cov}(R,Y)}_{(K-1)\times 1}
        \end{aligned}
        \nonumber
    \end{equation}
\end{proposition}

\section{Convergence of OLS}
\subsection{Approximation}
OLS Fit is
\begin{equation}
    \begin{aligned}
        \hat{\beta}=\left[\frac{1}{N}\sum_{i=1}^N X_i X'_i\right]^{-1}\left[\frac{1}{N}\sum_{i=1}^N X_iY_i\right]
    \end{aligned}
    \nonumber
\end{equation}
\begin{theorem}[Weak Law of Large Numbers (wLLN)]
    The weak law of large numbers (also called Khinchin's law) states that the sample average \underline{converges in probability} towards the expected value.
    $${\displaystyle {\begin{matrix}{}\\{\overline {X}}_{n}\ {\xrightarrow {P}}\ \mu \qquad {\text{when}}\ n\to \infty .\\{}\end{matrix}}}$$
    That is, for any positive number $\varepsilon$,
    $${\displaystyle \lim _{n\to \infty }\Pr \!\left(\,|{\overline {X}}_{n}-\mu |<\varepsilon \,\right)=1.}$$
\end{theorem}
\begin{enumerate}
    \item By LLN: $\frac{1}{N}\sum_{i=1}^N X_iY_i\xrightarrow{P} \mathbb{E}[XY]$
    \item By LLN and $f(X)=X^{-1}$ is continuous, $\left[\frac{1}{N}\sum_{i=1}^N X_iX'_i\right]\xrightarrow{P} \mathbb{E}[XX']^{-1}$
    \item Hence, $$\hat{\beta}=\left[\frac{1}{N}\sum_{i=1}^N X_i X'_i\right]^{-1}\left[\frac{1}{N}\sum_{i=1}^N X_iY_i\right] \xrightarrow{P} \mathbb{E}[XX']^{-1}\mathbb{E}[XY]= \beta_0$$
\end{enumerate}

\begin{theorem}[Central Limit Theorem (CLT)]\label{CLT}
    $$Z=\frac{\overline{X}-\mu}{\frac{\sigma}{\sqrt{n}}} \xrightarrow {D} N(0,1) \text{ when}\ n\to \infty$$
    $Z$ \underline{converges in distribution} to $N(0,1)$ as $n\to \infty$

    (converges in distribution: $P(\frac{\overline{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\leq a)\rightarrow \frac{1}{\sqrt{2\pi}}\int_{-\infty}^ae^{-\frac{x^2}{2}}dx$)
\end{theorem}
Application to OLS: Let $u=Y-X'\beta_0$. Then,
\begin{equation}
    \begin{aligned}
        \hat{\beta}&=\left[\frac{1}{N}\sum_{i=1}^N X_i X'_i\right]^{-1}\left[\frac{1}{N}\sum_{i=1}^N X_iY_i\right]\\
        &=\left[\frac{1}{N}\sum_{i=1}^N X_i X'_i\right]^{-1}\left[\frac{1}{N}\sum_{i=1}^N X_i(u_i+X'_i\beta_0)\right]\\
        &=\beta_0+\left[\frac{1}{N}\sum_{i=1}^N X_i X'_i\right]^{-1}\left[\frac{1}{\sqrt{N}}\sum_{i=1}^N X_i u_i\right]
    \end{aligned}
    \nonumber
\end{equation}
Then,
\begin{equation}
    \begin{aligned}
        \sqrt{N}(\hat{\beta}-\beta_0)&=\left[\frac{1}{N}\sum_{i=1}^N X_iX
        '_i\right]^{-1}\left[\frac{1}{\sqrt{N}}\sum_{i=1}^N X_i u_i\right]
    \end{aligned}
    \nonumber
\end{equation}
\begin{enumerate}
    \item By LLN, $\left[\frac{1}{N}\sum_{i=1}^N X_iX
    '_i\right]^{-1} \xrightarrow{P} \mathbb{E}[XX']^{-1}\triangleq \Gamma_0^{-1}$.
    \item By CLT, $\left[\frac{1}{\sqrt{N}}\sum_{i=1}^N X_i u_i\right]\sim \mathcal{N}(0,\Omega_0)$, where
    \begin{equation}
        \begin{aligned}
            \Omega_0=Var[X_iu_i]=\mathbb{E}[\|X_iu_i\|^2]=\mathbb{E}[\|x_i\|^2u_i^2]\leq \left(\mathbb{E}[\|x_i\|^4]\right)^\frac{1}{2}\mathbb{E}[u_i^4]^{\frac{1}{2}}
        \end{aligned}
        \nonumber
    \end{equation}
    Hence,
    \begin{equation}
        \begin{aligned}
            \sqrt{N}(\hat{\beta}-\beta_0) \xrightarrow{D} N\left(0,\Gamma_0^{-1}\Omega_0 \Gamma_0^{-1}\right)
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}
The estimation of $\Gamma_0$ and $\Omega_0$:
\begin{equation}
    \begin{aligned}
        \hat{\Gamma}&=\frac{1}{N}\sum_{i=1}^N X_i X'_i\\
        \hat{\Omega}&=\frac{1}{N}\sum_{i=1}^N X_i\hat{u_i}\hat{u_i}'X'_i,\quad \text{where }\hat{u_i}=Y_i-X'_i\hat{\beta}
    \end{aligned}
    \nonumber
\end{equation}
We have
\begin{equation}
    \begin{aligned}
        \hat{\Gamma}^{-1}\hat{\Omega}\hat{\Gamma}^{-1}\xrightarrow{P} \Gamma_0^{-1}\Omega_0 \Gamma_0^{-1}
    \end{aligned}
    \nonumber
\end{equation}
Then,
\begin{equation}
    \begin{aligned}
        \hat{\beta}\xrightarrow{approx}N\left(\beta_0,\frac{\hat{\Gamma}^{-1}\hat{\Omega}\hat{\Gamma}^{-1}}{N}\right)
    \end{aligned}
    \nonumber
\end{equation}

\subsection{Testing and Confidence Interval}
Let $\hat{\Lambda}=\hat{\Gamma}^{-1}\hat{\Omega}\hat{\Gamma}^{-1}$, $\Lambda=\Gamma_0^{-1}\Omega_0 \Gamma_0^{-1}$, $\sqrt{N}(\hat{\beta}_k-\beta_k) \xrightarrow{D} N\left(0,\Lambda_{kk}\right)$. Hence,
\begin{equation}
    \begin{aligned}
        T_N\triangleq \sqrt{N}\Lambda_{kk}^{-\frac{1}{2}}\left(\hat{\beta}_k-\beta_k\right)\xrightarrow{D} N(0,1)
    \end{aligned}
    \nonumber
\end{equation}

Consider the event $A=\mathbf{1}\left\{|T_N|\leq 1.96\right\}$. We have
\begin{equation}
    \begin{aligned}
        \text{Pr}(A=1)=\Phi(1.96)-\Phi(-1.96)=0.95
    \end{aligned}
    \nonumber
\end{equation}
Specifically,
\begin{equation}
    \begin{aligned}
        A&=\mathbf{1}\left\{|T_N|\leq 1.96\right\}\\
        &=\mathbf{1}\left\{\hat{\beta}_k-1.96\frac{\Lambda_{kk}^{\frac{1}{2}}}{\sqrt{N}}\leq\beta_k\leq \hat{\beta}_k+1.96\frac{\Lambda_{kk}^{\frac{1}{2}}}{\sqrt{N}}\right\}
    \end{aligned}
    \nonumber
\end{equation}
The ``Random Interval'' is
\begin{equation}
    \begin{aligned}
        \left[\hat{\beta}_k-1.96\frac{\Lambda_{kk}^{\frac{1}{2}}}{\sqrt{N}}, \hat{\beta}_k+1.96\frac{\Lambda_{kk}^{\frac{1}{2}}}{\sqrt{N}}\right]
    \end{aligned}
    \nonumber
\end{equation}


\subsection*{Testing Linear Restrictions}
Let $\theta=H\beta$, where $H$ is $p\times k$ and $\beta$ is $k\times 1$.
\begin{equation}
    \begin{aligned}
        H_0: \theta=\theta_0;\quad H_1:\theta\neq \theta_0
    \end{aligned}
    \nonumber
\end{equation}
We have
\begin{equation}
    \begin{aligned}
        \sqrt{N}(\hat{\theta}-\theta_0)=H\sqrt{N}\left(\hat{\beta}-\beta_0\right)\xrightarrow[H_0]{D} N(0,H\Lambda_0 H')
    \end{aligned}
    \nonumber
\end{equation}
Moreover,
\begin{equation}
    \begin{aligned}
        W_0=N\left(\hat{\theta}-\theta_0\right)(H\Lambda_0 H')^{-1}\left(\hat{\theta}-\theta_0\right)\xrightarrow[H_0]{D}\chi_p^2
    \end{aligned}
    \nonumber
\end{equation}
where $\mathbb{E}[\chi_p^2]=p$.


\section{Long, Short, Auxilary Regression}
$Y\in \mathbb{R}^{1}$, $X\in \mathbb{R}^{K}$, $K\in \mathbb{R}^{J}$.
Consider a researcher interested in the conditional distribution of the logarithm of weekly wages ($Y\in \mathbb{R}^{1}$) given years of competed schooling ($X\in \mathbb{R}^{K}$) and vector of additional worker attributes. This vector could include variables such as age, childhood test scores, and race. Let $W$ be this $J \times 1$ vector of additional variables.

We can run regression by two ways:
\begin{enumerate}
    \item Long regression: $\mathbb{E}^*[Y|X,W]=X'\beta_0+W'\gamma_0$.
    \item Short regression: $\mathbb{E}^*[Y|X]=X'b_0$.
\end{enumerate}
\begin{proposition}[Long Regression]
    Long regression is another form of best linear predictor.
    \begin{equation}
        \begin{aligned}
            \mathbb{E}^*[Y|X,W]&=\mathbb{E}^*[Y|Z]\\
            &=Z'\left(\mathbb{E}[ZZ']^{-1}\mathbb{E}[ZY]\right)\\
            &=X'\beta_0+W'\gamma_0
        \end{aligned}
        \nonumber
    \end{equation}
    where $\begin{pmatrix}
        \beta_0\\
        \gamma_0
    \end{pmatrix}=\mathbb{E}[ZZ']^{-1}\mathbb{E}[ZY]$, $Z=\begin{pmatrix}
        X\\
        W
    \end{pmatrix}$.
\end{proposition}

\begin{proposition}[Auxiliary Regression]
    $$\mathbb{E}^*[W|X]=\Pi_0 X$$
    which is multivariate regression. For each row $j=1,...,J$, $$\mathbb{E}^*[W_j|X]=X'\Pi_{j0}$$
    where $\Pi_{j0}=\mathbb{E}[XX']^{-1}\mathbb{E}[XW_j]$ and $\Pi_0=\begin{pmatrix}
        \Pi_{10}'\\
        \vdots\\
        \Pi_{J0}'
    \end{pmatrix}=\mathbb{E}[WX']\mathbb{E}[XX']^{-1}$.
\end{proposition}
\begin{theorem}[Law of Iterated Linear Predictors (LILP)]
    $$\mathbb{E}^*[Y|X]=\mathbb{E}^*[\mathbb{E}^*[Y|X,W]|X]$$
\end{theorem}
\underline{Facts:} Linear predictor is linear operator, $\mathbb{E}^*[X+Y|W]=\mathbb{E}^*[X|W]+\mathbb{E}^*[Y|W]$.\\
Let $Y=\mathbb{E}^*[Y|X,W]+u=X'\beta_0+W'\gamma_0+u$. Then,
\begin{equation}
    \begin{aligned}
        \mathbb{E}^*[Y|X]&=\mathbb{E}^*[X'\beta_0+W'\gamma_0+u|X]\\
        &=\mathbb{E}^*[X'\beta_0|X]+\mathbb{E}^*[W'\gamma_0|X]+\mathbb{E}^*[u|X]\\
        &=X'\beta_0+(\Pi_0X)'\gamma_0+0\\
        &=X'(\underbrace{\beta_0+\Pi_0'\gamma_0}_{b_0})
    \end{aligned}
    \nonumber
\end{equation}
\begin{proposition}[Short Regression]
    $$\mathbb{E}^*[Y|X]=X'b_0$$
    where $b_0=\beta_0+\Pi_0'\gamma_0$.
\end{proposition}

\section{Residual Regression}
Let the variation in $W$ unexplained by $X$.
\begin{equation}
    \begin{aligned}
        \underbrace{V}_{J\times 1}=\underbrace{W}_{J\times 1}-\underbrace{\mathbb{E}^*[W|X]}_{J\times 1}=W-\Pi_0X
    \end{aligned}
    \nonumber
\end{equation}
\begin{proposition}[Residual Regression]
    Let $\tilde{Y}=Y-\mathbb{E}^*[Y|X]$,
    $$\mathbb{E}^*[\tilde{Y}|V]=V'\gamma_0$$
\end{proposition}
\begin{proof}
    \begin{equation}
        \begin{aligned}
            Y&=X'\beta_0+W'\gamma_0+u\\
            \tilde{Y}&=X'\beta_0-\mathbb{E}^*[Y|X]+W'\gamma_0+u\\
            &=-X'(\Pi'_0\gamma_0)+W'\gamma_0+u\\
            &=V'\gamma_0+u\\
            \mathbb{E}^*[\tilde{Y}|V]&=V'\gamma_0
        \end{aligned}
        \nonumber
    \end{equation}
\end{proof}
By long regression,
\begin{equation}
    \begin{aligned}
        \mathbb{E}^*[Y|X,W]&=X'\beta_0+W'\gamma_0\\
        &=X'b_0-X'(\Pi'_0\gamma_0)+W'\gamma_0\\
        &=X'b_0+V'\gamma_0\\
        &=\mathbb{E}^*[Y|X]+\mathbb{E}^*[\tilde{Y}|V]
    \end{aligned}
    \nonumber
\end{equation}

\begin{theorem}[Frisch-Waugh Theorem]
    \begin{equation}
        \begin{aligned}
            \mathbb{E}^*[Y|X,V]&=\mathbb{E}^*[Y|X]+\mathbb{E}^*[Y|V]-\mathbb{E}[Y]\\
            &=\mathbb{E}^*[Y|X,W]
        \end{aligned}
        \nonumber
    \end{equation}
\end{theorem}


\begin{lemma}
    If $Cov(X,W)=0$, then
    \begin{equation}
        \begin{aligned}
            \mathbb{E}^*[Y|X,W]=\mathbb{E}^*[Y|X]+\mathbb{E}^*[Y|W]-\mathbb{E}[Y]
        \end{aligned}
        \nonumber
    \end{equation}
\end{lemma}
\begin{proof}
    Let $u=Y-\mathbb{E}^*[Y|X,W]$.
    \begin{equation}
        \begin{aligned}
            0&=\mathbb{E}[uW]\\
            &=\mathbb{E}[(Y-\mathbb{E}^*[Y|X]-\mathbb{E}^*[Y|W]+\mathbb{E}[Y])W]\\
            &=\underbrace{\mathbb{E}[(Y-\mathbb{E}^*[Y|W])W]}_{=0 \text{ by F.O.C.}}-\underbrace{\mathbb{E}[\mathbb{E}^*[Y|X]]}_{=\mathbb{E}[Y]}\mathbb{E}[W]+\mathbb{E}[Y]\mathbb{E}[W]
        \end{aligned}
        \nonumber
    \end{equation}
\end{proof}


\section{Card-Krueger Model}
Consider a model about log-learning based on schooling, ability, luck.
\begin{equation}
    \begin{aligned}
        Y(s)=\alpha_0+\beta_0 \underbrace{s}_{\text{schooling }s\in \mathbb{S}} +\underbrace{A}_\text{ability} + \underbrace{V}_\text{luck}
    \end{aligned}
    \nonumber
\end{equation}
Given a cost function about $s$:
\begin{equation}
    \begin{aligned}
        C(s)=\underbrace{C}_\text{cost heterogeneity}s+\frac{k_0}{2}s^2
    \end{aligned}
    \nonumber
\end{equation}
\begin{assumption}
    We assume
    \begin{enumerate}
        \item Information set $I_0=(C,A)$ are known by agent when choosing schooling.
        \item $V$ is independent of $C,A$: $V|C,A\triangleq V$.
    \end{enumerate}
\end{assumption}
Then, the observed schooling $s$ should satsify
\begin{equation}
    \begin{aligned}
        s&=\arg\max_s \mathbb{E}[Y(s)-C(s)\mid I_0]\\
        &=\arg\max_s \alpha_0+\beta_0 s+A-Cs-\frac{k_0}{2}s^2
    \end{aligned}
    \nonumber
\end{equation}
By F.O.C.
\begin{equation}
    \begin{aligned}
        \beta_0-C-k_0 s=0 \Rightarrow s=\frac{\beta_0-C}{k_0}
    \end{aligned}
    \nonumber
\end{equation}
\begin{enumerate}
    \item \textbf{Long Regression}:
    \begin{equation}
        \begin{aligned}
            \mathbb{E}^*[Y|s,A]=\alpha_0+\beta_0s+A
        \end{aligned}
        \tag{LR}
        \label{LR}
    \end{equation}
    \item Short Regression: $$\mathbb{E}^*[Y|s]=a_0+b_0s$$
    \item \textbf{Auxillary Regression}: By the best linear predictor, the $\mathbb{E}^*[A|s]$ can be written as
    \begin{equation}
        \begin{aligned}
            \mathbb{E}^*[A|s]&=\mathbb{E}[A]-\frac{\text{Cov}(A,s)}{\text{Var}(s)}\mathbb{E}[s]+\frac{\text{Cov}(A,s)}{\text{Var}(s)}s\\
            &=\mathbb{E}[A]-\eta_0\mathbb{E}[s]+\eta_0 s
        \end{aligned}
        \tag{AR}
        \label{AR}
    \end{equation}
    where $\eta_0=\frac{\text{Cov}(A,s)}{\text{Var}(s)}$ and $s=\frac{\beta_0-C}{k_0}$ and $\mathbb{E}[s]=\frac{\beta_0-\mu_C}{k_0}$,
    \begin{equation}
        \begin{aligned}
            \text{Cov}(A,s)&=\text{Cov}\left(A,\frac{\beta_0-C}{k_0}\right)=-\frac{\text{Cov}(A,C)}{k_0}=-\frac{\sigma_{AC}}{k_0}\\
            \text{Var}(s)&=\text{Var}\left(\frac{\beta_0-C}{k_0}\right)=\frac{\sigma_C^2}{k_0^2}\\
            \eta_0&=-k_0\frac{\sigma_{AC}}{\sigma_C^2}=-k_0\frac{\sigma_{AC}}{\sigma_A\sigma_C}\frac{\sigma_A}{\sigma_C}=-k_0\rho_{AC}\frac{\sigma_A}{\sigma_C}
        \end{aligned}
        \nonumber
    \end{equation}
    The Auxillary Regression is written as
    \begin{equation}
        \begin{aligned}
            \mathbb{E}^*[A|s]=\mathbb{E}[A]+k_0\rho_{AC}\frac{\sigma_A}{\sigma_C}\frac{\beta_0-\mu_C}{k_0}-k_0\rho_{AC}\frac{\sigma_A}{\sigma_C} s\\
            =\mathbb{E}[A]+\rho_{AC}\frac{\sigma_A}{\sigma_C}(\beta_0-\mu_C)-k_0\rho_{AC}\frac{\sigma_A}{\sigma_C} s
        \end{aligned}
        \tag{AR-1}
        \label{AR-1}
    \end{equation}
    Hence, the \textbf{Short Regression}
    \begin{equation}
        \begin{aligned}
            \mathbb{E}^*[Y|s]&=\mathbb{E}^*\left[\mathbb{E}^*[Y|s,A]|s\right]\\
            &=\mathbb{E}^*\left[\alpha_0+\beta_0s+A|s\right]\\
            &=\alpha_0+\beta_0s+\mathbb{E}^*[A|s]\\
            &=\underbrace{\alpha_0+\mathbb{E}[A]+\rho_{AC}\frac{\sigma_A}{\sigma_C}(\beta_0-\mu_C)}_{a_0}+\underbrace{\left(\beta_0-k_0\rho_{AC}\frac{\sigma_A}{\sigma_C}\right)}_{b_0}s
        \end{aligned}
        \tag{SR}
        \label{SR}
    \end{equation}
\end{enumerate}

\subsection{Proxy Variable Regression}
What if we don't observe $A$ or $C$. We observe some observed variables $W$ (\textbf{proxy variable}) instead.
\begin{assumption}
    We assume
    \begin{enumerate}
        \item Redundancy: $\mathbb{E}^*[Y|s,A,W]=\mathbb{E}^*[Y|s,A]$ ($W$ doesn't give extra information).
        \item Conditional Uncorrelatedness: $\mathbb{E}^*[A|s,W]=\mathbb{E}^*[A|W]=\Pi_0+W'\Pi_W$ (Auxillary Regression).
        \item Conditional Independence: $C\perp A | W=w$.
    \end{enumerate}
\end{assumption}
The \textbf{Proxy Variable Regression} is given by
\begin{equation}
    \begin{aligned}
        \mathbb{E}^*[Y|s,W]&=\mathbb{E}^*\left[\mathbb{E}^*[Y|s,A,W]|s,W\right]\\
        &=\mathbb{E}^*\left[\mathbb{E}^*[Y|s,A]|s,W\right]\\
        &=\mathbb{E}^*[\alpha_0+\beta_0s+A|s,W]\\
        &=\alpha_0+\beta_0s+(\Pi_0+W'\Pi_W)\\
        &=(\alpha_0+\Pi_0)+\beta_0 s+ W'\Pi_W
    \end{aligned}
    \tag{PVR}
    \label{PVR}
\end{equation}
A \underline{general form} of \textbf{Proxy Variable Regression} with
\begin{enumerate}
    \item Long Regression: $\mathbb{E}^*[Y|X,A]=X'\beta_0+A'\gamma_0$
    \item Redundancy: $\mathbb{E}^*[Y|X,A,W]=\mathbb{E}^*[Y|X,A]$
    \item Conditional Uncorrelatedness: $\mathbb{E}^*[A|X,W]=\mathbb{E}^*[A|W]=\Pi_0 W$\\
    where $\Pi_0$ is $P\times J$, $W$ is $J\times 1$, and $A$ is $P\times 1$.
\end{enumerate}
\begin{equation}
    \begin{aligned}
        \mathbb{E}^*[Y|X,W]&=\mathbb{E}^*\left[\mathbb{E}^*[Y|X,A,W]|X,W\right]\\
        &=\mathbb{E}^*\left[\mathbb{E}^*[Y|X,A]|X,W\right]\\
        &=\mathbb{E}^*\left[X'\beta_0+A'\gamma_0|X,W\right]\\
        &=X'\beta_0+\mathbb{E}^*[A|X,W]'\gamma_0\\
        &=X'\beta_0+W'\Pi'_0\gamma_0
    \end{aligned}
    \nonumber
\end{equation}


\chapter{Machine Learning in Inference}
Instead of given a prior distribution of $Y$, we are given a \textbf{training set} $T=(X_i,Y_i)_{i=1}^n$ where i.i.d. $(X_i,Y_i)\sim P$. (Distribution $P$ is unknown).

Risk: $R(\delta)=\mathbb{E}_P\left[l(\delta(X),Y)\right]$

The ture optimal decision rule is $$\delta_B=\argmin_\delta \mathbb{E}_P\left[l(\delta(X),Y)\right]$$
which is can't be computed since we don't know how actually $P$ is.

\section{Empirical Risk Minimization (ERM)}
Instead of computing optimal decision rule with $P$, we compute the optimal decisioin rule in the training set:
$$\hat{\delta}_n=\argmin_\delta \frac{1}{n}\sum_{i=1}^nl(\delta(X_i),Y_i)$$
The corresponding risk is $R(\hat{\delta}_n)=\mathbb{E}_P\left[l(\hat{\delta}_n(X),Y)\right]$. $\Delta R(\hat{\delta}_n)=R(\hat{\delta}_n)- R(\delta)>0$ always holds.

\textbf{Consistency:} if $\Delta R(\hat{\delta}_n) \rightarrow 0$ as $n \rightarrow \infty$.

\subsection{Example: Linear MMSE (LMMSE) estimator}
Use the decision role in the class of $\delta(x)=wx$. To find the linear MMSE (LMMSE) estimation $\delta^*(x)=w^*x$:
\begin{equation}
    \begin{aligned}
        w^*=\argmin_{w}\mathbb{E}_P\left[(wX-Y)^2\right]=\frac{\mathbb{E}[XY]}{\mathbb{E}[X^2]}
    \end{aligned}
    \nonumber
\end{equation}
The rule that minimizes the \textbf{empirical risk} is
\begin{equation}
    \begin{aligned}
        \hat{w}=\argmin_{w}\frac{1}{n}\sum_{i=1}^n(wX_i-Y_i)^2=\frac{\frac{1}{n}\sum_{i=1}^nX_iY_i}{\frac{1}{n}\sum_{i=1}^nX_i^2}
    \end{aligned}
    \nonumber
\end{equation}
The risk of the optimal rule $\delta^*=w^*x$ is $R(\delta^*)$ and the empirical risk under rule $\hat{\delta}(x)=\hat{w}x$ is $R(\hat{\delta}(x))$. $R(\hat{\delta})>R(\delta^*)$ always holds, and $$R(\hat{\delta})\rightarrow R(\delta^*)\text{ as }n \rightarrow \infty$$
According to CLT:
\begin{equation}
    \begin{aligned}
        \sqrt{n}\left(\frac{1}{n}\sum_iX_iY_i- \mathbb{E}(XY)\right) &\stackrel{d}{\longrightarrow} N(0,\sigma^2)\\
        \sqrt{n}\left(\frac{1}{n}\sum_iX_i^2- \mathbb{E}(X^2)\right) &\stackrel{d}{\longrightarrow} N(0,\sigma^2)
    \end{aligned}
    \nonumber
\end{equation}
Then,
\begin{equation}
    \begin{aligned}
        \frac{1}{n}\sum_iX_i^2=\mathbb{E}(X^2)+O(\frac{1}{\sqrt{n}})\\
        \frac{1}{n}\sum_iX_iY_i=\mathbb{E}(XY)+O(\frac{1}{\sqrt{n}})\\
    \end{aligned}
    \nonumber
\end{equation}
which means the error of estimators
\begin{equation}
    \begin{aligned}
        \hat{w}=\frac{\mathbb{E}(X^2)+O(\frac{1}{\sqrt{n}})}{\mathbb{E}(XY)+O(\frac{1}{\sqrt{n}})}=w^*+O(\frac{1}{\sqrt{n}})
    \end{aligned}
    \nonumber
\end{equation}
$$\hat{w}-w^*=O(\frac{1}{\sqrt{n}})$$
and the error of the risks:
\begin{equation}
    \begin{aligned}
        R(\hat{\delta})-R(\delta^*)&=\mathbb{E}_P[\hat{w}X-Y]^2-\mathbb{E}_P[w^*X-Y]^2\\
        &=\mathbb{E}_P[(\hat{w}-w^*)X+w^*X-Y]^2-\mathbb{E}_P[w^*X-Y]^2\\
        &=\mathbb{E}_P[(\hat{w}-w^*)X]^2+2(\hat{w}-w^*)\mathbb{E}_P[X(w^*X-Y)]\\
        &=(\hat{w}-w^*)\mathbb{E}_P(X^2)=O\left(\frac{1}{n}\right)
    \end{aligned}
    \nonumber
\end{equation}

\textbf{\underline{Complexity}:}
\begin{definition}
    A sequence $f(n)$ is $O(1)$ if $\lim_{n \rightarrow \infty}f(n)<\infty$.
\end{definition}

\begin{definition}
    A sequence $f(n)$ is $O(g(n))$ if $\frac{f(n)}{g(n)}$ is $O(1)$.
\end{definition}

\begin{definition}
    A sequence $f(n)$ is $o(1)$ if $\lim_{n \rightarrow \infty}\sup f(n)=0$.
\end{definition}

\begin{definition}
    A sequence $f(n)$ is $o(g(n))$ if $\lim_{n \rightarrow \infty}\sup \frac{f(n)}{g(n)}=0$.
\end{definition}

\begin{definition}
    A sequence $f(n)$ is \underline{asymptotic} to $g(n)$ if $\lim_{n \rightarrow \infty} \frac{f(n)}{g(n)}=1$. (This is denoted by $f(n)\sim g(n)$ as $a \rightarrow \infty$)
\end{definition}

\subsection{Penalized ERM}
$\delta(x)=\sum_{j=1}^Jw_jx^j$

Pick $J=d$ and use ERM with $d$ dimensional $w$: $$\argmin_{w\in \mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n[w^TX_i-Y_i]^2$$

Approach 1: Fix $d<<n$, use ERM.

Approach 2: (\textbf{Penalized ERM}) $$\min_\delta [R_{emp}(\delta)+J(\delta)]$$ ($J(\delta)$ is regularization (penalty) term)

\section{Stochastic Approximation}
Robbins and Monro (95)

\underline{Problem:} Find a root of function $h(x)$. ($f(x)=0$)

We do not observe $h(x)$ directly, but we observe $Y\sim P_x$ with
\begin{enumerate}[(1)]
    \item $\mathbb{E}[Y|X=x]=h(x)$
    \item $(Y|X=x)-h(x)$ is bounded
\end{enumerate}
\begin{example}
    $Y=X+Z$ with $\mathbb{E}[Z]=0$ and $Z$ is bounded.
\end{example}
Assumptions: 1. $h'(x^*)>0$; 2. $x^*$ is the unique root of $h$.

\textbf{SA Algorithm}
\begin{enumerate}[$\bullet$]
    \item Pick Sequence $\{a_n\}$ such that $\sum_{n=1}^\infty a_n=\infty$ and $\sum_{n=1}^\infty a^2_n<\infty$ (should converge to $0$ but not too quick) e.g. $a_n=n^{-\alpha}$ when $\alpha\in (\frac{1}{2},1]$.
    \item Initialize $X_1$
    \item Update for $n=1,2,...$, $Y_n\sim P(\cdot|X=X_n)$ $$X_{n+1}=X_n-a_n Y_n$$ until convergence.
\end{enumerate}

\begin{theorem}
    Under these assumptions $$X_n \stackrel{m.s.}{\longrightarrow} x^*\text{ as }n \rightarrow \infty$$ i.e., $\mathbb{E}(X_n-x^*)^2 \rightarrow 0$ as $n \rightarrow \infty$.
\end{theorem}

\begin{enumerate}[$\bullet$]
    \item \textbf{\underline{Performance Measure}} (Convergence rate): the root mean squared error (RMSE) $e_n=\sqrt{\mathbb{E}[(X_n-x^*)^2]}$.
    \item \underline{Projections}: If $x$ is constrained to live in an interval $I$, the update rule becomes $$X_{n+1}=\text{Proj}_x[X_n-a_nY_n]$$
    \item \underline{Averaging}: $$\bar{X}_n=\frac{1}{n}\sum_{i=1}^nX_i=\frac{X_n}{n}+\bar{X}_{n-1}\frac{n-1}{n}$$(nicer graph) (The benefits of this smoothing operation are mostly seen in the initial stages of the SA recursion, and \underline{do not improve the convergence rate}.)
\end{enumerate}

\begin{example}
    Let $h(x) = x$, in which case $x^* = 0$. $Y_n=h(X_n)+Z_n=X_n+Z_n$ where noise $Z_n$ is independent of $Y_n$ with $\mathbb{E}[Z_n]=0,Var(Z_n)=1$ and $Z_n$ is bounded.
\end{example}
Then,
    \begin{equation}
        \begin{aligned}
            X_{n+1}&=X_n-a_n(X_n+Z_n)\\
            &=(1-a_n)X_n-a_n Z_n
        \end{aligned}
        \nonumber
    \end{equation}
The MSE,
\begin{equation}
    \begin{aligned}
        e_{n+1}^2&=\mathbb{E}(X_{n+1}-x^*)^2=\mathbb{E}(X_{n+1})^2\\
        &=\mathbb{E}[(1-a_n)X_n-a_n Z_n]^2\\
        &=(1-a_n)^2\mathbb{E}X_n^2+a_n^2\mathbb{E}Z_n^2\\
        &=(1-a_n)^2e_n^2+a_n^2
    \end{aligned}
    \nonumber
\end{equation}
Pick $a_n=n^{-\alpha}$, where $\alpha\in (\frac{1}{2},1]$
\begin{equation}
    \begin{aligned}
        \Rightarrow e_{n+1}^2=(1-n^{-\alpha})^2e_n^2+n^{-2\alpha}
    \end{aligned}
    \nonumber
\end{equation}
\begin{center}
    \fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\center{Guess: $e_n=\sqrt{c}n^{-\beta}+H.O.T$}
    \begin{equation}
        \begin{aligned}
            c(n+1)^{-2\beta}+H.O.T=(1-n^{-\alpha})^2cn^{-2\beta}+n^{-2\alpha}+H.O.T
        \end{aligned}
        \nonumber
    \end{equation}
    (where $(n+1)^{-2\beta}=n^{-2\beta}(1+\frac{1}{n})^{-2\beta}=n^{-2\beta}[1-2\beta n^{^{-1}}+O(n^{-2})]$, by Taylor)
    \begin{equation}
        \begin{aligned}
            cn^{-2\beta}-2c\beta n^{^{-1-2\beta}}+H.O.T&=(1-n^{-\alpha})^2cn^{-2\beta}+n^{-2\alpha}+H.O.T\\
            -2c\beta n^{^{-1-2\beta}}+H.O.T&=-2cn^{-\alpha-2\beta}+n^{-2\alpha}+H.O.T
        \end{aligned}
        \nonumber
    \end{equation}
    \textbf{(For $\alpha<1$)}, $-2c\beta n^{^{-1-2\beta}}$ is not dominant term.
    $$H.O.T=-2cn^{-\alpha-2\beta}+n^{-2\alpha}+H.O.T$$
    Identify Power: $2\alpha=\alpha+2\beta$ $\Rightarrow$ $\beta=\frac{\alpha}{2}$ and $c=\frac{1}{2}$

    \textbf{(For $\alpha=1$)}, there are three dominant terms.
    $$-2c\beta n^{^{-1-2\beta}}+H.O.T=-2cn^{-1-2\beta}+n^{-2}+H.O.T$$
    Identify Power: $2=1+2\beta$ $\Rightarrow$ $\beta=\frac{1}{2}$ and $-2c\beta=-2c+1\Rightarrow c=1$

    $$e_n^2\sim c n^{-2\beta}$$
    To let the convergence rate as fast as possible, we want the $\beta$ to be as large as possible. Since $\beta=\frac{\alpha}{2}$, we pick the highest $\alpha=1 \Rightarrow \beta=\frac{1}{2},c=1$.
    $$e_n=O(n^{-\frac{1}{2}})\text{ with }a_n\sim\frac{1}{n}$$

    }}
\end{center}

\begin{example}
    Let $h(x) = x^3$, in which case $x^* = 0$. $Y_n=h(X_n)+Z_n=X_n^2+Z_n$ where noise $Z_n$ is independent of $Y_n$ with $\mathbb{E}[Z_n]=0,Var(Z_n)=1$ and $Z_n$ is bounded.
\end{example}
Then,
    \begin{equation}
        \begin{aligned}
            X_{n+1}&=X_n-a_n(X^3_n+Z_n)\\
            &=X_n-a_nX^3_n-a_n Z_n
        \end{aligned}
        \nonumber
    \end{equation}
Pick $a_n=n^{-\alpha}$, $\alpha\in (\frac{1}{2},1]$ $\Rightarrow \beta=\frac{1}{6},\alpha=\frac{2}{3}$ $\Rightarrow e_n\sim O(n^{-\frac{1}{6}})$

\section{Stochastic Gradient Descent (SGD)}
Solve $\min_{x\in\mathbb{R}^n}f(x)$.

We only use a \textbf{noisy version} $g(x,z)$ of $f(x)$, where $\mathbb{E}_z[g(x,z)]=f(x)$.
\begin{equation}
    \begin{aligned}
        \mathbb{E}_z[\nabla_x g(x,z)]=\nabla_x \mathbb{E}_z[g(x,z)]=\nabla f(x)
    \end{aligned}
    \nonumber
\end{equation}
Also pick sequence $\{a_n\}$ such that $\sum_{n=1}^\infty a_n=\infty$ and $\sum_{n=1}^\infty a^2_n<\infty$.

\textbf{SGD}
\begin{enumerate}[$\bullet$]
    \item Initialize $X_1$
    \item Update for $n=1,2,...$, $$X_{n+1}=X_n-a_n \nabla g(X_n,Z_n)$$
\end{enumerate}

\begin{example}
    $f(x)=\frac{1}{2}x^2,x\in \mathbb{R}$. Let $Z$ be a random variable with $\mathbb{E}(Z)=0,Var(Z)=1$. $$g(x,Z)=\frac{1}{2}(x+Z)^2-\frac{1}{2}$$
    \begin{equation}
        \begin{aligned}
            \mathbb{E}[g(x,Z)]&=\frac{1}{2}x^2=f(x)\\
            \nabla_x g(x,Z)&=x+Z \Rightarrow 
            \mathbb{E}[\nabla_xg(x,Z)]=\nabla f(x)
        \end{aligned}
        \nonumber
    \end{equation}
    $$X_{n+1}=X_n-a_n(X_n+Z_n)$$
    which is the same as the stochastic approximation.
\end{example}
\textbf{Main Results:} (Suppose the unique minimum is $x^*$)
\begin{enumerate}[(1)]
    \item \underline{Convergence:} $e_n \rightarrow 0$ as $n \rightarrow \infty$.
    \item \underline{Convergence Rate:} To achieve $\mathbb{E}[f(X_n)]-f(x^*)<\varepsilon$, we need $n=O(\frac{1}{\varepsilon})$ if $f$ is twice continuously differentiable and strongly convex.
\end{enumerate}

GD has linear convergence $\Rightarrow$ $e_n=O(e^{-cn})$; Solve $\varepsilon=O(e^{-cn}) \Rightarrow n=O(\ln \frac{1}{\varepsilon})$. (\textbf{SGD is much worse than GD}, cost more.)

\section{SGD Application to Empirical Risk Minimization (ERM)}
ERM problem is
$$\min_{w\in \mathbb{R}^d}\frac{1}{n}\sum_{i=1}^nL(\delta_w(X_i),Y_i)$$
$R_{emp}(w)=\frac{1}{n}\sum_{i=1}^nL(\delta_w(X_i),Y_i)$ is the empirical risk (e.g. $\delta_w(x)=w^Tx$, $L(\hat{y},y)=(\hat{y}-y)^2$)
To make $w$ more visible, we can write
\begin{equation}
    \begin{aligned}
        R_{emp}(w)=\frac{1}{n}\sum_{i=1}^nL(\delta_w(X_i),Y_i)
        =\frac{1}{n}\sum_{i=1}^nQ(X_i,Y_i,w)
    \end{aligned}
    \nonumber
\end{equation}
For penalized ERM we would similarly have $$\min_{w\in \mathbb{R}^d}\frac{1}{n}\sum_{i=1}^nQ(X_i,Y_i,w)+J(w)$$ $J(w)$ is the penalty (regularization) term.

In the problem $\min_{W\in \mathbb{R}^n}R_{emp}(w)=\frac{1}{n}\sum_{i=1}^nQ(X_i,Y_i,w)$

\subsection{Different Gradient Descent for ERM}
\begin{enumerate}
    \item[\textbf{GD}]
    \begin{enumerate}[$\bullet$]
        \item Initialize $W_1$
        \item Update for $k\geq 1$,
        Update: $W_{k+1}=W_k-a_k \frac{1}{n}\sum_{i=1}^n\nabla Q(X_i,Y_i,W_k)$
    \end{enumerate}
    \textbf{Computational cost:} The computational cost of GD is $O(dn)$ operations per iteration. Since GD has exponential convergence, the number of iterations needed to reach an optimization error of $\rho$ is $O(\log\frac{1}{\rho})$. Hence GD incurs a \underline{total computational cost of $O(dn\log\frac{1}{\rho})$} to reach a solution $W_k$ such that $R_{emp}(W_k)\leq \min_WR_{emp}(W)+\rho$
    \item[\textbf{SGD}]
    \begin{enumerate}[$\bullet$]
        \item Initialize $W_1$
        \item Update for $k\geq 1$,
        \begin{enumerate}[{Step} 1:]
            \item Pick $i$ uniformly over $\{1,...,n\}$
            \item $W_{k+1}=W_k-a_k \nabla Q(X_i,Y_i,W_k)$
        \end{enumerate}
    \end{enumerate}
    \textbf{Computational cost:} After $k$ iterations, $\mathbb{E}[R_{emp}(W_k)]\leq \min_W R_{emp}(W)+\rho$, for $k=O(\frac{1}{\rho})$ and $f$ twice differentiable and strongly convex. The cost per iteration is $O(d)$ (independent of $n$), so the \underline{total computational cost is $O(\frac{d}{\varepsilon})$}.
\end{enumerate}

\subsection{Constraints on Learning Problem}
\textbf{Why achieving a low value of $\rho$ is useful (low error of $R_{emp}(\cdot)$), since the cost function $R_{emp}(\cdot)$ is only a surrogate for the actual risk $R(\cdot)$?}


Typically, $d=O(n^b)$ where $0<b<1$.

For any numerical algorithm producing a decision rule $\tilde{\delta_n}$, the excess risk (compared to Bayes rule $\delta_B$) can be expressed as the sum of three terms:
\begin{equation}
    \begin{aligned}
        \Delta R(\tilde{\delta_n})&=R(\tilde{\delta_n})-R(\delta_B)\\&=[R(\tilde{\delta_n})-R(\hat{\delta_n})]+[R(\hat{\delta_n})-R(\delta^*)]+[R(\delta^*)-R(\delta_B)]
    \end{aligned}
    \nonumber
\end{equation}

where
\begin{equation}
    \begin{aligned}
        \delta_B&=\text{ Bayes rule}\\
        \delta^*&=\text{ best rule in } D=\argmin_{\delta\in D} R(\delta)\\
        \hat{\delta_n}&=\argmin_{\delta\in D}R_{emp}(\delta)\\
        \tilde{\delta_n}&=\text{solution of the algorithm after $k$ iterations}
    \end{aligned}
    \nonumber
\end{equation}
(Note: $D$ is the set of all available decision rule in approximation (e.g. all linear parameters $\{W,b\}$), which can't be better than Bayes rule)

The expected excess risk
\begin{equation}
    \begin{aligned}
        \epsilon =\mathbb{E}[\Delta R(\tilde{\delta_n})]=\underbrace{\mathbb{E}[R(\tilde{\delta_n})-R(\hat{\delta_n})]}_{\text{Comp. Error}=\rho}+\underbrace{\mathbb{E}[R(\hat{\delta_n})-R(\delta^*)]}_{\text{Est. Error}=O(\frac{d}{n})}+\underbrace{\mathbb{E}[R(\delta^*)-R(\delta_B)]}_{\text{Approx. Error}=O(d^{-\beta})}
    \end{aligned}
    \nonumber
\end{equation}

\textit{Estimation error} increases as $d$ increases, but \textit{approximation error} decreases as $d$ increases. To minimize the excess risk, we want to balance the last two items, that is $O(\frac{d}{n})=O(d^{-\beta})$: solve $$\frac{d}{n}=d^{-\beta} \Rightarrow d^{1+\beta}=n \Rightarrow d=n^{\frac{1}{1+\beta}}$$
$$\Rightarrow\text{ the last two items } O(\frac{d}{n})=O(d^{-\beta})=O(n^{-\gamma})$$
where $\gamma=\frac{\beta}{1+\beta}\in(0,1]$ is a constant.

To balance the three items, we want
\begin{center}
    $\rho=O(n^{-\gamma})\Rightarrow n=O(\rho^{-\frac{1}{\gamma}})$ and $d=O(n^{\frac{1}{1+\beta}})$
\end{center}

The update rule $W_{k+1}=W_k-a_k \nabla Q(X_k,Y_k,W_k)$, where $i\sim \text{Uniform}\{1,2,...,n\}$

\textbf{Relation to Online Learning:} When the training data are made available sequentially (instead of in a batch as assumed here), online learning can be used to sequentially learn the decision rules (or the weights that parameterize the decision rule).

\textbf{Variations on Basic SGD:} mini batch: replace $S$ by a subset $B$ and $n$ by $|B|$
$$\frac{1}{|B|}\sum_{i\in B}Q(X_i,Y_i,W_k)$$

\textbf{Averaging SGD:} $$\bar{W}_n=\frac{1}{n}\sum_{i=1}^nW_i=\frac{W_n}{n}+\bar{W}_{n-1}\frac{n-1}{n}$$

\textbf{SVRG (Stochastic Variance Randomed Gradient):} R. Johnson and T. Zhang, “Accelerating Stochastic Gradient Descent using Predictive Variance Reduction,” \textit{Proc. NIPS} 2013.

\textbf{Unsupervised learning:} If no explanatory variable $X$ is present, the problem reduces to
$$
\min _w \frac{1}{n} \sum_{i=1}^n Q\left(Y_i, w\right)
$$
which finds applications to various unsupervised learning problems. For instance the $k$-means clustering algorithm partitions $n$ data points $y_i, 1 \leq i \leq n$ in $\mathbb{R}^d$ into $k$ clusters with centroids $w_j, 1 \leq j \leq k$, in a way that minimizes the within-cluster sum-of-squares: WCSS $=\frac{1}{n} \sum_{i=1}^n \min _{1 \leq j \leq k}\left\|y_i-w_j\right\|^2$. Using the formalism of (7), we have $Q(y, w)=\min _{1 \leq j \leq k}\left\|y-w_j\right\|^2$ where $y \in \mathbb{R}^d$ and $w=\left\{w_j\right\}_{j=1}^k \in \mathbb{R}^{d \times k}$ is a matrix whose $k$ columns are the centroid vectors.


\chapter{Stochastic Integration Methods}
Integral $I=\int_Xf(x)dx$.

\section{Deterministic Methods (Better in Low Dimension)}
\subsection{Riemann Integration}
Riemann integral: approximation integral $I$ of $f(x)$ in $[a,b]$ with $$\hat{I}_n=\sum_{i=1}^n(\underbrace{x_i-x_{i-1}}_{\frac{b-a}{n}})f(x_i)$$
where $x_i=a+\frac{b-a}{n}i$. We can also denote $\hat{f}(x)=f(x_i)$ if $x\in (x_{i-1},x_i]$

The error $|\hat{I}_n-I|=\int_a^b|\hat{f}(x)-f(x)|dx$

Assume $f$ is differentiable and $\max_x|f'(x)|=c<\infty$, then $|\hat{f}(x)-f(x)|\leq \frac{b-a}{n}c$
$$\Rightarrow |\hat{I}_n-I|\leq \int_a^b\frac{b-a}{n}c\ dx=\frac{(b-a)^2}{n}c$$
That is $n\sim O\left(\varepsilon^{-1}\right)$

\subsection{Trapezoidal Rule}
Using average can be better. $$\hat{I}_n=\sum_{i=1}^n(\underbrace{x_i-x_{i-1}}_{\frac{b-a}{n}})\frac{f(x_i)+f(x_{i-1})}{2}$$
The upper bound of error is $|\hat{I}_n-I|\leq \frac{C}{n^2}$ for some constant $C$.

That is $n\sim O\left(\varepsilon^{-\frac{1}{2}}\right)$

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{Riemann and Trapezoidal.png}
    \caption{(a) Riemann approximation; (b) Trapezoidal approximation.}
    \label{}
\end{figure}\end{center}

\subsection{Multidimensional Integration}
When we want to do integral in high dimension, it will be really hard.

For $d-$dimensional integrals, the trapezoidal rule yields an approximation error $|\hat{I}_n-I|\leq \frac{C}{n^\frac{2}{d}}$ for some constant $C$. That is $n\sim O\left(\varepsilon^{-\frac{d}{2}}\right)$. $n$ needs to increase exponentially with $d$ to achieve a target approximation error $\varepsilon$. This phenomenon is known as \underline{the curse of dimensionality}.
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.3]{two-dim int.png}
    \caption{Two-dimensional integration using regular grid.}
    \label{}
\end{figure}\end{center}




\section{Stochastic Methods (Better in High Dimension)}
\subsection{Classical Monte Carlo Integration}

Compute the expectation $$\xi=\mathbb{E}_p[h(x)]=\int_X \underbrace{p(x)h(x)}_{f(x)}dx$$

The methods described below can be used to solve the following problems: (1) General $\int_X f$; (2) Compute the probability of falling into a subset $a\subset X :P(a)=\int_a p(x)dx$, where $h(x)=\mathbf{1}_{x\in a}$

The Monte Carlo approach is as follows: Given $X_1, X_2, \cdots, X_n$ drawn i.i.d from the pdf
$p$, estimate $\xi $ by the empirical average $$\hat{\xi}_n=\frac{1}{n}\sum_{i=1}^nh(X_i)$$

$\mathbb{E}_p[\hat{\xi}_n]=\mathbb{E}_p[h(X)]=\xi$. $\hat{\xi}_n \stackrel{a.s.}{\longrightarrow}\xi$ as $n \rightarrow \infty$ by SLLW.

$Var(\hat{\xi}_n-\xi)=Var(\hat{\xi}_n)=\frac{1}{n}Var[h(x)]=O\left(\frac{1}{n}\right) \Rightarrow sd(\hat{\xi}_n)=\frac{\sqrt{Var[h(x)]}}{\sqrt{n}}$

That is $n\sim O\left(n^{-\frac{1}{2}}\right)$

The stochastic methods \textbf{outperform} when the deterministic ones for dimensions \underline{$d >4$} and are \textbf{worse} for \underline{$d<4$}.

\subsection{Importance Sampling}
Draw $X_i,i=1,...,n$ i.i.d from pdf $q$ $$\hat{\xi}_n=\frac{1}{n}\sum_{i=1}^n\frac{p(X_i)}{q(X_i)}h(X_i)$$
It is an unbiased estimator of $\xi$
\begin{equation}
    \begin{aligned}
        \mathbb{E}[\hat{\xi}_n]=\mathbb{E}_q[\frac{p(X_i)}{q(X_i)}h(X_i)]=\int_X p(x)h(x)dx=\xi
    \end{aligned}
    \nonumber
\end{equation}
$\hat{\xi}_n \stackrel{a.s.}{\longrightarrow}\xi$ as $n \rightarrow \infty$ by SLLW.

Its variance is
\begin{equation}
    \begin{aligned}
        {Var}_q(\hat{\xi}_n)&=\frac{1}{n}{Var}_q\left[\frac{p(X_i)}{q(X_i)}h(X_i)\right]\\
        &=\frac{1}{n}\left(\int_X\frac{p^2(x)}{q(x)}h^2(x)dx-\xi^2\right)
    \end{aligned}
    \nonumber
\end{equation}
The idea of importance sampling is to find a good $q$ such that $${Var}_q(\hat{\xi}_n)<{Var}_p(\hat{\xi}_n)$$

\subsection*{Error Meausre}
The \textit{relative error} of the importance-sampling estimator is defined as
$$
\delta_{\mathrm{rel}}\left(\hat{\xi}_n\right) \triangleq \frac{\sqrt{\operatorname{Var}_q\left(\hat{\xi}_n\right)}}{\xi}=\sqrt{\frac{\operatorname{Var}_q\left[\frac{p(X)}{q(X)} h(X)\right]}{\xi^2 n}} .
$$
The \textit{number} of simulations needed to achieve a relative error of $\delta$ is
$$
n_{I S}(\delta)=\frac{\operatorname{Var}_q\left[\frac{p(X)}{q(X)} h(X)\right]}{\xi^2 \delta^2} .
$$
The \textit{gain} \underline{relative to a Monte Carlo simulation} is defined as
$$
\Gamma=\frac{n_{M C}(\delta)}{n_{I S}(\delta)}=\frac{\operatorname{Var}_p[h(X)]}{\operatorname{Var}_q\left[\frac{p(X)}{q(X)} h(X)\right]}
$$

\textbf{In the example of $\xi=P(a), h(x)=\mathbf{1}_{x\in a}$:} Suppose $\xi=P(a)\approx 10^{-9}$ (small), $\hat{\xi}_n=\frac{1}{n}\sum_{i=1}^n\mathbf{1}_{X_i\in a}$. $\mathbf{1}_{X_i\in a}$ is $Bernoulli(\xi)$. We have $Var(\hat{\xi}_n)=\frac{\xi(1-\xi)}{n}$.

We can use relative error to measure $$\delta_{\mathrm{rel}}\left(\hat{\xi}_n\right)=\frac{\sqrt{Var(\hat{\xi}_n)}}{\xi}=\sqrt{\frac{1-\xi}{n\xi}}$$
and the number of simulation need to get relative error $\delta$ is $$
n_{I S}(\delta)=\frac{1-\delta} {\xi \delta^2} .
$$

\textbf{Find the optimal $q$:}
$$\min_q\int_X\frac{p^2(x)}{q(x)}h^2(x)dx-\xi^2$$
write $$\int_X\frac{p^2(x)}{q(x)}h^2(x)dx-\xi^2=\mathbb{E}_q\left[\left(\underbrace{\frac{p(x)}{q(x)}h(x)}_{Z}\right)^2\right]$$
Since $x^2$ is convex function, by Jensen's inequality
$$\mathbb{E}_q\left[\left(\underbrace{\frac{p(x)}{q(x)}h(x)}_{Z}\right)^2\right]\geq \left(\mathbb{E}_q\left[\underbrace{\frac{p(x)}{q(x)}h(x)}_{Z}\right]\right)^2$$
This equality holds if and only if $\frac{p(x)}{q(x)}h(x)=\alpha,\forall x\in X$, $\alpha$ is a constant.

Since $q$ is pdf., we can infer $$q(x)=\frac{p(x)h(x)}{\int_Xp(x)h(x)dx}$$
which is as hard as the original problem. In practice, one is content to find a “good” $q$ that assigns high probability to the important region where $p(x)h(x)$ is large. Ideally the ratio $\frac{p(x)}{q(x)}h(x)$ would be roughly constant over $X$.


\chapter{Particle Filtering}
Kalman filtering is used in tracking problems (dynamic models). Particle Filtering is an extension of Kalman filtering.
\section{Kalman Filtering (Linear Dynamic System)}
\begin{enumerate}
    \item Unknown state sequence $X_t\in \mathbb{R}^m,t=0,1,2,...$
    \item Observations $Y_t\in \mathbb{R}^k,t=0,1,2,...$
    \item $X_{t+1}=F_tX_t+U_t$, $F_t\in \mathbb{R}^{m\times m}, U_t\sim P_{U_t}$
    \item $Y_t=H_tX_t+V_t$, $H_t\in \mathbb{R}^{k\times m}, V_t\sim P_{V_t}$
\end{enumerate}

We want to solve two problems
\begin{enumerate}
    \item \textbf{Estimation Problem:} Evaluate Linear MMSE (LMMSE) of $X_t$ given $Y_{0:t}$. $$\hat{X}_{t|t}=WY_{0:t}+b$$
    \item \textbf{Prediction Problem:} Predict Linear MMSE (LMMSE) of $X_{t+1|t}$ given $Y_{0:t}$. (Really hard)
\end{enumerate}
We can solve closed-form solutions.

\section{Particle Filtering (Nonlinear Dynamic System)}
Particle filtering is a nonlinear form of Kalman filtering, which doesn't have closed-from solutions.

We consider a \underline{Nonlinear Dynamic System}
\begin{equation}
    \begin{aligned}
        X_{t+1}\sim q(\cdot|X_t)\\
        Y_{t}\sim r(\cdot|X_t)\\t=0,1,2,...
    \end{aligned}
    \nonumber
\end{equation}
where $q(X_{t+1}|X_t)$ is the transition probability distribution, and $r(Y_t|X_t)$ is the conditional probability distribution for the observations. Hence, $X_t$ is a Markov process and $Y_t$ follows a Hidden Markov Model (HMM).
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{HMM.png}
    \caption{Hidden Markov Model}
    \label{}
\end{figure}\end{center}
We also consider these two probelms.
\begin{enumerate}
    \item \textbf{Estimation Problem:} Evaluate $X_t$ given $Y_{0:t}$.
    \item \textbf{Prediction Problem:} Predict $X_{t+1|t}$ given $Y_{0:t}$.
\end{enumerate}

\subsection{Bayesian Recursive Filtering}
In this section we use Bayesian approach and use MMSE estimation $l(\hat{x}_t,x_t)=\|x_t-\hat{x}_t\|^2$

Estimation and prediction in conditional forms are:
\begin{equation}
    \begin{aligned}
        \hat{X}_{t|t}&=\mathbb{E}[X_{t}|Y_{0:t}]=\int_{\mathbb{R}^m}x_{t}P(X_{t}|Y_{0:t})dx_{t}\\
        \hat{X}_{t+1|t}&=\mathbb{E}[X_{t+1}|Y_{0:t}]=\int_{\mathbb{R}^m}x_{t+1}P(X_{t+1}|Y_{0:t})dx_{t+1}
    \end{aligned}
    \nonumber
\end{equation}

Apparently the posterior p.d.f cannot be evaluated due to the curse of dimensionality as $t$ increases. However, they can in principle be evaluated \textit{recursively} using the following two-step procedure.

\begin{enumerate}[\textbf{Step}]
    \item \textbf{ 1: Prediction.} $P(X_{t+1}|Y_{0:t})$ can be expressed in term of $P(X_t|Y_{0:t})$:
    \begin{equation}
        \begin{aligned}
            P(X_{t+1}|Y_{0:t})&=\int_{\mathbb{R}^m}P(X_{t+1},X_t|Y_{0:t})dx_t\\
            &=\int_{\mathbb{R}^m}P(X_{t+1}|X_t, Y_{0:t})P(X_t|Y_{0:t})dx_t\\
            &=\int_{\mathbb{R}^m}q(X_{t+1}|X_t)P(X_t|Y_{0:t})dx_t\\
        \end{aligned}
        \nonumber
    \end{equation}
    \item \textbf{ 2: Update.} We can also express $P(X_t|Y_{0:t})$ in terms of $P(X_t|Y_{0:t-1})$
    \begin{equation}
        \begin{aligned}
            P(X_{t}|Y_{0:t})&=P(X_t|Y_{t},Y_{0:t-1})\\
            &=\frac{P(Y_t|X_t,Y_{0:t-1})P(X_t|Y_{0:t-1})}{P(Y_t|Y_{0:t-1})}\\
            &=\frac{r(Y_t|X_t)P(X_t|Y_{0:t-1})}{\int_{\mathbb{R}^m}r(Y_t|X_t)P(X_t|Y_{0:t-1})dx_t}
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}

\subsection{Particle Filter (bootstrap filter)}
Suppose we have $n$ i.i.d. samples of $X_t$ drawn from $p(x_t|Y_{0:t})$: $X_t(1),X_t(2),...,X_t(n)$.
\begin{equation}
    \begin{aligned}
        X_{t}(i)\sim p(\cdot|Y_{0:t}),1\leq i\leq n
    \end{aligned}
    \tag{\text{Sample 1}}
\end{equation}

We can use above recursive filtering method to generate estimation of $X_{t+1}$.
\begin{enumerate}[\textbf{Step}]
    \item \textbf{ 1: Prediction.} Using the transition probability $q(\cdot|X_t(i)),1\leq i\leq n$ to generate $n$ independent random variables
    \begin{equation}
        \begin{aligned}
            X^*_{t+1}(i)\sim q(\cdot|X_t(i)),\ 1\leq i\leq n
        \end{aligned}
        \tag{\text{Sample 2}}
    \end{equation}
    \item \textbf{ 2: Update.} Upon \textbf{receiving a new measurement $y_{t+1}$}, evaluate the \textit{importance weights} (nonnegative and summing to $1$)
    \begin{equation}
        \begin{aligned}
            w_i=\frac{r(y_{t+1}|X_{t+1}^*(i))}{\sum_{j=1}^n r(y_{t+1}|X_{t+1}^*(j))},\ 1\leq i\leq n
        \end{aligned}
        \nonumber
    \end{equation}
    Then we resample $n$ times from the set $\{X_{t+1}^*(i)\}_{i=1}^n$ with respective probabilities $\{w_i\}_{i=1}^n$, obtaining i.i.d samples $\{X_{t+1}(j)\}_{j=1}^n$ with probabilities
    \begin{equation}
        \begin{aligned}
            Pr[X_{t+1}(j)=X_{t+1}^*(i)]=w_i,\ 1\leq i,j\leq n
        \end{aligned}
        \tag{\text{Sample 3}}
    \end{equation}
\end{enumerate}
By the \textbf{weighted bootstrap theorem}, as $n \rightarrow \infty$, the distribution of the
resampled $\{X_{t+1}(j)\}_{j=1}^n$ converges to the desired posterior.


Potential issues: 1. $n$ is not large enough. 2. Sample impoverishment





\chapter{EM Algorithm}

The ML estimator: $\hat{\theta}_{ML}=\argmax_{\theta\in S}\ln p_\theta(y)$. Numerical evaluation of maximum-likelihood (ML) estimates is often difficult. The likelihood function may have multiple extreme and the parameter $\theta$ may be multidimensional, all of which are problematic for any numerical algorithm.

\subsection*{Maximum-Likelihood (ML) Estimation}
Given a vector $\vec{y}$, find the $\theta$ that maximizes $p_\theta(\vec{y})=\prod_{i=1}^n P(y_i|\theta)$
$$\hat{\theta}_{ML}=\argmax_{\theta\in S}\ln p_\theta(\vec{y})=\argmax_{\theta\in S}\sum_{i=1}^n \ln P(y_i|\theta)$$
Solving the closed-form solution is quite hard sometimes, so we may use EM algorithm.

\section{General Structure of the EM Algorithm}

\subsection*{What we want to estimate:}
$\theta\in S$ is an unknown parameter that we want to estimate.
\subsection*{What we know:}
To help us solve the solution, we construct an unobservable vector $\vec{z}$ corresponding to $\vec{y}$.
\begin{enumerate}
    \item There is a complete data space $Z$ and an incomplete data space $Y$.
    \item The reality is $z\in Z$, which has p.d.f $P(z|\theta)$. ($\ln P(z|\theta)$'s derivative should be constructed to be easy.)
    \item \textbf{Instead of observing the $z$ directly, we can observe $y=h(z)\in Y$ which has p.d.f $P(y|\theta)$.}
    \item $h(z)=y$ is a many-to-one mapping. $$P(y,z|\theta)=P(z|\theta),\quad \forall z\in h^{-1}(y)$$
    \item We can infer that the relationship between $P(z|\theta)$ and $P(y|\theta)$ is
    \begin{equation}
        \begin{aligned}
            P(z|\theta)&=P(z|y,\theta)P(y|\theta),\quad \forall z\in h^{-1}(y)\\
            P(y|\theta)&=\sum_{z\in h^{-1}(y)}P(z|\theta),\quad \forall y
        \end{aligned}
        \nonumber
    \end{equation}
    \item For any function $f$, $\mathbb{E}_z[f(z)|y]$ depends on the p.d.f.
    \begin{equation}
        \begin{aligned}
            \mathbb{E}_{z|\theta}[f(z)|y]=\sum_{z\in h^{-1}(y)}P(z|y,\theta)f(z)
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{EM1.png}
    \caption{Complete and incomplete data spaces $Z$ and $Y$.}
    \label{}
\end{figure}\end{center}

%The incomplete-data and complete-data loglikelihood functions are respectively denoted by
%\begin{equation}
%    \begin{aligned}
        %l_{id}(\theta)\triangleq \ln p_\theta(y);\ l_{cd}(\theta)\triangleq \ln q_\theta(z)
%    \end{aligned}
%    \nonumber
%\end{equation}
\subsection*{EM Algorithm}
Instead of computing the $P(\vec{y}|\theta)$ directly, we use the relationship $h(z)=y$ and $P(\vec{z}|\theta)$ to estimate $\theta$.

Suppose we have a prior belief of the relationship between $y$ and $z$: $P(z|y,\theta^{(k)})$. Given $\vec{y}$, since maximizing $\ln P(\vec{y}|\theta)$ is hard, we maximize the expected value of $\ln P(\vec{z}|\theta)|\vec{y}$ under the prior belief (i.e., finding the $\theta$ that can properly represent the relationship between $\vec{y}$ and $\vec{z}$), that is
\begin{equation}
    \begin{aligned}
        \theta&=\argmax_\theta \mathbb{E}_{z|y,\theta^{(k)}} [\ln P(\vec{z}|\theta)|\vec{y}]\\
        &=\argmax_\theta\sum_{i=1}^n \sum_{z_i\in h^{-1}(y_i)}P(z_i|y_i,\theta^{(k)})\ln P(z_i|\theta)
    \end{aligned}
    \nonumber
\end{equation}

EM algorithm alternates between Expectation (E) and Maximization (M) steps:
\begin{enumerate}
    \item Initialize $\hat{\theta}^{(0)}$
    \item For $k=0,1,2,...$\\
    \textbf{Expectation (E)-Step:}
        Compute
        \begin{equation}
            \begin{aligned}
                Q(\theta|\hat{\theta}^{(k)})
                &=\mathbb{E}_{z|y,\hat{\theta}^{(k)}}[\ln P(\vec{z}|\theta)|\vec{y}]\\
                &=\sum_{i=1}^n \sum_{z_i\in h^{-1}(y_i)}P(z_i|y_i,\theta^{(k)})\ln P(z_i|\theta)
            \end{aligned}
            \nonumber
        \end{equation}
    \textbf{Maximization (M)-Step} $$\hat{\theta}^{(k+1)}=\argmax_{\theta\in S}Q(\theta|\hat{\theta}^{(k)})$$
\end{enumerate}

\begin{definition}
    $\theta^*$ is a stable point of the EM algorithm if $\exists$ subsequence that converges to $\theta^*$.

    e.g. $1,3,\frac{1}{2},3,\frac{1}{3},3,...\frac{1}{n},3,...$
\end{definition}

\section{Example 1: Variance Estimation}
Observation $Y=S+N$, $S\sim \mathcal{N}(0,\theta)$ is independent of $N\sim \mathcal{N}(0,\theta)$ $\Rightarrow$ $Y\sim \mathcal{N}(0,\theta+1)$. $p_\theta(y)=\frac{1}{\sqrt{2\pi(\theta+1)}}e^{-\frac{y^2}{2(\theta+1)}}$. We want to estimate $\theta$.
\subsection{Maximum-Likelihood (ML) Estimation}
\begin{equation}
    \begin{aligned}
        \ln p_\theta(y)=-\frac{1}{2}\ln (2\pi)-\frac{1}{2}\ln(\theta+1)-\frac{y^2}{2(\theta+1)}
    \end{aligned}
    \nonumber
\end{equation}
take derivation of $\theta$ to be equal to $0$
\begin{equation}
    \begin{aligned}
        -\frac{1}{2(\theta+1)}+\frac{y^2}{2(\theta+1)^2}=0
    \end{aligned}
    \nonumber
\end{equation}
We can get $$\hat{\theta}=y^2-1$$
Then, $$\hat{\theta}_{ML}=\left\{\begin{matrix}
    0,&y^2\leq 1\\
    y^2-1,&y^2>1
\end{matrix}\right.$$

\subsection{EM Algorithm}
Let $Z=(S,N)$, $y=h(z)=s+n$.
\begin{equation}
    \begin{aligned}
        q_\theta(z)=q_\theta(s,n)=\frac{1}{\sqrt{2\pi\theta}}e^{-\frac{s^2}{2\theta}}\frac{1}{\sqrt{2\pi}}e^{-\frac{n^2}{2}}
    \end{aligned}
    \nonumber
\end{equation}
Then $$\ln q_\theta(z)=\ln \frac{1}{\sqrt{2\pi}}e^{-\frac{n^2}{2}}-\frac{1}{2}\ln (2\pi)-\frac{1}{2}\ln(\theta)-\frac{s^2}{2\theta}$$
\textbf{E-Step:}
        Compute
        \begin{equation}
            \begin{aligned}
                Q(\theta|\hat{\theta}^{(k)})
                &=\mathbb{E}_{z|\hat{\theta}^{(k)}}[\ln q_\theta(z)|Y=y]\\
                &=\sum_{z\in h^{-1}(y)}q_{\hat{\theta}^{(k)}}(z)\ln q_\theta(z)\\
                &=\ln \frac{1}{\sqrt{2\pi}}e^{-\frac{n^2}{2}}-\frac{1}{2}\ln (2\pi)-\frac{1}{2}\ln(\theta)-\frac{\mathbb{E}_{z|\hat{\theta}^{(k)}}(s^2)}{2\theta}
            \end{aligned}
            \nonumber
        \end{equation}
\textbf{M-Step}
\begin{equation}
    \begin{aligned}
        \hat{\theta}^{(k+1)}&=\argmax_{\theta\in S}Q(\theta|\hat{\theta}^{(k)})\\
        0&=-\frac{1}{2\hat{\theta}^{(k+1)}}+\frac{\mathbb{E}_{z|\hat{\theta}^{(k)}}(s^2)}{2(\hat{\theta}^{(k+1)})^2}\\
        \hat{\theta}^{(k+1)}&=\mathbb{E}_{z|\hat{\theta}^{(k)}}(s^2)=\frac{\hat{\theta}^{(k)}}{\hat{\theta}^{(k)}+1}\left(\frac{\hat{\theta}^{(k)}}{\hat{\theta}^{(k)}+1}y^2+1\right)
    \end{aligned}
    \nonumber
\end{equation}
Then we can solve the stable point
\begin{equation}
    \begin{aligned}
        \hat{\theta}^{*}=\frac{\hat{\theta}^{*}}{\hat{\theta}^{*}+1}\left(\frac{\hat{\theta}^{*}}{\hat{\theta}^{*}+1}y^2+1\right)\\
        \Rightarrow \hat{\theta}^{*}=0, \hat{\theta}^{*}=y^2-1
    \end{aligned}
    \nonumber
\end{equation}
According to the relation between $\hat{\theta}^{(k)}$ and $\hat{\theta}^{(k+1)}$, we can infer
$$\hat{\theta}^*=\left\{\begin{matrix}
    0,&y^2\leq 1\\
    y^2-1,&y^2>1
\end{matrix}\right.$$







\section{Example 2: Estimation of Gaussian Mixtures}
Assume the data $\vec{Y}=\left\{Y_i, 1 \leq i \leq n\right\} \in \mathbb{R}^n$, are drawn iid from a pdf $p_\theta(y)$ which is the mixture of $m$ univariate Gaussians with respective probabilities $\pi(j)$, means $\mu_j$, and variances $\sigma_j^2$, for $1 \leq j \leq m$ :
$$p_\theta(y|j)=\phi\left(y ; \mu_j, \sigma_j^2\right)$$
$$
p_\theta(y)=\sum_{j=1}^m \pi(j) \phi\left(y ; \mu_j, \sigma_j^2\right), \quad y \in \mathbb{R}
$$
where
$$\phi\left(y ; \mu, \sigma^2\right) \triangleq \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left\{-\frac{(y-\mu)^2}{2 \sigma^2}\right\}$$
denotes the Gaussian pdf with mean $\mu$ and variance $\sigma^2$.
\subsection{Unknown Means: ML estimation is hard}
We initially assume that $\{\pi(j)\}$ and $\left\{\sigma_j^2\right\}$ are given and that we only need to estimate the means $\left\{\mu_j\right\}$. Thus, $\theta=\mu \in \mathbb{R}^m$.

Unfortunately the ML estimator cannot be derived in closed form. Indeed, the loglikelihood function for $\theta$ is
$$
\ln \prod_{i=1}^n p_\theta(y_i)=\sum_{i=1}^n \ln p_\theta\left(y_i\right)=\sum_{i=1}^n \ln \sum_{j=1}^m \pi(j) \phi\left(y_i ; \mu_j, \sigma_j^2\right)
$$
and maximizing it is a $m$-dimensional, nonconcave maximization problem.

Taking the derivative of $\mu_j, j=1,...,m$,
\begin{equation}
    \begin{aligned}
        0&=\frac{1}{\sigma^2_j}\sum_{i=1}^n(y_i-\mu_j)\frac{\pi(j)\phi(y_i ; \mu_j, \sigma_j^2)}{\sum_{j=1}^m\pi(j)\phi(y_i ; \mu_j, \sigma_j^2)}\\
        &=\frac{1}{\sigma^2_j}\sum_{i=1}^n(y_i-\mu_j)\pi_\theta(j|y_i)
    \end{aligned}
    \nonumber
\end{equation}
where $\pi_\theta(j|y_i)\triangleq \frac{\pi(j)\phi(y_i ; \mu_j, \sigma_j^2)}{\sum_{j=1}^m\pi(j)\phi(y_i ; \mu_j, \sigma_j^2)}$.
The system may have multiple solutions corresponding to local maxima or even local minima or saddle points of the likelihood function.

\subsection{Unknown Means: EM Algorithm}

There is a complete data $Z_i=(J_i,Y_i),=1,...,n$, where $J_i$ is the random label that was drawn to produce $Y_i$. $z=\{j_i,y_i\}_{i=1}^n$ is the sample.
\begin{equation}
    \begin{aligned}
        q_\theta(z)&=\prod_{i=1}^n\left(\pi(j_i)p_\theta(y_i|j_i)\right)
    \end{aligned}
    \nonumber
\end{equation}
$$\ln q_{\theta}(z)=\sum_{i=1}^n[\ln \pi(j_i)+\ln p_{\theta}(y_i|j_i)]$$

Initialize $\hat{\theta}^{(0)}$

Iteration:
\begin{equation}
    \begin{aligned}
        Q(\theta|\hat{\theta}^{(k)})
        &=\sum_{i=1}^n\mathbb{E}_{\hat{\theta}^{(k)}}[\ln \pi(j_i)+\ln p_{\theta}(y_i|j_i)|Y_i=y_i]\\
        &=\sum_{i=1}^n\sum_{j=1}^m\pi_{\hat{\theta}^{(k)}}(j|y_i)[\ln \pi(j)+\ln p_{\theta}(y_i|j)]\\
        &=cst-\sum_{i=1}^n\sum_{j=1}^m\pi_{\hat{\theta}^{(k)}}(j|y_i)\frac{(y_i-\mu_j)^2}{2\sigma_j^2}\\
        &=cst-\sum_{i=1}^n\sum_{j=1}^m\frac{\pi(j)\phi(y_i ; \hat{\mu}_j^{(k)}, \sigma_j^2)}{\sum_{j=1}^m\pi(j)\phi(y_i ; \hat{\mu}_j^{(k)}, \sigma_j^2)}\frac{(y_i-\mu_j)^2}{2\sigma_j^2}
    \end{aligned}
    \nonumber
\end{equation}
where $\ln p_{\theta}(y_i|j)=-\frac{1}{2}\ln(2\pi\sigma_j^2)-\frac{(y_i-\mu_j)^2}{2\sigma_j^2}$.

Take derivative of $\mu_j$,
\begin{equation}
    \begin{aligned}
        0&=\frac{\partial Q(\theta|\hat{\theta}^{(k)})}{\partial \mu_j}=\sum_{i=1}^n\pi_{\hat{\theta}^{(k)}}(j|y_i)\frac{(y_i-\mu_j)}{\sigma_j^2}\\
        \hat{\mu}_j^{(k+1)}&=\frac{\sum_{i=1}^n\pi_{\hat{\theta}^{(k)}}(j|y_i)y_i}{\sum_{i=1}^n\pi_{\hat{\theta}^{(k)}}(j|y_i)}
    \end{aligned}
    \nonumber
\end{equation}

Recall the $\hat{\theta}_{ML}$
\begin{equation}
    \begin{aligned}
        \hat{\theta}_{ML,j}=\frac{\sum_{i=1}^n\pi_{\hat{\theta}_{ML}}(j|y_i)y_i}{\sum_{i=1}^n\pi_{\hat{\theta}_{ML}}(j|y_i)}
    \end{aligned}
    \nonumber
\end{equation}
$\hat{\theta}_{ML}$ is the stable point. (if exist)

\subsection{Unknown Mixture Probabilities, Means and Variances}
\textbf{ML Estimation:}

If $\theta \triangleq\left\{\pi(j), \mu_j, \sigma_j^2, 1 \leq j \leq m\right\}$ is unknown, the ML estimator $\hat{\theta}_{\mathrm{ML}}$ satisfies the following nonlinear system of equations:
$$
\begin{aligned}
\hat{\mu}_{\mathrm{ML}, j} &=\frac{\sum_{i=1}^n y_i \pi_{\hat{\theta}^{(k)}}\left(j \mid y_i\right)}{\sum_{i=1}^n \pi_{\hat{\theta}^{(k)}}\left(j \mid y_i\right)} \\
\hat{\sigma}_{\mathrm{ML}, j}^2 &=\frac{\sum_{i=1}^n\left(y_i-\hat{\mu}_{\mathrm{ML}, j}\right)^2 \pi_{\hat{\theta}^{(k)}}\left(j \mid y_i\right)}{\sum_{i=1}^n \pi_{\hat{\theta}(k)}\left(j \mid y_i\right)} \\
\hat{\pi}_{\mathrm{ML}}(j) &=\frac{1}{n} \sum_{i=1}^n \pi_{\hat{\theta}^{(k)}}\left(j \mid y_i\right) \quad 1 \leq j \leq m
\end{aligned}
$$
where
$$
\pi_\theta\left(j \mid y_i\right)=\frac{\pi(j) \phi\left(y_i ; \mu_j, \sigma_j^2\right)}{\sum_{j=1}^m \pi(j) \phi\left(y_i ; \mu_j, \sigma_j^2\right)}, \quad 1 \leq j \leq m
$$
\textbf{E-step:}
\begin{equation}
    \begin{aligned}
        Q(\theta|\hat{\theta}^{(k)})
        &=cst-\sum_{i=1}^n\sum_{j=1}^m\pi_{\hat{\theta}^{(k)}}(j|y_i)\frac{(y_i-\mu_j)^2}{2\sigma_j^2}\\
        &=cst-\sum_{i=1}^n\sum_{j=1}^m\frac{\hat{\pi}^{(k)}(j)\phi(y_i ; \hat{\mu}_j^{(k)}, \hat{\sigma_j^2}^{(k)})}{\sum_{j=1}^m\hat{\pi}^{(k)}(j)\phi(y_i ; \hat{\mu}_j^{(k)}, \hat{\sigma_j^2}^{(k)})}\frac{(y_i-\mu_j)^2}{2\sigma_j^2}
    \end{aligned}
    \nonumber
\end{equation}
\textbf{M-Step:}
\begin{equation}
    \begin{aligned}
        \hat{\mu}_j^{(k+1)} &=\frac{\sum_{i=1}^n y_i \pi_{\hat{\theta}^{(k)}}\left(j \mid y_i\right)}{\sum_{i=1}^n \pi_{\hat{\theta}(k)}\left(j \mid y_i\right)} \\
        \left(\hat{\sigma}_j^2\right)^{(k+1)} &=\frac{\sum_{i=1}^n\left(y_i-\hat{\mu}_j^{(k+1)}\right)^2 \pi_{\hat{\theta}^{(k)}}\left(j \mid y_i\right)}{\sum_{i=1}^n \pi_{\hat{\theta}^{(k)}}\left(j \mid y_i\right)} \\
        \hat{\pi}^{(k+1)}(j) &=\frac{1}{n} \sum_{i=1}^n \pi_{\hat{\theta}^{(k)}}\left(j \mid y_i\right), \quad 1 \leq j \leq m .
    \end{aligned}
    \nonumber
\end{equation}


\section{Convergence of EM Algorithm}
\begin{theorem}
    The likelihood sequence $p_{\hat{\theta}^{(k)}}(y)$, $k = 0, 1, 2, ...$ is nondecreasing.
\end{theorem}
\begin{proof}
    Assume for notational simplicity that the random variables $Y$ and $Z$ are discrete. Hence, their joint distribution is given by
    \begin{equation}
        \begin{aligned}
            P_\theta(y,z)&=q_\theta(z)p_\theta(y|z)=q_\theta(z)\mathbf{1}_{\{y=h(z)\}}\\
            &=p_\theta(y)q_\theta(z|y)
        \end{aligned}
        \nonumber
    \end{equation}
    Given $y$, the following identity holds for all $z \in h^{-1}(y)$:
    \begin{equation}
        \begin{aligned}
            p_\theta(y)&=\frac{q_\theta(z)}{q_\theta(z|y)}
        \end{aligned}
        \nonumber
    \end{equation}
    Taking the logarithm,
    \begin{equation}
        \begin{aligned}
            \ln p_\theta(y)&= \ln q_\theta(z)-\ln q_\theta(z|y),\ \forall z\in h^{-1}(y)
        \end{aligned}
        \nonumber
    \end{equation}
    Taking the conditional expectation with respect to $q_{\hat{\theta}}(z|y)$,
    \begin{equation}
        \begin{aligned}
            \ln p_\theta(y)&= \sum_{z\in h^{-1}(y)}q_{\hat{\theta}}(z|y)\ln q_\theta(z)-\sum_{z\in h^{-1}(y)}q_{\hat{\theta}}(z|y)\ln q_\theta(z|y)
        \end{aligned}
        \tag{1}
    \end{equation}
    \textbf{Expectation (E)-Step:}
        Compute
        \begin{equation}
            \begin{aligned}
                Q(\theta|\hat{\theta}^{(k)})
                &=\sum_{z\in h^{-1}(y)}q_{\hat{\theta}^{(k)}}(z|y)\ln q_\theta(z)
            \end{aligned}
            \nonumber
        \end{equation}
    \textbf{Maximization (M)-Step} $$\hat{\theta}^{(k+1)}=\argmax_{\theta\in S}Q(\theta|\hat{\theta}^{(k)})$$

    According to $(1)$,
    \begin{equation}
        \begin{aligned}
            \ln p_\theta (y)&=Q(\theta|\hat{\theta}^{(k)})-H(q_{\hat{\theta}^{(k)}},q_\theta)\\
            \ln p_{\hat{\theta}^{(k+1)}} (y)-\ln p_{\hat{\theta}^{(k)}} (y)&=(Q(\hat{\theta}^{(k+1)}|\hat{\theta}^{(k)})-Q(\hat{\theta}^{(k)}|\hat{\theta}^{(k)}))-(H(q_{\hat{\theta}^{(k)}},q_{\hat{\theta}^{(k+1)}})-H(q_{\hat{\theta}^{(k)}},q_{\hat{\theta}^{(k)}}))
        \end{aligned}
        \nonumber
    \end{equation}
    Since $\hat{\theta}^{(k+1)}=\argmax_{\theta\in S}Q(\theta|\hat{\theta}^{(k)})$, $Q(\hat{\theta}^{(k+1)}|\hat{\theta}^{(k)})-Q(\hat{\theta}^{(k)}|\hat{\theta}^{(k)})\geq 0$.
    \begin{equation}
        \begin{aligned}
            H(q_{\hat{\theta}^{(k)}},q_{\hat{\theta}^{(k+1)}})-H(q_{\hat{\theta}^{(k)}},q_{\hat{\theta}^{(k)}})=D(q_{\hat{\theta}^{(k)}}\| q_{\hat{\theta}^{(k+1)}})\geq 0
        \end{aligned}
        \nonumber
    \end{equation}
    Hence, we can conclude $\ln p_{\hat{\theta}^{(k+1)}} (y)-\ln p_{\hat{\theta}^{(k)}} (y)\geq 0$. Then $p_{\hat{\theta}^{(k)}}$ should be nondecreasing in $k$.
\end{proof}

\begin{corollary}
    Assume that $S$ is a closed, bounded subset of Euclidean space, the functions $Q(\theta|\theta')$ and $H(\theta|\theta')$ are continuously differentiable, and the loglikelihood function $\ln p_{\hat{\theta}^{(k)}}$ is differentiable and bounded. Then the sequence $\ln p_{\hat{\theta}^{(k)}}$ converges, and any limit point $\theta^*\in \text{interior}(S)$ of the EM sequence is a solution of the likelihood equation $\nabla \ln p_{\theta}=0$.
\end{corollary}


\section{EM As an Alternating Maximization Algorithm}
Define an \textit{auxiliary cost function} $L(q,\theta)$.

Incomplete data $Y$; Complete data $Z$. Still $h(z): Z \rightarrow Y$.

$\mathcal{Q}_y=\{q: q(z)=0, \forall z\in h^{-1}(y)\}$

\underline{EM updates}
\begin{enumerate}
    \item E-Step:
\end{enumerate}













\chapter{Hidden Markov model (HMM)}
A Markov chain ${X_t}_{t\geq 1}$ is observed as $\{Y_t\}_{t\geq 1}$. The state sets are finite sets $S_x, S_y$. Suppose the initial state distribution is $\pi$. The \textit{transition probability matrix} of the MC is
\begin{equation}
    \begin{aligned}
        A(i,j)=P(X_{t+1}=j|X_{t}=i),\ i,j\in S_x
    \end{aligned}
    \nonumber
\end{equation}
and the \textit{emission probability matrix} is
\begin{equation}
    \begin{aligned}
        B(i,j)=P(Y_t=j|X_t=i),\ i\in S_x, j\in S_y
    \end{aligned}
    \nonumber
\end{equation}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.15]{HDM.png}
    \caption{Hidden Markov Model (HMM)}
    \label{}
\end{figure}\end{center}

Relative problems include
\begin{enumerate}[\textbf{Problem} 1:]
    \item Estimate $X_t$ given $Y_{1:t}$ (Using MAP or MMSE criterion: particle filtering)
    \item Estimate $X_{t+1}$ given $Y_{1:t}$ (Using MAP or MMSE prediction: particle filtering)
    \item Estimate $X_{1:t}$ given $Y_{1:t}$ (MAP, MMSE)
    \item Estimate the HMM parameters $\theta=(\pi,A,B)$ given $Y_{1:t}$ (learning)
\end{enumerate}

\section{Viterbi Algorithm: (MAP) estimate $X_{1:t}$ given $Y_{1:t}$}
\subsection{MAP estimation problem}
The MAP estimation problem arises in a variety of applications, and Viterbi derived a remarkable algorithm for solving it exactly. The probability of state $\vec{x}\in S_x^n$ is given by
\begin{equation}
    \begin{aligned}
        P(\vec{x})=\pi(x_1)\prod_{t=1}^{n-1}A(x_t,x_{t+1})
    \end{aligned}
    \nonumber
\end{equation}
and the conditional probability of the observed sequence $\vec{y}$ given the state sequence $\vec{x}$ is
\begin{equation}
    \begin{aligned}
        P(\vec{y}|\vec{x})=\prod_{t=1}^n B(x_t,y_t)
    \end{aligned}
    \nonumber
\end{equation}
Hence, the joint probability of $\vec{x}$ and $\vec{y}$ is
\begin{equation}
    \begin{aligned}
        P(\vec{x},\vec{y})=P(\vec{x})P(\vec{y}|\vec{x})=\pi(x_1)\prod_{t=1}^{n-1}A(x_t,x_{t+1})\prod_{t=1}^n B(x_t,y_t)
    \end{aligned}
    \nonumber
\end{equation}
Then the MAP estimation problem is
\begin{equation}
    \begin{aligned}
        \vec{x}^*&=\argmax_{\vec{x}}P(\vec{x}|\vec{y})=\argmax_{\vec{x}}\frac{P(\vec{x},\vec{y})}{P(\vec{y})}=\argmax_{\vec{x}}P(\vec{x},\vec{y})\\
        &=\argmax_{\vec{x}}\ \pi(x_1)\prod_{t=1}^{n-1}A(x_t,x_{t+1})\prod_{t=1}^n B(x_t,y_t)\\
        &=\argmax_{\vec{x}}\ \ln\pi(x_1)+\sum_{t=1}^{n-1}\ln A(x_t,x_{t+1})+\sum_{t=1}^n\ln B(x_t,y_t)
    \end{aligned}
    \nonumber
\end{equation}

\subsection{Viterbi Algorithm}
Let $f(x_1)=\ln\pi(x_1)+\ln B(x_1,y_1)$, $g_t(x_t,x_{t+1})=\ln A(x_t,x_{t+1})+\ln B(x_{t+1},y_{t+1})$. Then the estimation problem is written in the form
\begin{equation}
    \begin{aligned}
        \vec{x}^*=\argmax_{\vec{x}}\ \left[\varepsilon (\vec{x})=f(x_1)+\sum_{u=1}^{n-1}g_u(x_u,x_{u+1})\right]
    \end{aligned}
    \nonumber
\end{equation}
Let $V(1,x)=f(x)$ and
\begin{equation}
    \begin{aligned}
        V(t,x_t=x)\triangleq \max_{x_1,x_2,...,x_{t-1}}\left[\varepsilon([x_1,...,x_t])=f(x_1)+\sum_{u=1}^{t-1}g_u(x_u,x_{u+1})\right]
    \end{aligned}
    \nonumber
\end{equation}
\begin{equation}
    \begin{aligned}
        V(t,x_t=x)=\max_{x'} \left[V(t-1,x_{t-1}=x')+g_{t-1}(x',x)\right],\ t\geq 2
    \end{aligned}
    \nonumber
\end{equation}
Then, when $t=n$ we have
\begin{equation}
    \begin{aligned}
        \max_{\vec{x}}\varepsilon(\vec{x})=\max_{x} V(n,x_n=x)
    \end{aligned}
    \nonumber
\end{equation}

The complexity of the algorithm is $O(n|S_x|)$ storage and $O(n|S_x|^2)$ computation.

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{dia.png}
    \caption{(a) Trellis diagram; (b)—(e) evolution of the Viterbi algorithm, showing surviving paths and values $V(t,x)$ at times $t = 2,3,4,5$; (f) optimal path $\vec{x}^* = (0,2,0,2,0)$ and its value $\varepsilon(\vec{x}^*) = 11$.}
    \label{}
\end{figure}\end{center}

\section{Bayesian Estimation of a Sequence: Need (MMSE) estimate $X_{1:t}$ given $Y_{1:t}$}
Consider Bayesian estimation under an additive squared-error loss function: $$L(\vec{x},\hat{\vec{x}})=\sum_{t=1}^nL(x_t,\hat{x}_t)=\sum_{t=1}^n\left(x_t-\hat{x}_t\right)^2$$
The Bayesian estimator $\hat{\vec{x}}$ achieves
\begin{equation}
    \begin{aligned}
        \min_{\hat{\vec{x}}}\sum_{\vec{x}\in X^n}L(\vec{x},\hat{\vec{x}})P(\vec{x}|\vec{y})=\sum_{t=1}^n\min_{\hat{\vec{x}}_t}\sum_{x_t\in X}L(x_t,\hat{x}_t)P(x_t|\vec{y})
    \end{aligned}
    \nonumber
\end{equation}
In particular, under squared-error loss, we obtain the conditional mean estimator
\begin{equation}
    \begin{aligned}
        \hat{\vec{x}}_t=\sum_{x_t\in X}x_t P\left(X_t=x|Y=\vec{y}\right),\quad 1\leq t\leq n
    \end{aligned}
    \nonumber
\end{equation}

\section{Forward-Backward Algorithm: (MMSE) estimate $X_{1:t+1}$ given $Y_{1:t}$}
\begin{enumerate}
    \item Evaluate $P\left(X_t=x|Y=\vec{y}\right)$ for $t=1,2,...,n$ and $x\in \mathcal{X}$. (Used to (MMSE) estimate $X_{1:t}$ given $Y_{1:t}$)
    \item Evaluate $P\left(X_t=x,X_{t+1}=x'|Y=\vec{y}\right)$ for $t=1,2,...,n$ and $x,x'\in \mathcal{X}$ (Used to learn parameters $\theta=(\pi,A,B)$)
\end{enumerate}
Define the shorthands
$$
\begin{aligned}
\gamma_t(x) & \triangleq \mathrm{P}\left\{X_t=x \mid \vec{Y}=\vec{y}\right\} \\
\xi_t\left(x, x^{\prime}\right) & \triangleq \mathrm{P}\left\{X_t=x, X_{t+1}=x^{\prime} \mid \vec{Y}=\vec{y}\right\}, \quad x, x^{\prime} \in \mathcal{X}
\end{aligned}
$$
Hence $\gamma_t$ is the first marginal of $\xi_t$. The forward-backward algorithm allows efficient computation of these probabilities.
\subsection{$\gamma_t(x) \triangleq \mathrm{P}\left\{X_t=x \mid \vec{Y}=\vec{y}\right\}$}
We begin with
$$
\gamma_t(x)=\mathrm{P}\left\{X_t=x \mid \vec{Y}=\vec{y}\right\}=\frac{\mathrm{P}\left\{X_t=x, \vec{Y}=\vec{y}\right\}}{\sum_{x \in \mathcal{X}} \mathrm{P}\left\{X_t=x, \vec{Y}=\vec{y}\right\}}, \quad 1 \leq t \leq n
$$
Write the numerator as a product of two conditional distributions,
$$
\begin{aligned}
\mathrm{P}\left\{\vec{Y}=\vec{y}, X_t=x\right\} & \stackrel{(a)}{=} \underbrace{\mathrm{P}\left\{Y_{1: t}=y_{1: t}, X_t=x\right\}}_{\mu_t(x)} \underbrace{\mathrm{P}\left\{Y_{t+1: n}=y_{t+1: n} \mid X_t=x\right\}}_{\nu_t(x)} \\
&=\mu_t(x) \nu_t(x), \quad 1 \leq t<n
\end{aligned}
$$
where (a) follows from the Markov chain $Y_{1: t} \rightarrow X_t \rightarrow Y_{t+1: n}$. For $t=n$, we let $\nu_n(x) \equiv 1$. Combining above two equations we have
$$
\gamma_t(x)=\frac{\mu_t(x) \nu_t(x)}{\sum_{x \in \mathcal{X}} \mu_t(x) \nu_t(x)} .
$$
\begin{enumerate}[(1)]
    \item The first factor in the product of $\mathrm{P}\left\{\vec{Y}=\vec{y}, X_t=x\right\}$ is
    $$
    \mu_t(x)=\mathrm{P}\left\{Y_{1: t}=y_{1: t}, X_t=x\right\}, \quad x \in \mathcal{X}, 1 \leq t \leq n,
    $$
    for which we derive a \textbf{forward recursion}. The recursion is initialized with
    $$
    \mu_1(x)=\mathrm{P}\left\{Y_1=y_1, X_1=x\right\}=\pi(x) B\left(x, y_1\right) .
    $$
    For $t \geq 1$ we express $\mu_{t+1}$ in terms of $\mu_t$ as follows:
    $$
    \begin{aligned}
    \mu_{t+1}(x) &=\mathrm{P}\left\{Y_{1: t+1}=y_{1: t+1}, X_{t+1}=x\right\} \\
    & \stackrel{(a)}{=} \mathrm{P}\left\{Y_{1: t}=y_{1: t}, X_{t+1}=x\right\} \mathrm{P}\left\{Y_{t+1}=y_{t+1} \mid X_{t+1}=x\right\} \\
    &=B\left(x, y_{t+1}\right) \sum_{x^{\prime} \in \mathcal{X}} \mathrm{P}\left\{Y_{1: t}=y_{1: t}, X_{t+1}=x, X_t=x^{\prime}\right\} \\
    & \stackrel{(b)}{=} B\left(x, y_{t+1}\right) \sum_{x^{\prime} \in \mathcal{X}} \mathrm{P}\left\{Y_{1: t}=y_{1: t}, X_t=x^{\prime}\right\} \mathrm{P}\left\{X_{t+1}=x \mid X_t=x^{\prime}\right\} \\
    &=B\left(x, y_{t+1}\right) \sum_{x^{\prime} \in \mathcal{X}} \mu_t\left(x^{\prime}\right) \mathrm{A}\left(x^{\prime}, x\right), \quad t=1,2, \cdots, n-1
    \end{aligned}
    $$
    where (a) holds because $Y_{1: t} \rightarrow X_{t+1} \rightarrow Y_{t+1}$ forms a Markov chain, and (b) because $Y_{1: t} \rightarrow X_t \rightarrow X_{t+1}$ forms a Markov chain.
    \item The second factor in the product of $\mathrm{P}\left\{\vec{Y}=\vec{y}, X_t=x\right\}$ is
    $$
    \nu_t(x)=\mathrm{P}\left\{Y_{t+1: n}=y_{t+1: n} \mid X_t=x\right\}, \quad x \in \mathcal{X}, 1 \leq t<n .
    $$
    Starting from $\nu_n(x) \equiv 1$, we have the following \textbf{backward recursion}, expressing $\nu_{t-1}$ in terms of $\nu_t$ for $2 \leq t \leq n$ :
    $$
    \begin{aligned}
    \nu_{t-1}(x) &=\mathrm{P}\left\{Y_{t: n}=y_{t: n} \mid X_{t-1}=x\right\} \\
    &=\sum_{x^{\prime} \in \mathcal{X}} \mathrm{P}\left\{Y_{t: n}=y_{t: n}, X_t=x^{\prime} \mid X_{t-1}=x\right\} \\
    & \stackrel{(a)}{=} \sum_{x^{\prime} \in \mathcal{X}} \mathrm{P}\left\{Y_{t: n}=y_{t: n} \mid X_t=x^{\prime}\right\} \mathrm{P}\left\{X_t=x^{\prime} \mid X_{t-1}=x\right\} \\
    & \stackrel{(b)}{=} \sum_{x^{\prime} \in \mathcal{X}} \mathrm{P}\left\{Y_{t+1: n}=y_{t+1: n} \mid X_t=x^{\prime}\right\} \mathrm{P}\left\{Y_t=y_t \mid X_t=x^{\prime}\right\} \mathrm{P}\left\{X_t=x^{\prime} \mid X_{t-1}=x\right\} \\
    &=\sum_{x^{\prime} \in \mathcal{X}} \nu_t\left(x^{\prime}\right) \mathrm{B}\left(x^{\prime}, y_t\right) \mathrm{A}\left(x, x^{\prime}\right), \quad t=n, n-1, \cdots, 2
    \end{aligned}
    $$
    where (a) holds because $X_{t-1} \rightarrow X_{t} \rightarrow Y_{t:n}$ forms a Markov chain, and (b) because $Y_{t+1: n} \rightarrow X_t \rightarrow Y_t$ forms a Markov chain.
\end{enumerate}

\subsection{$\xi_t\left(x, x^{\prime}\right) \triangleq \mathrm{P}\left\{X_t=x, X_{t+1}=x^{\prime} \mid \vec{Y}=\vec{y}\right\}$}
Next we derive an expression for
$$
\xi_t\left(x, x^{\prime}\right)=\mathrm{P}\left\{X_t=x, X_{t+1}=x^{\prime} \mid \vec{Y}=\vec{y}\right\}=\frac{\mathrm{P}\left\{\vec{Y}=\vec{y}, X_t=x, X_{t+1}=x^{\prime}\right\}}{\sum_{x, x^{\prime} \in \mathcal{X}} \mathrm{P}\left\{\vec{Y}=\vec{y}, X_t=x, X_{t+1}=x^{\prime}\right\}}
$$
We have
\begin{equation}
    \begin{aligned}
        \mathrm{P}&\left\{\vec{Y}=\vec{y}, X_t=x, X_{t+1}=x^{\prime}\right\}\\
        &\stackrel{(a)}{=} \mathrm{P}\left\{Y_{1: t+1}=y_{1: t+1}, X_t=x, X_{t+1}=x^{\prime}\right\} \mathrm{P}\left\{Y_{t+2: n}=y_{t+2: n} \mid X_{t+1}=x^{\prime}\right\}\\
        &\stackrel{(b)}{=} \mathrm{P}\left\{Y_{1: t}=y_{1: t}, X_t=x\right\} \mathrm{P}\left\{X_{t+1}=x^{\prime} \mid X_t=x\right\} \mathrm{P}\left\{Y_{t+1}=y_{t+1} \mid X_{t+1}=x^{\prime}\right\} \nu_{t+1}\left(x^{\prime}\right)\\
        &=\mu_t(x) \mathrm{A}\left(x, x^{\prime}\right) \mathrm{B}\left(x^{\prime}, y_{t+1}\right) \nu_{t+1}\left(x^{\prime}\right)
    \end{aligned}
    \nonumber
\end{equation}
where (a) holds because $\left(Y_{1: t+1}, X_t\right) \rightarrow X_{t+1} \rightarrow Y_{t+2: n}$ forms a Markov chain, and (b) because $Y_{1: t} \rightarrow X_t \rightarrow X_{t+1} \rightarrow Y_{t+1}$ forms a Markov chain. Hence
$$
\xi_t\left(x, x^{\prime}\right)=\frac{\mu_t(x) \mathrm{A}\left(x, x^{\prime}\right) \mathrm{B}\left(x^{\prime}, y_{t+1}\right) \nu_{t+1}\left(x^{\prime}\right)}{\sum_{x, x^{\prime} \in \mathcal{X}} \mu_t(x) \mathrm{A}\left(x, x^{\prime}\right) \mathrm{B}\left(x^{\prime}, y_{t+1}\right) \nu_{t+1}\left(x^{\prime}\right)}, \quad 1 \leq t \leq n, x, x^{\prime} \in \mathcal{X}
$$

\subsection{Scaling Factors}
Unfortunately the recursions above are numerically unstable for large $n$ because the probabilities $\mu_t(x)$ and $\nu_t(x)$ vanish exponentially with $n$ and are sums of many small terms of different sizes. The following approach is more stable. Define
$$
\begin{aligned}
\alpha_t(x) &=\mathrm{P}\left\{X_t=x \mid Y_{1: t}=y_{1: t}\right\}, \\
\beta_t(x) &=\frac{\mathrm{P}\left\{Y_{t+1: n}=y_{t+1: n} \mid X_t=x\right\}}{\mathrm{P}\left\{Y_{t+1: n}=y_{t+1: n} \mid Y_{1: t}=y_{1: t}\right\}}, \\
c_t &=\mathrm{P}\left\{Y_t=y_t \mid Y_{1: t-1}=y_{1: t-1}\right\}
\end{aligned}
$$
Then
$$
\begin{aligned}
\gamma_t(x) &=\alpha_t(x) \beta_t(x) \\
\xi_t\left(x, x^{\prime}\right) &=c_t \alpha_t(x) \mathrm{B}\left(x, y_t\right) \mathrm{A}\left(x, x^{\prime}\right) \beta_t\left(x^{\prime}\right)
\end{aligned}
$$
A forward recursion can be derived for $\alpha_t$ and $c_t$, and a backward recursion for $\beta_t$.

The time and storage complexity of the algorithm is $O\left(n|\mathcal{X}|^2\right)$.










\input{ML_tex/Graphic Models}



















\chapter{Variational Inference, Mean-Field Techniques}
Approximate complicated p.d.f. $p(\vec{x})$ with tractable $q(\vec{x})$, where $q\in Q=$ tractable set of distributions.

Use divergence to measure:
\begin{equation}
    \begin{aligned}
        \min_{q\in Q}\quad D(q\| p)
    \end{aligned}
    \nonumber
\end{equation}
note that $D$ is convex in $q$.

\section{Naive Mean-Field Methods}
The \textit{naive mean field method} approximates a distribution by a product distribution.

Assume the $q$ has the form $q(\vec{x})=\prod_{i=1}^nq_i (x_i)$. Assume $x_i\in X=$ finite set.
\begin{equation}
    \begin{aligned}
        D(q\| p)&=\mathbb{E}_q\left[\ln\frac{q(\vec{X})}{p(\vec{X})}\right]\\
        &=\sum_{i=1}^n \mathbb{E}_{q}\left[\ln q_i(X_i)\right]- \mathbb{E}_q\left[\ln p(\vec{X})\right]\\
        &=\sum_{i=1}^n\sum_{x_i\in X}q_i(x_i)\ln q_i(x_i)-\sum_{\vec{x}\in X^n}\left(\prod_{i=1}^nq_i (x_i)\right)\ln p(\vec{x})
    \end{aligned}
    \nonumber
\end{equation}

Solve 
\begin{equation}
    \begin{aligned}
        \min_{\{q_i\}}&\quad D(\prod_{i=1}^nq_i\| p)\\
        s.t.&\sum_{x_i\in X}q_i(x_i)=1,i=1,...,n
    \end{aligned}
    \nonumber
\end{equation}
Using Lagrangian method:
\begin{equation}
    \begin{aligned}
        L(q,\vec{\lambda})&=D(q\| p)+\sum_{i=1}^n\lambda_i\left(\sum_{x\in X}q_i(x_i)-1\right)\\
        0=\frac{\partial L(q,\vec{\lambda})}{\partial q_i(x_i)}&=1+\ln q_i(x_i)-\sum_{\vec{x}':x'_i=x_i}\left(\prod_{j\neq i}^nq_j (x'_j)\right)\ln p(\vec{x}')+\lambda_i
    \end{aligned}
    \nonumber
\end{equation}
Hence, $q_i(x_i)$ should in the form:
\begin{equation}
    \begin{aligned}
        q_i(x_i)&=\frac{1}{e^{1+\lambda_i}}e^{\sum_{\vec{x}':x'_i=x_i}\left(\prod_{j\neq i}^nq_j (x'_j)\right)\ln p(\vec{x}')}\\
        &=\frac{1}{Z_i}exp\left({\mathbb{E}_{\prod_{j\neq i}^nq_j}[\ln p(X_{1:i-1},x_i,X_{i+1:n})]}\right)
    \end{aligned}
    \nonumber
\end{equation}
Iteration Algorithm:
\begin{equation}
    \begin{aligned}
        q_i^{(k+1)}(x_i)=\frac{1}{Z_i}exp\left({\mathbb{E}_{\prod_{j\neq i}^nq_j^{(k)}}[\ln p(X_{1:i-1},x_i,X_{i+1:n})]}\right)
    \end{aligned}
    \nonumber
\end{equation}

\subsection{Graphical Models}
Consider $P=$ pairwise Markov model
\begin{equation}
    \begin{aligned}
        p(\vec{x})&=\frac{1}{Z}\prod_{(i,j)\in E}\psi_{ij}(x_i,x_j)\\
        \ln p(\vec{x})&=-\ln{Z}+\sum_{(i,j)\in E}\ln\psi_{ij}(x_i,x_j)\\
    \end{aligned}
    \nonumber
\end{equation}
The expectation can be written as
\begin{equation}
    \begin{aligned}
        {\mathbb{E}_{\prod_{j\neq i}^nq_j}[\ln p(X_{1:i-1},x_i,X_{i+1:n})]}&=\mathbb{E}_{\prod_{j\neq i}^nq_j}[\sum_{(i,j)\in E}\ln\psi_{ij}(x_i,x_j)]+cst\\
        &=\sum_{j\in N(i)}\mathbb{E}_{q_j}[\ln\psi_{ij}(x_i,x_j)]+cst\\
        &=\sum_{j\in N(i)}\sum_{x_j\in X}q_j(x_j)\ln\psi_{ij}(x_i,x_j)+cst
    \end{aligned}
    \nonumber
\end{equation}
and thus,
\begin{equation}
    \begin{aligned}
        q_i(x_i)
        &=\frac{1}{Z_i}exp\left(\sum_{j\in N(i)}\sum_{x_j\in X}q_j(x_j)\ln\psi_{ij}(x_i,x_j)\right)
    \end{aligned}
    \nonumber
\end{equation}

\subsection{Ising Model}
Consider a 2-D torus $V$ with $|V| = n$ nodes, and $X = \{\pm 1\}$. Each node is connected to its upper, lower, right, and left neighbors. The distribution is of the form
\begin{equation}
    \begin{aligned}
        p(\vec{x})=\frac{1}{Z}exp\left(\beta\sum_{(i,j)\in E}x_ix_j\right),\quad \vec{x}\in \{\pm 1\}^n
    \end{aligned}
    \nonumber
\end{equation}
with $\beta \geq 0$. The parameter $\beta$ represents the inverse of a temperature. For $\beta = 0$ the distribution is uniform, hence fully factorized. For large positive values of $\beta$, configurations $\vec{x}$ with strong correlations are favored.

\textbf{2-D Ising Model}
$$\psi_{ij}(x_i,x_j)=e^{\beta x_ix_j}$$
\begin{equation}
    \begin{aligned}
        \Rightarrow q_i(x_i)=\frac{1}{Z_i}\prod_{j\in N(i)}exp\left(-\sum_{x_j=\pm 1}q_j(x_j)\beta x_i x_j\right)
    \end{aligned}
    \nonumber
\end{equation}

Since each $X_i$ is a Bernoulli random variable, the decision varibale $q_i$ can be represented by a single parameter which we choose to be the mean $m_i = q_i(1) - q_i(-1) \in [-1, 1]$. Equivalently,
\begin{equation}
    \begin{aligned}
        m_i = q_i(1) - q_i(-1)&\Leftrightarrow q_i(1)=\frac{1+m_i}{2},\ q_i(-1)=\frac{1-m_i}{2}\\
        &\Rightarrow q_i(x_i)=\frac{1}{Z_i}\prod_{j\in N(i)}exp\left(-\beta x_i m_j\right)
    \end{aligned}
    \nonumber
\end{equation}
Then, our problem is finding the optimal $\{m_i\}$.

\begin{equation}
    \begin{aligned}
        q_i(1)=\frac{1}{Z_i}exp\left(-\beta\sum_{j\in N(i)} m_j\right);\ q_i(-1)=\frac{1}{Z_i}exp\left(\beta\sum_{j\in N(i)} m_j\right)
    \end{aligned}
    \nonumber
\end{equation}
The normalization constant is given by
\begin{equation}
    \begin{aligned}
        Z_i=exp\left(\beta\sum_{j\in N(i)} m_j\right)+exp\left(-\beta\sum_{j\in N(i)} m_j\right)=2{cosh}\left(\beta\sum_{j\in N(i)}m_j\right)
    \end{aligned}
    \nonumber
\end{equation}
Hence, $$m_i=q_i(1)-q_i(-1)={tanh}\left(\beta\sum_{j\in N(i)}m_j\right)$$
\textbf{Convergence}. We show that the algorithm always converges if a uniform initialization is used, i.e., $m_i^{(0)} = m^{(0)}$ for all $i \in V$. Then the (simultaneous) update equation for the means is
\begin{equation}
    \begin{aligned}
        m^{(k+1)}={tanh}\left(4\beta m^{(k)}\right)
    \end{aligned}
    \nonumber
\end{equation}
where the factor of $4$ arises because each vertex has $4$ neighbors. Analysis of convergence depends on the value of $\beta$, and we need consider two cases.
\begin{enumerate}[(1)]
    \item \textbf{Case I: $\beta < \frac{1}{4}$:} The mapping is a contraction mapping for $\beta<\frac{1}{4}$, and so the fixed point of this mapping is $\lim_{k \rightarrow \infty} m^{(k)} = 0$, for any initialization $m^{(0)}$. Hence, the variational approximation is uniform: $q(\vec{x}) = 2^{-n}$ for all $x \in \{\pm 1\}^V$.
    \item \textbf{Case II: $\beta > \frac{1}{4}$:} In this case, the equation $m=\tanh (4 \beta m)$ has three possible solutions 0 and $\pm m^*$ where $m^*>0$. If the algorithm is initialized with $m^{(0)}=0$, then subsequent iterations do not change this value. If the algorithm is initialized with $m^{(0)}>0$, it converges to $m^*$. Finally, if the algorithm is initialized with $m^{(0)}<0$, it converges to $-m^*$. In the latter two cases (convergence to either $m^*$ or $-m^*$), the variational approximations $q_i$ are nonuniform.
    \item \textbf{Case III: $\beta = \frac{1}{4}$:} phase transition
\end{enumerate}
The case $\beta>\frac{1}{4}$ is related to percolation theory in statistical physics. It may be shown that the distribution $p$ favors configurations featuring large homogeneous regions. The correlation between any two nodes is significant, even for large graphs. This behavior is completely different from the case $\beta<\frac{1}{4}$, where the correlation between distant nodes dies out with distance (similarly to a homogeneous, irreducible Markov chain). The case $\beta=\frac{1}{4}$ is known as a phase transition.

\section{Exponential Families of Probability Distributions}
\begin{definition}[$d-$dimensional exponential families]
    In canonical form: $d-$dimensional exponential families
    $$p_\theta(y)=\frac{h(y)}{Z(\theta)} e^{\sum_{k=1}^d\theta_k T_k(y)}$$
    $T_k (\cdot)$ are called sufficient statistics. \textbf{Partition function} $Z(\theta)$ is the normalization constant ensuring that the density $p_\theta$ integrates to $1$.\\
    It can also be written
    $$p_\theta(y)=e^{\theta^T T(y)-A(\theta)}$$
    The \textbf{log partition function} (aka cumulant function) $A(\theta) = ln Z(\theta)$
\end{definition}

\textbf{Example 1:} $P_\theta=N(\theta,1)$, $$p_\theta(y)=\frac{1}{\sqrt{2\pi}}e^{-\frac{(y-\theta)^2}{2}}=\frac{e^{\frac{y^2}{2}}}{\sqrt{2\pi}e^{-\frac{\theta^2}{2}}}e^{-\theta y}$$

\textbf{Example 2:} $P_\theta=N(0,\theta^{-1})$, $\theta$ is the inverse covariance matrix. $$p_\theta(y)=\frac{|\theta|^{\frac{1}{2}}}{\sqrt{2\pi}}e^{-\frac{1}{2}y^T\theta y}$$

\textbf{Example 3:} 2D-Ising Model $$p_\theta(y)=\frac{1}{Z(\theta)}e^{\theta y_iy_j},\theta>0$$
Generalized 2D-Ising Model $$p_\theta(y)=\frac{1}{Z(\theta)}e^{\sum_{i\in V}\theta_i y_i +\sum _{i\sim j}\theta_{ij} y_iy_j}$$

The natural parameter set:
$$\Theta=\{\theta:\int_X e^{\theta^T T(y)}dy<\infty\}$$

The divergence in exponential form
\begin{equation}
    \begin{aligned}
        D(P_\theta\| P_{\theta'})&=\mathbb{E}_\theta \left[\ln \frac{P_\theta (Y)}{P_{\theta'} (Y)}\right]\\
        &=\mathbb{E}_\theta[\theta^T T(Y)-A(\theta)-(\theta')^T T(Y)+A(\theta')]\\
        &=-[A(\theta)-A(\theta')]+(\theta-\theta')^T \mathbb{E}_\theta [T(Y)]
    \end{aligned}
    \nonumber
\end{equation}

\textbf{Cumulant-generating function} (cgf)
\begin{equation}
    \begin{aligned}
        \kappa(u)=\ln \mathbb{E}(e^{u^TT(Y)})=\ln \int e^{u^TT(y)}(e^{\theta^T T(y)-A(\theta)}) dy
    \end{aligned}
    \nonumber
\end{equation}
\begin{equation}
    \begin{aligned}
    \nabla \kappa(0)&=\mathbb{E}_\theta[T(Y)] \\
    \nabla^2 \kappa(0)&=\operatorname{Cov}_\theta[T(Y)]
    \end{aligned}
    \nonumber
\end{equation}

We can compute
\begin{equation}
    \begin{aligned}
        \int e^{\theta^T T(y)-A(\theta)} dy&=1\\
        \int [T(y)-\nabla A(\theta)] e^{\theta^T T(y)-A(\theta)} dy&=0\\
        \int[T(y)-\nabla A(\theta)]p_\theta(y) dy&=0\\
        \mathbb{E}_\theta [T(Y)]&=\nabla A(\theta)
    \end{aligned}
    \nonumber
\end{equation}
Hence,
\begin{equation}
    \begin{aligned}
    \nabla A(\theta) &=\nabla \kappa(0)=\mathbb{E}_\theta[T(X)] \\
    \nabla^2 A(\theta) &=\nabla^2 \kappa(0)=\operatorname{Cov}_\theta[T(X)]
    \end{aligned}
    \nonumber
\end{equation}

Definition. The set of realizable mean parameters $\mathcal{M}$ is the set of $\mu$ that are the expected value of $T(X)$ under some distribution $p$ (not necessarily in the exponential family). Thus
$$
\mathcal{M} \triangleq\left\{\mu \in \mathbb{R}^d: \exists p: \mathbb{E}_p[T(X)]=\mu\right\}
$$
which is a convex set.\\

\textbf{Example 6.} Consider Generalized 2D-Ising Model $$p_\theta(y)=\frac{1}{Z(\theta)}e^{\sum_{i\in V}\theta_i y_i +\sum _{i\sim j}\theta_{ij} y_iy_j}$$
For $y\in\{0,1\}$
\begin{equation}
    \begin{aligned}
        \mu_i&=\mathbb{E}_\theta \left[T_i(Y)\right]=P_\theta (Y_i=1)\\
        \mu_{ij}&=\mathbb{E}_\theta \left[T_i(Y)T_j(Y)\right]=P_\theta (Y_i=Y_j=1)\\
    \end{aligned}
    \nonumber
\end{equation}
\textbf{Example 7.} If $T(x)=x x^{\top} \in \mathbb{R}^{n \times n}$ then $\mu$ is a correlation matrix, and so $\mathcal{M}$ is the set of all $n \times n$ symmetric nonnegative definite matrices.

\section{ML Estimation}
Consider $n$ iid samples $X^{(i)}, 1 \leq i \leq n$ drawn from the exponential distribution $p_\theta$. The ML estimator of $\theta$ given these $n$ samples is obtained by solving
$$
\begin{aligned}
\hat{\theta}_{ML}&=\max _\theta \frac{1}{n} \sum_{i=1}^n \ln p_\theta\left(X^{(i)}\right)\\
&=\max _\theta \frac{1}{n} \sum_{i=1}^n\left[\theta^{\top} T\left(X^{(i)}\right)-A(\theta)\right] \\
&=\max _\theta\left[\theta^{\top} \hat{\mu}-A(\theta)\right]
\end{aligned}
$$
where $\hat{\mu}$ is the mean parameter
$$
\hat{\mu} \triangleq \frac{1}{n} \sum_{i=1}^n T\left(X^{(i)}\right)
$$
$A(\theta)$ is a convex function, we can solve optimal solution by solving critical point.
\begin{equation}
    \begin{aligned}
        \nabla A(\hat{\theta}_{ML})=\hat{\mu}
    \end{aligned}
    \nonumber
\end{equation}
The gradient mapping could be hard to invert, however. For instance, for the Ising model example we easily obtain
$$
\hat{\mu}=\frac{1}{n} \sum_{i=1}^n \sum_{j \sim k} X_j^{(i)} X_k^{(i)}
$$
but inverting the gradient mapping is a hard problem. Such is generally the case if $p$ is a distribution over a Markov network with cycles.

\section{Maximum Entropy}
Consider a random variable $X$ over a finite set $\mathcal{X}$. Its probability distribution $p$ is unknown, however we are given the expected value $\mu_k=\mathbb{E}_p\left[T_k(X)\right]$ of $d$ statistics $T_k(X), 1 \leq k \leq d$. A classical problem, which originates from statistical physics, is to find $p$ that maximizes the entropy $H(p)=-\sum_x p(x) \ln p(x)$ subject to the $d$ constraints above. Assuming the feasible set is nonvoid, the resulting distribution is called the maximum-entropy (or maxent()) distribution.

Since $H(p)$ is concave, the constraints are linear in $p$, and the probability simplex is a convex set, the maxent problem is concave. Its solution is obtained by introducing $d$ Lagrange multipliers $\lambda_k, 1 \leq k \leq d$ associated with the mean constraints, and a Lagrange multiplier $\lambda_{d+1}$ associated with the constraint $\sum_x p(x)=1$. Ignoring momentarily the nonnegativity constraints, we maximize the Lagrangian
$$
\mathcal{L}(p, \lambda) \triangleq-\sum_{x \in \mathcal{X}} p(x) \ln p(x)+\sum_{k=1}^d \lambda_k\left(\sum_{x \in \mathcal{X}} p(x) T_k(x)-\mu_k\right)+\lambda_{d+1}\left(\sum_{x \in \mathcal{X}} p(x)-1\right)
$$
over $p$, subject to the $d+1$ equality constraints

The first-order optimality conditions are given by
\begin{equation}
    \begin{aligned}
        0=\frac{\partial \mathcal{L}(p, \lambda)}{\partial p(x)}=-\ln p(x)-1+\sum_{k=1}^d\lambda_kT_k(x)+\lambda_{d+1}
    \end{aligned}
    \nonumber
\end{equation}
Hence, $$p(x)=\frac{1}{Z}e^{\sum_{k=1}^d\lambda_kT_k(x)}$$
where $Z=e^{1-\lambda_{d+1}}$

\begin{equation}
    \begin{aligned}
        H(p)=\mathbb{E}_p[\ln p(X)]=-\theta^T \mathbb{E}_p[T(X)]+A(\theta)=-\max_\theta(\theta^T \mu-A(\theta))
    \end{aligned}
    \nonumber
\end{equation}

\textbf{Example.} Let $X=\left(X_1, X_2\right) \in\{0,1\}^2$ and consider maximizing entropy subject to the constraint $\mathbb{E}\left[X_1 X_2\right]=\mu$ where $\mu \in(0,1)$. We obtain $p(x)=\frac{1}{Z} \exp \left\{\lambda x_1 x_2\right\}$. Since $\sum_x p(x)=1$, the normalization constant is obtained as $Z=e^\lambda+3$. We obtain $\lambda$ from the constraint
$$
\mu=\mathbb{E}_P\left[X_1 X_2\right]=\frac{e^\lambda}{e^\lambda+3} \quad \Rightarrow \quad \lambda=\ln \frac{3 \mu}{1-\mu}
$$
The maxent solution takes the form
$$
p(x)=\left\{\begin{matrix}
    \mu &\left(x_1, x_2\right)=(1,1) \\ \frac{1-\mu}{3} & \text { else }
\end{matrix}\right.
$$
and has entropy is $H(p)=-\mu \ln \mu-(1-\mu) \ln \frac{1-\mu}{3}$.\\
A similar version of the maxent problem exists for continuous random variables. The entropy function is replaced with the differential entropy functional $h(p) \triangleq-\int p \ln p$, and the maxent solution again takes an exponential form.


\section{}
\begin{equation}
    \begin{aligned}
        \min_{q\in Q}\ D(q\| p)&=\min_q \mathbb{E}_q\left[\ln\frac{q(x)}{p_\theta(x)}\right]\\
        &=\min_q \left[A(\theta)-\theta^T \mathbb{E}_q[T(x)]-H(q)\right]\\
        &=A(\theta)-\max_\mu \max_{q: \mathbb{E}_q[T(x)]=\mu}\left[\theta^T \mathbb{E}_q [T(x)]+H(q)\right]\\
        &=A(\theta)-\max_{\mu\in M}\left[\theta^T \mu +\max_{q: \mathbb{E}_q[T(x)]=\mu}H(q)\right]
    \end{aligned}
    \nonumber
\end{equation}
Since $\max_{q: \mathbb{E}_q[T(x)]=\mu}H(q)$ is exactly an entropy maximum problem, we let $A^*(\mu)=\max_{q: \mathbb{E}_q[T(x)]=\mu}H(q)$.

As we showed: (1). $A^*(\mu)=\max_\theta [\theta^T\mu- A(\theta)]$, $A(\theta)=\max_\mu [\theta^T\mu-A^*(\mu)]$
\begin{equation}
    \begin{aligned}
        \min_{q\in Q}\ D(q\| p)&=A(\theta)-\max_{\mu\in M}\left[\theta^T \mu +A^*(\mu)\right]
    \end{aligned}
    \nonumber
\end{equation}


\section{Connection between Exponential Families and Graphic Models}

Pairwise Markov network over $G(V,E)$
\begin{equation}
    \begin{aligned}
        p(\vec{x})&=\frac{1}{Z}\left(\prod_{i\in V}\psi_i(x_i)\right)\left(\prod_{(i,j)\in E}\psi_{ij}(x_ix_j)\right)\\
        &=\frac{1}{Z}e^{\sum_{i\in V}\ln \psi_i(x_i)+\sum_{(i,j)\in E}\ln \psi_{ij}(x_ix_j)}\\
        &=\frac{1}{Z}e^{\sum_{i\in V}\sum_{x\in X}\ln \psi_i(x_i) \mathbf{1}_{x=i}+\sum_{(i,j)\in E}\sum_{x,x'\in X}\ln \psi_{ij}(x_ix_j)\mathbf{1}_{x=i,x'=j}}
    \end{aligned}
    \nonumber
\end{equation}
We can let 
\begin{equation}
    \begin{aligned}
        T_{ix}(x)&=\mathbf{1}_{x_i=x},& \forall i\in V, \forall x\in X\\
        \theta_i(x)&=\ln\psi_i(x),&\forall i\in V, \forall x\in X\\
        T_{ijxx'}(x,x')&=\mathbf{1}_{x_i=x,x_i=x'},& \forall (i,j)\in E, \forall x,x'\in X\\
        \theta_{ij}(x)&=\ln\psi_{ij}(x,x'),&\forall (i,j)\in E, \forall x,x'\in X\\
    \end{aligned}
    \nonumber
\end{equation}
The probability can be transformed into exponential families.

The dimension of this family is $d = |V||X| + |E||X|^2$.

%\textbf{Binary Example:} Consider the case $X=\{0,1\}$ binary pairwise markov network.
%
%$|V|=2$,
%\begin{equation}
%    \begin{aligned}
%        p(x_1,x_2)&=\frac{1}{Z}\psi_1(x_1)\psi_2(x_2)\psi_{12}(x_1,x_2)\\
%        &=\frac{1}{Z}e^{\ln \psi_1(x_1)+\ln \psi_2(x_2) +\theta (x_1\land x_2)}
%    \end{aligned}
%    \nonumber
%\end{equation}
%In this form, if $\theta>0$ $\Rightarrow$ $12$ is attractive edge; if $\theta<0$ $\Rightarrow$ $12$ is repulsive edge.
%
%$|V|=3$,
%\begin{equation}
%    \begin{aligned}
%        p(x_1,x_2,x_3)&=\frac{1}{Z}\psi_1(x_1)\psi_2(x_2)\psi_3(x_3)\psi_{12}(x_1,x_2)\psi_{23}(x_2,x_3)\psi_{13}(x_1,x_3)\\
%        &=\frac{1}{Z}\psi_1(x_1)\psi_2(x_2)\psi_3(x_3)e^{\sum_{(i,j)\in E}\theta_{ij} (x_i\land x_j)}
%    \end{aligned}
%    \nonumber
%\end{equation}
%
%More general:
%\begin{equation}
%    \begin{aligned}
%        p(\vec{x})=\frac{1}{Z}e^{\sum_{i\in V}\theta_i x_i+\sum_{(i,j)\in E}\theta_{ij} (x_i\land x_j)}
%    \end{aligned}
%    \nonumber
%\end{equation}
%In this form, if $\theta_{ij}>0$ $\Rightarrow$ $ij$ is attractive edge; if $\theta_{ij}<0$ $\Rightarrow$ $ij$ is repulsive edge.
%



\subsection{Marginal polytope}
\begin{definition} [marginal polytope]
    The mean parameters associated with the distribution $p_\theta$ are the 1-dimensional marginals
    for the vertices,
    \begin{equation}
        \begin{aligned}
            \mu_{i}(x)&=\mathbb{E}_\theta[T_{ix}(x)]=P_\theta(x_i=x)=\text{marginal distribution of }X_i
        \end{aligned}
        \nonumber
    \end{equation}
    and the pairwise marginals associated with the edge set $E$,
    \begin{equation}
        \begin{aligned}
            \mu_{ij}(x,x')&=\mathbb{E}_\theta[T_{ijxx'}(x,x')]=P_\theta(x_i=x,x_j=x')=\text{2D marginal distribution of }(X_i,X_j)
        \end{aligned}
        \nonumber
    \end{equation}
    The \underline{set of realizable mean parameters}, $\mathcal{M}$, is then called the \textbf{marginal polytope} and denoted
    by $\mathcal{M}(G)$.
\end{definition}

\subsection{Locally Consistent Marginal Distributions}
Given a graph $G = (V,E)$, consider the set of marginal distributions $\tau_i$ on individual nodes $i \in V$ and pairwise marginals $\tau_{ij}$ on edges $(i, j) \in E$ that are locally consistent in the sense

\begin{equation}
    \begin{aligned}
        \sum_x \tau_i(x)&=1,\ \tau_i(x)\geq 0,& \forall i\in V,x\in X\\
        \sum_{x,x'}\tau_{ij}(x,x')&=1,\ \tau_{ij}(x,x')\geq 0,& \forall (i,j)\in E,x,x'\in X\\
        \sum_{x_j\in X}\tau_{ij}(x_i,x_j)&=\tau_i(x_i),& \forall (i,j)\in E, x_i,x_j\in X\\
        \sum_{x_i\in X}\tau_{ij}(x_i,x_j)&=\tau_j(x_j),& \forall (i,j)\in E, x_i,x_j\in X
    \end{aligned}
    \nonumber
\end{equation}

\begin{definition}[ local marginal polytope]
    The \textbf{local marginal polytope} $\mathcal{L}(G)$ is the set of $\tau=\left(\left\{\tau_i\right\}_{i \in V},\left\{\tau_{i j}\right\}_{(i, j) \in E}\right)$ that satisfy the above consistency conditions.
\end{definition}

This is a fairly simple polytope defined by $|V|+\left(2|X|+|X|^2\right)|E|$ linear constraints. Clearly the marginal polytope $\mathcal{M}(G)$ is a subset of $\mathcal{L}(G)$, but is the converse true?

\begin{proposition}[ For foster $\mathcal{M}(G)=\mathcal{L}(G)$]
    If $G=(V,E)$ is a forest then $\mathcal{M}(G)=\mathcal{L}(G)$. Any probability distribution on $G$ can be expressed as follows in terms of its $1-D$ and pairwise marginals:
    $$
    p(\vec{x})=\left(\prod_{i \in V} \mu_i\left(x_i\right)\right)\left(\prod_{(i, j) \in E} \frac{\mu_{i j}\left(x_i, x_j\right)}{\mu_i\left(x_i\right) \mu_j\left(x_j\right)}\right)
    $$
\end{proposition}
\begin{proof}
    We first prove the claim for a tree. Any tree can be generated starting from a single node and adding one edge at a time. The claim can be proven by induction. It clearly holds for a graph consisting of two nodes and a single edge $(i, j)$, since $p(x)=$ $\mu_{i j}\left(x_i, x_j\right)$ in this case. If a new node $k$ and a new edge $(j, k)$ are added to an existing tree $\left(\mathcal{V}^{\prime}, \mathcal{E}^{\prime}\right)$ where $j \in \mathcal{V}^{\prime}$, we obtain a new tree $(\mathcal{V}, \mathcal{E})$ with $\mathcal{V}=\mathcal{V}^{\prime} \cup\{k\}$ and $\mathcal{E}=\mathcal{E}^{\prime} \cup\{(j, k\}$. If
    $$
    p\left(\boldsymbol{x}_{\mathcal{V}^{\prime}}\right)=\left(\prod_{i \in \mathcal{V}^{\prime}} \mu_i\left(x_i\right)\right)\left(\prod_{(i, j) \in \mathcal{E}^{\prime}} \frac{\mu_{i j}\left(x_i, x_j\right)}{\mu_i\left(x_i\right) \mu_j\left(x_j\right)}\right)
    $$
    then
    $$
    p(\boldsymbol{x})=p\left(\boldsymbol{x}_{\mathcal{V}^{\prime}}, x_k\right)=p\left(\boldsymbol{x}_{\mathcal{V}^{\prime}}\right) p\left(x_k \mid x_j\right)=p\left(\boldsymbol{x}_{\mathcal{V}^{\prime}}\right) \frac{\mu_{j k}\left(x_j, x_k\right)}{\mu_j\left(x_j\right)}
    $$
    satisfies the claim. Next, if the forest contains more than one tree, the distribution $p(\boldsymbol{x})$ factors over the trees, and the claim still holds.
    
    Finally, to any $\mu \in \mathcal{L}(\mathcal{G})$ we can associate a global distribution $p$ using the claim. The marginals and pairwise marginals of $p$ are given by $\mu$, hence $\mu \in \mathcal{M}(\mathcal{G})$. This proves the first part of the claim.
\end{proof}
\begin{example}
    However for a graph that is not a tree, $\mathcal{M}(G)$ is in general a strict subset of $\mathcal{L}(G)$. Consider
    for instance the $3$-cycle with node set $V = \{1, 2, 3\}$ and edge set $E = \{(1; 2); (2; 3); (3; 1)\}$.
    Let $X=\{0,1\}$ and consider $\tau_1,\tau_2,\tau_3$ that are uniform over $X$, and
    \begin{equation}
        \begin{aligned}
            \tau_{12}=\tau_{23}=\begin{bmatrix}
                0.5-\varepsilon&\varepsilon\\
                \varepsilon&0.5-\varepsilon
            \end{bmatrix},\ \tau_{31}=\begin{bmatrix}
                \varepsilon&0.5-\varepsilon\\
                0.5-\varepsilon&\varepsilon
            \end{bmatrix}
        \end{aligned}
        \nonumber
    \end{equation}
\end{example}
for some $\epsilon \in(0,0.5)$. By inspection, $\tau \in \mathcal{L}(\mathcal{G})$. However, for $\epsilon$ small enough, the definitions of $\tau_{12}, \tau_{23}, \tau_{31}$ imply respectively that $X_1=X_2, X_2=X_3$, and $X_3 \neq X_1$ with high probability. These conditions are incompatible, hence $\tau \notin \mathcal{M}(\mathcal{G})$.

In this example, the edge set is small, and it is relatively easy to determine that $\tau \notin \mathcal{M}(\mathcal{G})$. For a large graph, this would generally not be computationally feasible. Since $\tau$ may not be the marginals of any joint distribution on $\mathcal{G}, \tau$ are often referred to as \textbf{pseudomarginals}.

\begin{definition}[pseudomarginal]
    Marginal distribution $\tau$ such that $\tau\in \mathcal{L}(\mathcal{G})$ and $\tau\notin \mathcal{M}(\mathcal{G})$ is called \textbf{pseudomarginals}.
\end{definition}



\subsection{Entropy on Tree Graphs}
Any distribution $p$ defined on a tree graph is of the form $p(\vec{x})=\left(\prod_{i \in V} \mu_i\left(x_i\right)\right)\left(\prod_{(i, j) \in E} \frac{\mu_{i j}\left(x_i, x_j\right)}{\mu_i\left(x_i\right) \mu_j\left(x_j\right)}\right)$. Hence, its entropy is given by
$$
\begin{aligned}
H(p) &=\mathbb{E}_p[-\ln p(\vec{X})] \\
&=\sum_{i \in V} \mathbb{E}_p\left[-\ln \mu_i\left(X_i\right)\right]-\sum_{(i, j) \in E} \mathbb{E}_p\left[\ln \frac{\mu_{i j}\left(X_i, X_j\right)}{\mu_i\left(X_i\right) \mu_j\left(X_j\right)}\right] \\
&=\sum_{i \in V} H\left(\mu_i\right)-\sum_{(i, j) \in E} I\left(\mu_{i j}\right)
\end{aligned}
$$
where
$$
\begin{aligned}
I\left(\mu_{i j}\right) & \triangleq \mathbb{E}_{\mu_{i j}}\left[\ln \frac{\mu_{i j}\left(X_i, X_j\right)}{\mu_i\left(X_i\right) \mu_j\left(X_j\right)}\right] \\
&=D\left(\mu_{i j} \| \mu_i \mu_j\right) \\
&=H\left(\mu_i\right)+H\left(\mu_j\right)-H\left(\mu_{i j}\right)
\end{aligned}
$$
is the \textbf{mutual information} associated with the pairwise marginal $\mu_{i j}$. Since this is a Kullback-Leibler divergence, it is nonnegative. The mutual information is zero if $X_i$ and $X_j$ are independent random variables and is upper-bounded by both $H\left(\mu_i\right)$ and $H\left(\mu_j\right)$ (value achieved if $X_j$ is a function of $X_i$, or vice-versa).

The entropy of $p$ is easily computed but is not concave in $\mu$. Equivalently, the set of distributions $p$ on a tree graph is generally nonconvex. (Consider a length-3 chain for instance.)


\subsection{Naive Mean-Field Methods In Graph}
Approximate complicated p.d.f. $p_\theta(\vec{x})=e^{\theta^T I(\vec{x})-A(\theta)}$ in $G=(V,E)$ with tractable $q(\vec{x})\in G'(V,E')$, where $E'\subset E$. $q(\vec{x})=\prod_{i\in V}q_i(x_i)$ (naive mean field method assumes fully factorized).

Minimizing divergence:
\begin{equation}
    \begin{aligned}
        D(q\| p_\theta)&=\mathbb{E}_q \left[\ln\frac{q(\vec{x})}{p_\theta(\vec{x})}\right]\\
        &=-\theta^T \mathbb{E}_q\left[T(\vec{x})\right]+A(\theta)-H(q)
    \end{aligned}
    \nonumber
\end{equation}
$Q=\{q:\mathbb{E}_q[T(\vec{x})]=\mu, \mu\in M'\}$
\begin{equation}
    \begin{aligned}
        \min_{q\in Q} D(q\| p_\theta)= A(\theta)-\max_{\mu\in M'} [\theta^T \mu-A^*(\mu)]
    \end{aligned}
    \nonumber
\end{equation}
\begin{equation}
    \begin{aligned}
        \max_{\{\mu_i\}_{i\in V}} [\theta^T \mu-A^*(\mu)]&=\sum_{i\in V} \sum_{x\in X}\theta_{ix}\mu_i(x)+\sum_{(i,j)\in E}\sum_{x,x'}\theta_{ijxx'}\mu_{ij}(x,x')+\sum_{i\in V}H(\mu_i)
    \end{aligned}
    \nonumber
\end{equation}
Taking Lagrangian and taking derivative
\begin{equation}
    \begin{aligned}
        0=\frac{\partial L(\mu,\lambda)}{\partial \mu_i(x)} \Rightarrow \mu_{i(x)}=\frac{1}{Z} e^{\theta_{ix}+\sum_{i\in N(i)}\sum_{x'\in X}\theta_{ijxx'}\mu_j(x')}
    \end{aligned}
    \nonumber
\end{equation}

\subsection{Structural Mean Field Optimization}
$q(\vec{x})=q_1(x_1)q_2(x_2|x_1)q_3(x_3|x_2)$ (Markov Chain in a tree $G'=(V,E')$)
\begin{equation}
    \begin{aligned}
        \mu_{12}(x_1,x_3)&=\sum_{x_2}p(x_1,x_2,x_3)=\sum_{x_2}p(x_1,x_2)p(x_3|x_2)=\sum_{x_2}p(x_1,x_2)\frac{p_{23}(x_2,x_3)}{p(x_2)}
    \end{aligned}
    \nonumber
\end{equation}

\subsection{Bethe Entropy Approximation}
When we compute the entropy of a tree graph, the entropy equals to
$$
H(p) \triangleq \sum_{i \in V} H\left(\mu_i\right)-\sum_{(i, j) \in E} I\left(\mu_{i j}\right)
$$
(\textbf{only holds in tree graph!}).

In a more general situation. For a distribution $p$ that is not defined on a tree graph, $H(p)$ does not admit a simple expression, and cannot be expressed simply in terms of 1-D marginals and pairwise marginals. (Verify on a 3-cycle). However, if these marginals are known, one could use an approximation to $H(p)$.
\begin{definition}[Bethe approximation]
    We \underline{use the equation satisfied in tree graph to approximate entropy in general situations}. This approximation is known as the \textbf{Bethe approximation}, and the functional
    $$
    H_{\text {Bethe }}(\tau) \triangleq \sum_{i \in V} H\left(\tau_i\right)-\sum_{(i, j) \in E} I\left(\tau_{i j}\right), \quad \tau \in \mathcal{L}(\mathcal{G})
    $$
    is known as the \textbf{Bethe entropy}. This "entropy" is well defined for all pseudomarginals $\tau \in \mathcal{L}(\mathcal{G})$.

    The \textbf{Bethe variational problem} is defined as
    $$
    A_{\text {Bethe }}(\theta) \triangleq \max _{\tau \in \mathcal{L}(\mathcal{G})}\left[\theta^{\top} \tau+H_{\text {Bethe }}(\tau)\right]
    $$
    and is relatively tractable owing to the simple nature of $\mathcal{L}(\mathcal{G})$ and the availability of a closed-form expression for $H_{\text {Bethe }}(\tau)$.
\end{definition}
Compare with the expression
$$
A(\theta)=\sup _{\mu \in \mathcal{M}(\mathcal{G})}\left[\theta^{\top} \mu+H\left(p_{\theta(\mu)}\right)\right]
$$
that is unfortunately intractable because of the complex nature of $\mathcal{M}(\mathcal{G})$ and the lack of an explicit form for $H\left(p_\mu\right)$. For a general graph, $\mathcal{M}(\mathcal{G}) \subset \mathcal{L}(\mathcal{G})$ and Bethe entropy is an approximation to entropy; $A_{\text {Bethe }}(\theta)$ is not a bound on $A(\theta)$, only an approximation (see example below). For a tree graph however, $\mathcal{M}(\mathcal{G})=\mathcal{L}(\mathcal{G})$ and $A_{\text {Bethe }}(\theta)=A(\theta)$

\begin{example}[ (Inexactness of Bethe approximation)]
    Consider a fully connected graph with four nodes, $V = \{1, 2, 3, 4\}$, uniform 1-D marginals $\mu_i, i \in V$, and pairwise marginals. $\mu_{ij}=\begin{bmatrix}
        0.5&0\\
        0&0.5
    \end{bmatrix}$, $\forall i,j\in V$ $\Rightarrow$ $X_i=X_j$ w.p.1 . $\vec{x}=[0,0,0,0]$ or $[1,1,1,1]$ with probability $0.5$ each.
    
    We have $\mu \in \mathcal{M}(\mathcal{G})$; indeed the distribution $p$ that places probability $\frac{1}{2}$ on the sequences $(0,0,0,0)$ and $(1,1,1,1)$ satisfies the marginal constraints above. We have $H\left(\mu_i\right)=\ln 2$ for all $i \in \mathcal{V}$ and $I\left(\mu_{i j}\right)=\ln 2$ for all $i \neq j \in \mathcal{V}$. Since there are 6 edges, we obtain
    $$
    H_{\text {Bethe }}(\mu)=4 \ln 2-6 \ln 2=-2 \ln 2<0
    $$
    which shows that the Bethe entropy does not satisfy the same properties as an entropy (it can be negative). The actual entropy $H(p) = \ln 2 > 0$.
\end{example}


\chapter{$\ell_1$ Penalized Least Squares Minimization}
We will focus on an $\ell_1$-penalized least-squares problem where the objective function is the sum of a quadratic function representing "fit to the data" and a regularization term which is the l1 norm of an unknown signal to be recovered. The first term is smooth, the second is not.

\section{Problem Statement}
Given an observation vector $y \in \mathbb{R}^m$, a $m \times n$ matrix $\mathrm{A}$, a constant $\lambda>0$, find a vector $x \in \mathbb{R}^n$ that achieves the minimum of
$$
f(x) \triangleq \frac{1}{2}\|y-\mathrm{A} x\|^2+\lambda\|x\|_1 .
$$
The first component is half the squared $\ell_2$ norm of $r \triangleq y-A x$, which can be interpreted as an observation error. The second component, $\|x\|_1 \triangleq \sum_{i=1}^n\left|x_i\right|$, is the $\ell_1$ norm of $x$.

The problem admits a Bayesian interpretation, in which the observations $y$ are the sum of $A x$ and white Gaussian noise with mean zero and variance $\sigma^2$, $$y=Ax+z,\text{ where }z\sim \mathcal{N}(0,\sigma^2)$$

and $x$ is a realization of a random vector with iid entries following a double-exponential (Laplace) distribution. In this case,
$$
\ln p(y \mid x)+\ln p(x)=-\frac{n}{2} \ln \left(2 \pi \sigma^2\right)-\frac{1}{2 \sigma^2}\|y-\mathrm{A} x\|^2-n \ln 2-\|x\|_1
$$
hence minimizing $f$ is equivalent to MAP estimation for the above Bayesian problem, with $\lambda=2 \sigma^2$.

The structure of $A$ depends on the application. In signal processing and computer vision, $A$ is usually related to a convolution operator, describing for instance motion blur in video. In compressive sensing, the entries of $A$ are typically iid random variables. The algorithms we will focus on do not require $A$ to have any special structure. We begin with two important simple cases (identity and orthonormal A), then move to the general problem.

\section{Special Cases}
\subsection{Definition: Soft Threshold}
\begin{definition}[Soft Threshold]
    $$S_\lambda (y)\triangleq\left\{\begin{matrix}
        y-\lambda,&\text{for }y\geq \lambda\\
        y+\lambda&\text{for }y\leq -\lambda\\
        0&\text{for }|y|< \lambda\\
    \end{matrix}\right.$$
\end{definition}
\subsection{Identity $A$}
Let $m = n$ and $A$ be the identity matrix
$$
f(x) \triangleq \sum_{i=1}^n\left(\frac{1}{2}(y_i-x_i)^2+\lambda |x_i|\right)
$$
\begin{equation}
    \begin{aligned}
        f'(x)&=\left\{\begin{matrix}
            -y+x+\lambda \text{sign}(x),&x\neq 0\\
            \text{does not exist},&x=0
        \end{matrix}\right.
    \end{aligned}
    \nonumber
\end{equation}
The solution $x$ is obtained by applying a \underline{soft threshold} to each component $y_i$ of the observations,
$$0=f'(x) \Rightarrow x=S_\lambda (y)\triangleq\left\{\begin{matrix}y-\lambda,&\text{for }y\geq \lambda\\
y+\lambda&\text{for }y\leq -\lambda\\
0&\text{for }|y|< \lambda\\
\end{matrix}\right.$$

\subsection{Orthonormal $A$}
If $m = n$ and $A$ is orthonormal, then $A^{-1} = A^T$ and
\begin{equation}
    \begin{aligned}
        \|y-Ax\|^2=\|A(A^Ty-x)\|^2=\|A^Ty-x\|^2
    \end{aligned}
    \nonumber
\end{equation}
$$\Rightarrow x=S_\lambda (A^Ty)$$

\subsection{Quadratic Optimization ($\lambda=0$)}
We now consider general A. It is useful to first study the case $\lambda=0$, in which case $f$ is quadratic and the optimization problem is smooth. The solution $x$ satisfies the necessary first-order optimality condition
$$
0=\nabla f(x)=-\mathrm{A}^{\top}(y-\mathrm{A} x) \quad \in \mathbb{R}^n .
$$
If $\operatorname{rank}(\mathrm{A}) \geq n$ (which implies $m \geq n$ ), the unique solution is $x=\left(\mathrm{A}^{\top} \mathrm{A}\right)^{-1} \mathrm{A}^{\top} y$.\\
Otherwise, the solution is nonunique. Any $x=\mathrm{A}^{+} y+z$ where $z \in \operatorname{Null}(\mathrm{A})$ and $\mathrm{A}^{+} \in \mathbb{R}^{n \times m}$ is the \textbf{Moore pseudo-inverse} of $A$, is a solution. The minimum-norm solution is $x=\mathrm{A}^{+} y$.

Even though a closed-form solution exists, for large $n$ one would avoid the computationally expensive matrix inverse and use an iterative algorithm such as gradient descent or conjugate gradient to derive the solution. The gradient descent update takes the form
$$
x^{k+1}=x^k+\alpha \mathrm{A}^{\top}\left(y-\mathrm{A} x^k\right), \quad k=1,2,3, \cdots
$$
where $\alpha$ is the step size.

\section{General Solution: Lasso}

\section{General Solution: Iterative Soft Thresholding Algorithm (ISTA)}
The idea is to tackle the difficult optimization problem by solving a sequence of simple optimization problems. Often (as is the case here) the simple optimization problems will admit an easily-computable closed-form solution.

\subsection{Proximal Minimization Algorithm}
\begin{definition}[Proximal Minimization Algorithm]
    Consider a convex function $F$ and a $n \times n$ positive definite matrix W. The iterative algorithm with update equation
    $$
    x^{k+1}=\arg \min _{x \in \mathbb{R}^n}\left\{F(x)+\frac{1}{2}\left(x-x^k\right)^T \mathrm{W}\left(x-x^k\right)\right\}
    $$
    is a \textbf{proximal minimization algorithm}.
\end{definition}

If $\mathrm{W}$ is suitably chosen, the algorithm is a majorization-minimization algorithm.
\begin{lemma}[Nondecreasing and Convergence]
    Such algorithms are of the form
    $$
    x^{k+1}=\arg \min _{x \in \mathbb{R}^n} Q\left(x, x^k\right)
    $$
    where the function $Q\left(\cdot, x^{\prime}\right)$ is easy to minimize, $Q(x, x)=F(x)$, and $Q\left(x, x^{\prime}\right) \geq F(x)$. The sequence $F\left(x^k\right)$ is \underline{nondecreasing} because by application of the above properties and the definition of $x^{k+1}$
    $$
    F\left(x^{k+1}\right) \leq Q\left(x^{k+1}, x^k\right) \leq Q\left(x^k, x^k\right)=F\left(x^k\right) .
    $$
    One may also show that the sequence $x^k$ \underline{converges to a minimum of $F$}.
\end{lemma}

\subsection{Apply to $\ell_1$-penalized least-squares}
To apply this strategy to $\ell_1$-penalized least-squares, let $F(x)=\frac{1}{2}\|y-\mathrm{A} x\|^2+\lambda\|x\|_1$ and
$$
Q\left(x, x^{\prime}\right)=F(x)+\frac{c}{2}\left\|x-x^{\prime}\right\|^2-\frac{1}{2}\left\|\mathrm{~A}\left(x-x^{\prime}\right)\right\|^2
$$
where \underline{$c$ is larger than the maximum eigenvalue of $\mathrm{A}^{\top} \mathrm{A}$}. Then $\mathrm{W} \triangleq c \mathbf{I}_{n}-\mathrm{A}^{\top} \mathrm{A}$ is a symmetric positive definite matrix and
$$
\begin{aligned}
Q\left(x, x^k\right) &=F(x)+\frac{c}{2}\left\|x-x^k\right\|^2-\frac{1}{2}\left\|\mathrm{~A}\left(x-x^k\right)\right\|^2 \\
&=F(x)+\frac{1}{2}\left(x-x^k\right)^{\top} \underbrace{\left(c \mathbf{I}_{n}-\mathrm{A}^{\top} \mathrm{A}\right)}_{=\mathrm{W}}\left(x-x^k\right)
\end{aligned}
$$
satisfies the properties of a majorizing function.

We now show that $Q\left(\cdot, x^k\right)$ is easy to minimize:
$$
\begin{aligned}
Q\left(x, x^k\right) &=\lambda\|x\|_1+\frac{1}{2}\|y-\mathrm{A} x\|^2+\frac{c}{2}\left\|x-x^k\right\|^2-\frac{1}{2}\left\|\mathrm{~A}\left(x-x^k\right)\right\|^2 \\
&=\lambda\|x\|_1-x^{\top} \underbrace{\left[\mathrm{A}^{\top}\left(y-\mathrm{A} x^k\right)+c x^k\right]}_{=c u^k}+\frac{c}{2}\|x\|^2+\mathrm{constant} \\
&=\frac{c}{2}\left\|x-u^k\right\|^2+\lambda\|x\|_1+\mathrm{constant}
\end{aligned}
$$
where $u^k \triangleq x^k+\frac{1}{c} \mathrm{~A}^{\top}\left(y-\mathrm{A} x^k\right)$. The minimization of $Q\left(\cdot, x^k\right)$ takes the same form as \textit{identity $A$ situation}. Hence, the solution is obtained in closed form using the componentwise soft threshold operator:
$$
x^{k+1}=\arg \min _x Q\left(x, x^k\right)=S_{\frac{\lambda}{c}}\left(u^k\right)=S_{\frac{\lambda}{c}}\left(\frac{1}{c} \mathrm{~A}^{\top}\left(y-\mathrm{A} x^k\right)+x^k\right)
$$
This is the update equation for the \textbf{Iterative Soft Thresholding Algorithm (ISTA)}. Observe that the equation is an extension of the gradient descent update for the purely quadratic problem (with $\lambda=0$ and step size $\alpha=\frac{1}{c}$).

\section{Convergence Rate}
We say \textbf{linear convergence} if
\begin{equation}
    \begin{aligned}
        \|x^k-x^*\|\leq ab^k
    \end{aligned}
    \nonumber
\end{equation}
where $a>0$ and $b\in (0,1)$.

The \underline{sufficient condition} of \textbf{linear convergence} is
\begin{equation}
    \begin{aligned}
        \lim_{k \rightarrow \infty}\sup\frac{\|x^{k+1}-x^*\|}{\|x^k-x^*\|}\leq \beta
    \end{aligned}
    \nonumber
\end{equation}
for some $\beta\in (0,1)$.

As an application, we consider $\min_x\{f(x)=x^TQx\}$ where $Q\succ 0$, achieves $x^*$ at $0$.

GD $\Rightarrow $ step size $\alpha<\frac{2}{\lambda_{\max}(Q)}$. The optimal step size $\alpha_{opt}=\frac{2}{\lambda_{\max}(Q)+\lambda_{\min}(Q)}$

Condition number of $Q$ is $\kappa=\frac{\lambda_{\max}(Q)}{\lambda_{\min}(Q)}\geq 1$. With $\alpha=\alpha_{opt}$, $\beta=\frac{\|x^{k+1}\|}{\|x^k\|}=\frac{\kappa-1}{\kappa+1}<1$

\subsection*{Heavy -Ball Method}
\begin{equation}
    \begin{aligned}
        x^{k+1}=x^k-\alpha \nabla f(x^k)+\beta (x^k-x^{k-1})
    \end{aligned}
    \nonumber
\end{equation}
(with momentum), $\beta>0$.
For minimization of $\min f(x)=x^TQx$, heavy ball can then be shown to be equivalent to conjugate gradient when $\alpha$ and $\beta$ are optimized.

\section{Fast Iterative Soft Thresholding Algorithm (FISTA)}
In the ISTA, updates take the form of
\begin{equation}
    \begin{aligned}
        x^{k+1} &=S_{\lambda / c}\left(\frac{1}{c} \mathrm{~A}^{\top}\left(y-\mathrm{A} {x}^k\right)+{x}^k\right)
    \end{aligned}
    \nonumber
\end{equation}
In the FISTA, consider the sequence $t_{k+1}=\frac{1}{2}\left(1+\sqrt{1+4t_k^2}\right)$, initialized with $t_1=1$. It can easily be shown that $t_k=\frac{k}{2}[1+o(1)]$




And the FISTA takes the form:
$$
\begin{aligned}
x^{k+1} &=S_{\lambda / c}\left(\frac{1}{c} \mathrm{~A}^{\top}\left(y-\mathrm{A} \tilde{x}^k\right)+\tilde{x}^k\right) \\
\tilde{x}^{k+1} &=x^k+\frac{t_k-1}{t_{k+1}}\left(x^k-x^{k-1}\right)
\end{aligned}
$$
where $\tilde{x}^1=x^{(0)}$ and the second term in the right side is a "momentum" term, as in the heavy ball algorithm. The constant $c$ should be larger than the maximum eigenvalue of $A^{\top} A$.

Analysis of the convergence rate of FISTA-type algorithms is a current research topic.

\section{Alternating Direction Method of Multipliers (ADMM)}
The Alternating Direction Method of Multipliers (ADMM) is an augmented Lagrangian method.

The general idea is to reformulate the minimization problem
$$
\min _x\{g(x)+h(x)\}
$$
as
$$
\min _{x, z}\{g(x)+h(z)\} \quad \text { subj. to } \quad x=z
$$
which is solved using an augmented Lagrangian approach.

Identify $g(x)=\frac{1}{2}\|y-\mathrm{A} x\|^2$ and $h(x)=\lambda\|x\|_1$, fix some penalty parameter $\nu>0$, and write the augmented Lagrangian as
$$
\mathcal{L}(x, z, u)=\frac{1}{2}\|y-\mathrm{A} x\|^2+\lambda\|z\|_1+\frac{\nu}{2}\|x-z\|^2+u^{\top}(x-z)
$$
which is linear in the vector of Lagrange multipliers $u$, strongly convex in $(x, z)$, and nonsmooth in $z$ but easy to minimize over $z$ given $(x, u)$. The update equations are
$$
\begin{aligned}
x^{k+1} &=\arg \min _x \mathcal{L}\left(x, z^k, u^k\right) \\
&=\left(\mathrm{A}^{\top} \mathrm{A}+\nu \mathrm{l}\right)^{-1}\left(\mathrm{~A}^{\top} y+\nu z^k-u^k\right) \\
z^{k+1} &=\arg \min _z \mathcal{L}\left(x^{k+1}, z, u^k\right) \\
&=S_{\lambda / \nu}\left(x^{k+1}-\nu^{-1} u^k\right) \\
u^{k+1} &=u^k+\nu\left(x^{k+1}-z^{k+1}\right), \quad k=1,2,3, \cdots .
\end{aligned}
$$

\chapter{Compressive Sensing}
The problem is to recover a sparse signal $x\in \mathbb{R}^n$ by solving $y=Ax$, where $y\in \mathbb{R}^m$, $A\in \mathbb{R}^{m\times n}$, and $x\in \mathbb{R}^n$ is a sparse vector. $m<<n$. In general this would be a severely underdetermined linear system, but recovery is possible if the signal $x$ is sparse (contains mostly zeroes), or at least approximately sparse.

\section{Definitions related to Sparsity}
\textbf{Definitions:}
\begin{enumerate}
    \item 
    \begin{definition}[$\ell_p$ norm]
        The \underline{$\ell_p$ norm} of a vector $x \in \mathbb{R}^n$ is
            $$\|x\|_p \triangleq\left(\sum_{i=1}^n\left|x_i\right|^p\right)^{\frac{1}{p}}\text{ for all }p \geq 1$$
    \end{definition}
    \item 
    \begin{definition}[$\ell_0$ pseudonorm]
        The \underline{$\ell_0$ pseudonorm} of a vector $x \in \mathbb{R}^n$ is $$\|x\|_0 \triangleq \sum_{i=1}^n \mathbf{1}_{\left\{x_i \neq 0\right\}}$$ i.e., the number of nonzero components of $x$.
    \end{definition}
    \item 
    \begin{definition}[$k$-sparse]
        A signal $x \in \mathbb{R}^n$ is \underline{$k$-sparse} if $$\|x\|_0 \leq k$$ i.e., the number of nonzero components of $x$ is smaller than $k$.
    \end{definition}
    \item 
    \begin{definition}[set of $k$-sparse signals]
        The \underline{set of $k$-sparse signals} is $$\Sigma_k \triangleq\left\{x \in \mathbb{R}^n:\|x\|_0 \leq k\right\}$$
    \end{definition}
    This is a union of $\left(\begin{array}{l}n \\ k\end{array}\right)$-dimensional subspaces of $\mathbb{R}^n$, which is not a linear space.
    
    For instance let $n=2$ and $k=1$, then $\Sigma_1$ is the union of the horizontal and vertical axes in the 2-D plane.

    Many signals are sparse in a transform domain (for instance, Fourier-sparse) or approximately sparse. For instance, the wavelet coefficients of an image or a speech signal are approximately sparse, and one can typically construct good approximations of such signals by using only the largest $5 \%$ of their components.
    \begin{claim}[Producing $k$-sparse signal]
        One can go from an approximately sparse to an (exactly) $k$-sparse signal by applying a \textbf{hard threshold operator, $\hat{x}=H_k(x)$}, producing a vector \textbf{in which all but the $k$ largest (in magnitude) coefficients of $x$ are set to zero.}
    \end{claim}
    \item 
    \begin{definition}[$\ell_p$ approximation error]
        The \underline{$\ell_p$ approximation error} of $x \in \mathbb{R}^n$ in $\Sigma_k$ is
        $$
        e_{k, p}(x) \triangleq \min _{\hat{x} \in \Sigma_k}\|x-\hat{x}\|_p, \quad p \geq 1
        $$
    \end{definition}
    \begin{lemma}[Properties]
    \begin{enumerate}
        \item If $x$ is $k$-sparse, i.e., $x \in \Sigma_k$, then $e_{k, p}(x)=0$ for all $p$. (Optimal $\hat{x}$ is exactly the $x$)
        \item Otherwise, the approximation error typically \underline{vanishes geometrically with $k$}, i.e.,
        $$
        e_{k, p}(x) \leq c k^{-r}
        $$
        for some $c, r>0$, generally dependent on $x$.
        \item It is easily shown that the minimum of error is \underline{achieved by $\hat{x}=H_k(x)$}.
    \end{enumerate}
    \end{lemma}
\end{enumerate}

Other forms of sparsity are frequently encountered:
\begin{enumerate}[$\bullet$]
    \item \textbf{Low-dimensional manifolds:} let $\Theta$ be a compact subset of $\mathbb{R}^k$ and $f: \Theta \rightarrow \mathbb{R}^n$ a continuously differentiable mapping, then $x=f(\theta), \theta \in \Theta$ belongs to a $k$-dimensional manifold embedded in $\mathbb{R}^n$. This model applies to face images under varying illumination.
    \item \textbf{Low-rank matrices:} let $x$ be a $n_1 \times n_2$ matrix of rank $r<\min \left(n_1, n_2\right)$. Then $x$ can be represented using the singular value decomposition $x=\sum_{j=1}^r \sigma_j u_j v_j^{\top}$ where $\sigma_j$ are the (positive) singular values, and $u_j \in \mathbb{R}^{n_1}$ and $v_j \in \mathbb{R}^{n_2}$ are the singular vectors for $1 \leq j \leq r$. This model finds applications to computer vision, geolocalization, and collaborative filtering ( $c f$ the Netflix recommender system).
\end{enumerate}

\section{Measurement Matrix}
\begin{definition}[Measurement Matrix]
    The measurement matrix $A$ is a $m\times n$ matrix where $m \ll n$ (fat matrix). The observations are given by $\vec{y} = A \vec{x} \in \mathbb{R}^m$ in case of noise-free measurements.
\end{definition}
In this section we consider two basic questions are:
\begin{enumerate}
    \item What properties should $A\in \mathbb{R}^{m\times n}$ have so that $\vec{x}\in \mathbb{R}^n$ can be recovered from $\vec{y}\in \mathbb{R}^m$?
    \item If $A$ satisfies those properties, what recovery algorithm can be used?
\end{enumerate}

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{MM.png}
    \caption{Measurement of sparse signal $\vec{x}$ with support set $\Omega$ of size $k$.}
    \label{}
\end{figure}\end{center}
Denote by $A_i$, $1 \leq i \leq n$ the column vectors of $A$ and let $\Omega \subset \{1,2,...,n\}$ be the support set of the vector $\vec{x}$, i.e., $\Omega = \{i : x_i \neq 0\}$. Define the reduced matrix $A_\Omega = \{A_i, i \in \Omega\}$ and the reduced vector $\vec{x}_\Omega =\{x_i, i\in \Omega\}$. Then $$\vec{y}=A \vec{x}=A_\Omega \vec{x}_\Omega $$



\subsection{Matrix Preliminaries}
The SVD decomposition of $A$ takes the form $A=U[D \mid 0] V^T$ where $U$ and $V$ are $m \times m$ and $n \times n$ unitary matrices (whose conjugate transpose equals to inverse), respectively, $\mathrm{D}=\operatorname{diag}\left(\sigma_k\right)_{k=1}^m$, and 0 is the $m \times(n-m)$ all-zero matrix. The $m$ nonnegative entries $\sigma_k, 1 \leq k \leq m$, are the singular values of $A$. The matrix A can be expanded as a sum of $m$ rank-one matrices, $\mathrm{A}=\sum_{k=1}^m \sigma_k u_k v_k^{\top}$ where $\left\{u_k\right\}$ and $\left\{v_k\right\}$ are the columns of $U$ and $V$, respectively.

The spectral norm of $A$ is
$$
\|\mathrm{A}\| \triangleq \sup _{\vec{x}:\|\vec{x}\|_2=1}\|\mathrm{A} \vec{x}\|_2
$$
and is equal to the largest singular value of $A$.

The $m \times m$ Gram matrix $\mathrm{G} \triangleq \mathrm{AA}^{\top}$ is symmetric and nonnegative definite and can be expressed as $\mathrm{G}=\mathrm{UD}^2 \mathrm{U}^{\top}$ hence its eigenvalues are given by $\lambda_k(\mathrm{G})=\sigma_k^2(\mathrm{~A}), 1 \leq k \leq m$.

\begin{definition}[Null Space]
    The \underline{null space} of $\mathrm{A}$ is $$N(\mathrm{A}) \triangleq\left\{\vec{x} \in \mathbb{R}^n: \mathrm{A} \vec{x}=0\right\}$$
    This is a subspace of $\mathbb{R}^n$, whose dimension is \underline{at most $n - m$}.
\end{definition}

\begin{definition}[Spark]
    The \underline{spark} of $A$ is the smallest number of columns of $A$ that are linearly dependent.\\
    If any set of $q$ columns of the matrix are linearly independent, $\text{spark}(A)=q+1\in [2,m+1]$.
\end{definition}
The minimum value of the spark of any matrix is $2$, and is achieved by any matrix that has two identical columns.

Computing the spark of a matrix in an \underline{NP-hard problem}. In contrast, the rank of a matrix, which is the largest number of columns that are linearly independent, is easy to compute.

\subsection{Recovery of k-Sparse Signals}
\begin{lemma}
    Unique recovery of $x \in \Sigma_k$ given measurement matrix $A$ is possible $\Rightarrow$ $\Sigma_{2k} \cap N(A) = \emptyset$.
\end{lemma}
\begin{proof}
    To recover any \underline{$k$-sparse signal}, we need that $A\vec{x} \neq A\vec{x}'$ for any distinct $\vec{x},\vec{x}' \in \Sigma_k$. Hence, $\vec{x}-\vec{x}' \notin N(A)$. Since $\vec{x}-\vec{x}'$ is a $2k$-sparse signal and in fact $\Sigma_{2k} ={\vec{x}-\vec{x}' : \vec{x},\vec{x}'\in \Sigma_k}$, we need $\Sigma_{2k} \cap N(A) = \emptyset$.
\end{proof}

\begin{theorem}
    Unique recovery of $\vec{x} \in \Sigma_k$ given measurement matrix $A$ is possible $\Leftrightarrow$ $\text{spark}(A)> 2k$. ($q\geq 2k$)
\end{theorem}
\begin{proof}
Eldar, Y. C., \& Kutyniok, G. (Eds.). (2012). Compressed sensing: theory and applications, Chapter 1.
\end{proof}

\subsection{Restricted Isometry Property}
\begin{definition}[Restricted Isometry Property (RIP)]
    The matrix $A$ satisfies the \underline{RIP property of order $k$} if there exists $\delta_k \in (0, 1)$ such that
    \begin{equation}
        \begin{aligned}
            (1-\delta_k)\|\vec{x}\|^2_2\leq\|A\vec{x}\|_2^2\leq (1+\delta_k)\|\vec{x}\|^2_2,\quad \forall \vec{x}\in \Sigma_k
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}
If $A$ satisfies the RIP property of order $k$, then $A$ approximately \underline{preserves the $\ell_2$ distance} between any pair $\vec{x},\vec{x}' \in \Sigma_k$. This provides a \textit{stable embedding} of $k$-sparse signals in $\mathbb{R}^m$. This property will be key to derive a recovery algorithm that is robust to noise.

\begin{claim}[Equivalent Formulation of RIP]
    Since $\vec{y}=A \vec{x}=A_\Omega \vec{x}_\Omega$, an \textbf{equivalent formulation} of the RIP is $$(1-\delta_k)\|\vec{x}_\Omega\|^2_2\leq\|A_\Omega \vec{x}_\Omega\|_2^2\leq (1+\delta_k)\|\vec{x}_\Omega\|^2_2,\quad \forall \Omega: |\Omega|=k$$
for all $\Omega$ of size $k$ and for all $\vec{x}_\Omega\in \mathbb{R}^k$.
\begin{equation}
    \begin{aligned}
        \frac{\|A_\Omega \vec{x}_\Omega\|_2^2}{\|\vec{x}_\Omega\|_2^2}-1&=\frac{\vec{x}_\Omega^T\left(A^T_\Omega A_\Omega-I_k\right)\vec{x}_\Omega}{\vec{x}_\Omega^T\vec{x}_\Omega}\in\left[-\delta_k,\delta_k\right],\quad \forall \Omega: |\Omega|=k\\
        \Rightarrow \delta_k&=\max_{\Omega:|\Omega|=k}\|A^T_\Omega A_\Omega-I_k\|
    \end{aligned}
    \nonumber
\end{equation}
\end{claim}

\begin{theorem}[Measurement Bound]
    Assume $A\in \mathbb{R}^{m\times n}$ satisfies the RIP of order $2k$ with RIP constant $\delta_{2k}\in (0,\frac{1}{2}]$. Then $$m\geq ck\ln\frac{n}{k}$$ where the constant $c=\frac{1}{2}\ln(1+\sqrt{24})\approx 0.28$.
\end{theorem}

\section{Robust Signal Recovery from Noiseless Observations}
We now consider the so-called robust CS problem: the signal $\vec{x}$ is approximately sparse, and the observations are noiseless. A reasonable attempt to recover a sparse signal would be the so-called $\ell_0$ recovery problem
$$
\min _{\vec{x} \in \mathbb{R}^n}\|\vec{x}\|_0 \quad \text { subj. to } \quad \mathrm{A} \vec{x}=\vec{y}
$$
Unfortunately this problem is highly nonconvex. Solving it essentially requires evaluating all possible support sets of $\vec{x}$, which is a combinatorial problem.

A reasonable substitute is the so-called $\ell_1$ recovery problem
$$
\min _{\vec{x} \in \mathbb{R}^n}\|\vec{x}\|_1 \quad \text { subj. to } \quad \mathrm{A} \vec{x}=\vec{y}
$$
This procedure tends to produce sparse solutions, as illustrated in the Figure: the line $y=A x$ is tangent to the $\ell_1$ ball $\|\boldsymbol{x}\|_1=$ cst at the solution $\boldsymbol{x}$, producing a 1-sparse solution.
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.25]{l1re.png}
    \caption{$\ell_1$ recovery for $n=2$ and $m=1$}
    \label{}
\end{figure}\end{center}
The following fundamental theorem shows that the $\ell_1$ recovery procedure is remarkably good.
\begin{theorem}[$\ell_1$ recovery procedure is good]
    Assume $A$ satisfies the RIP of order $2 k$ with constant $\delta_{2 k}<\sqrt{2}-1$. $\hat{x}=\argmin_{\vec{x}\in \mathbb{R}^n: A \vec{x}=\vec{y}}\|\vec{x}\|_1$. Then
    $$
    \|\hat{x}-x\|_2 \leq \frac{c}{\sqrt{k}} e_{k,1}(x)
    $$
    and
    $$
    \|\hat{x}-x\|_1 \leq c e_{k,1}(x)
    $$
    with $c=2 \frac{1-(1-\sqrt{2}) \delta_{2 k}}{1-(1+\sqrt{2}) \delta_{2 k}}$. $e_{k,1}(x)\triangleq \min_{\hat{x}\in \Sigma_k}\|x-\hat{x}\|_1$ is the \underline{$\ell_1$ approximation error} of $x$ in $\Sigma_k$.
\end{theorem}
\begin{proof}
    Based on the triangle inequality and the inequality $\frac{\|u\|_1}{\sqrt{k}} \leq\|u\|_2 \leq \sqrt{k}\|u\|_{\infty}$ for all $u \in \Sigma_k$.
\end{proof}

\begin{corollary}
    If $x \in \Sigma_k$ then $\hat{x}=x$ (exact recovery).
\end{corollary}
If $x \notin \Sigma_k$ then the quality of the reconstruction is nearly as good as if an \textit{oracle} gave us the location of the $k$ largest absolute components and we measured those directly. (The oracle produces $\hat{x}=H_k(x)$, achieving $e_{k, p}(x)$ for all $p \geq 1$.)

Since $\delta_{2 k}<\sqrt{2}-1<\frac{1}{2}$, the measurement bound $m\geq ck\ln\frac{n}{k}$ applies, and we need as few as $m=O\left(k \ln \frac{n}{k}\right)$ measurements to satisfy the conditions of the theorem.

\section{Robust Signal Recovery from Noisy Observations}
\subsection{Bounded Noise}
We consider observations corrupted by bounded noise: $y=\mathrm{A} x+z$ where $\|z\|_2 \leq \epsilon$. We study the $\ell_1$ recovery problem
$$
\min _{x \in \mathbb{R}^n}\|x\|_1 \quad \text { subj. to } \quad\|\mathrm{A} x-y\|_2 \leq \epsilon
$$
which is closely related to the Lasso problem and can be solved using the algorithms introduced in the previous chapter.
\begin{theorem}
    Assume A satisfies the RIP of order $2 k$ with constant $\delta_{2 k}<\sqrt{2}-1$. Then
    $$
    \|\hat{x}-x\|_2 \leq \frac{c_0}{\sqrt{k}} e_{1, k}(x)+c_1 \epsilon
    $$
    with constants
    $$
    c_0=2 \frac{1-(1-\sqrt{2}) \delta_{2 k}}{1-(1+\sqrt{2}) \delta_{2 k}}, \quad \text { and } \quad c_1=4 \frac{\sqrt{1+\delta_{2 k}}}{1-(1+\sqrt{2}) \delta_{2 k}} .
    $$
\end{theorem}
For $\delta_{2 k}=\frac{1}{4}$ the theorem holds with $c_0 \leq 5.5$ and $c_1 \leq 6$. For $\epsilon=0$, the result coincides with that given in the previous section (noise-free case).







\end{document}