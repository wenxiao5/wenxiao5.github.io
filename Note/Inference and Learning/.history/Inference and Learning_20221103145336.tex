%!TEX program = xelatex
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{authblk}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{verbatim}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{indentfirst}
\usepackage{amssymb}
\usepackage{listings}

\lstset{basicstyle=\ttfamily, keywordstyle=\bfseries}
\linespread{1.6}\setlength{\footskip}{20pt}\setlength{\parindent}{0pt}
\usetikzlibrary{shapes,snakes}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\DeclareMathOperator{\col}{col}
\usepackage{booktabs}
\newtheorem{theorem}{Theorem}
\newtheorem{note}{Note}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{claim}{Claim}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{hyperref}
\newcommand{\code}{	exttt}
\geometry{a4paper,scale=0.8}
\title{\textbf{Machine Learning}}
\author[*]{Wenxiao Yang}
\affil[*]{Department of Mathematics, University of Illinois at Urbana-Champaign}
\date{2022}
\begin{document}
\maketitle
\tableofcontents
\newpage




\section{Information-Theoretic Functional}
\textbf{Definitions}
\begin{enumerate}
    \item \textbf{Entropy} of pmf $\{p(y),y\in Y\}$ $$H(p)=-\sum_{y\in Y}p(y)\ln p(y)$$
    (concave in $p$)
    \item \textbf{The Kullback-Leibler divergence (or relative entropy)} of two pmf's $p(x), x\in X$ and $q(x), x\in X$ is defined as
    \begin{equation}
        \begin{aligned}
            D(p\| q)=\sum_{y\in Y}p(y)\ln\frac{p(y)}{q(y)}\geq 0
        \end{aligned}
        \nonumber
    \end{equation}
    with equality iff $p=q$. (convex in $(p,q)$)
    \item \textbf{The cross-entropy} of a pmf $p(x),x\in X$ relative to another pmf $q(x), x\in X$
    \begin{equation}
        \begin{aligned}
            H(p, q)&=-\sum_{y\in Y}p(y)\ln q(y)\\&=H(p)+D(p\| q)
        \end{aligned}
        \nonumber
    \end{equation}
    $H(p,q)\geq H(p)$, the lower bound is achieved by $q=p$.
\end{enumerate}



\section{Learning Basics}
\subsection{Parameters and Hyperparameters}
\begin{definition}
    \textbf{Parameters} are values learned by the model given the data.
\end{definition}
e.g. $\beta, W, b, \theta$.
\begin{definition}
    \textbf{Hyperparameters} are values supplied to tune the model and cannot be learned from data.
\end{definition}
e.g. Number of Hidden Layers, Neurons, and Epochs to Train. Learning Rate, and Batch Size.

\subsection{Neural Network: Back Propagation Algorithm}
\underline{\textbf{Neural Network}}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{neuron1.png}
    \caption{Simple Neural Network}
    \label{}
\end{figure}\end{center}
Given a vector input $x$, we need to find the best estimator $\hat{y}$ which minimizes lost function. In the figure that has only one layer and one pathway, we find the parameter $(\omega,b)$ to form an input $\omega^Tx+b$ to neuron $\sigma$ (activation function).  Then, the final output (estimator) of the network is $\hat{y}=\sigma(\omega^Tx+b)$.


\subsubsection{Activations}
\begin{definition}
    \underline{Activation functions} are element-wise gates for letting information propagate to future layers either transformed with non-linearities or left as-is.
\end{definition}
\textbf{Example of activation function:}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{activations.png}
    \caption{Activations}
    \label{}
\end{figure}\end{center}
\begin{enumerate}[(1)]
    \item \textbf{Identity:} $$\text{identity}(x)=I(x)=x$$
    \item \textbf{Binary:} $$\text{binary}(x)=\text{step}(x)=\left\{\begin{matrix}
        1,&x\geq 0\\
        0,&x<0
    \end{matrix}\right.$$
    \item \textbf{Sigmoid:}$$\sigma(x)=\frac{1}{1+e^{-x}}$$
    \begin{equation}
        \begin{aligned}
            \sigma(-x)=1-\sigma(x);\
            \frac{d\sigma(x)}{dx}=\sigma(x)\cdot(1-\sigma(x))
        \end{aligned}
        \nonumber
    \end{equation}
    \begin{equation}
        \begin{aligned}
            \frac{\partial \sigma(\vec{x})}{\partial \vec{x}}=\begin{bmatrix}
                \sigma(x_1)(1-\sigma(x_1)) &\cdots&0\\
                \vdots&\ddots&\vdots\\
                0&\cdots&\sigma(x_n)(1-\sigma(x_n))
            \end{bmatrix}=\text{diag}(\sigma(\vec{x})\cdot (1-\sigma(\vec{x})))
        \end{aligned}
        \nonumber
    \end{equation}
    \item \textbf{TanH:} $$tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$$(TanH is a rescaled sigmoid) $$tanh(x)=\frac{e^{2x}-1}{e^{2x}+1}=1-2\sigma(-2x)=2\sigma(2x)-1$$
    \item \textbf{ReLU:} $$g(x)=\max(0,x)$$
    \item \textbf{Leaky ReLU:} $$g(x)=\max(0.1x,x)$$
    \item \textbf{Softmax:} $S_j(\vec{x})=\frac{e^{x_j}}{\sum_{i=1}^ne^{x_i}}$, $$S(\vec{x})=\left[\frac{e^{x_1}}{\sum_{i=1}^ne^{x_i}},\frac{e^{x_2}}{\sum_{i=1}^ne^{x_i}},\cdots, \frac{e^{x_n}}{\sum_{i=1}^ne^{x_i}}\right]^T$$
    $$\frac{\partial S_j(\vec{x})}{\partial x_j}=S_j(\vec{x})[1-S_j(\vec{x})]$$
\end{enumerate}


\subsubsection{Multilayer Neural Network}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{MNN.png}
    \caption{Multilayer Neural Network}
    \label{}
\end{figure}\end{center}
\begin{enumerate}[$\bullet$]
    \item Number of neurons in each layer can be different.
    \item All weights on edge connecting layers $m-1$ and $m$ is matrix $W^{(m)}$, with $w_{ij}^{(m)}$ being the weight connecting output $j$ of layer $m-1$ with neuron $i$ of layer $m$.
    \item Input to network is vector $x$; output of layer $m$ is vector $y^{(m)}$
    \begin{equation}
        \begin{aligned}
            y_i^{(1)}=\sigma(x_i^{(1)}),&\text{ with }x_i^{(1)}=\sum_jw_{ij}^{(1)}x_j+b_i^{(1)}\\
            y^{(1)}=\sigma(x^{(1)}),&\text{ with }x^{(1)}=W^{(1)}x+b^{(1)}\\
            y^{(2)}=\sigma(x^{(2)}),&\text{ with }x^{(2)}=W^{(2)}y^{(1)}+b^{(2)}\\
            \vdots&\\
            y^{(M)}=\sigma(x^{(M)}),&\text{ with }x^{(M)}=W^{(M)}y^{(M-1)}+b^{(M)}\\
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}

We want to find the weights $W^{(1)},\cdots,W^{(M)},b^{(1)},\cdots,b^{(M)}$ so that the output of last layer
\begin{equation}
    \begin{aligned}
        \hat{y}=y^{(M)}\approx f^*(x)=y
    \end{aligned}
    \nonumber
\end{equation}
$f^*(x)$ is the unknown thing we need to predict.

We use \underline{labelled training data}, i.e.
\begin{equation}
    \begin{aligned}
        (x[1],y[1]),(x[2],y[2]),\cdots (x[N],y[N])
    \end{aligned}
    \nonumber
\end{equation}
Minimize the "empirical" loss on training data.
\begin{equation}
    \begin{aligned}
        J=\sum_{i=1}^N L(y[i],\hat{y}[i])
    \end{aligned}
    \nonumber
\end{equation}
where $\bar{y}[i]$ is the output of NN whose input is $x[i]$.

\begin{enumerate}[$\bullet$]
    \item $L$ is the function of $W^{(1)},\cdots,W^{(M)},b^{(1)},\cdots,b^{(M)}$ to measure the loss. e.g. the square loss $$L(y,\hat{y})=(y-\hat{y})^2$$
    \item We wish to minimize $J$ using a \underline{gradient descent procedure}.
    \item To compute gradient we need:
    \begin{equation}
        \begin{aligned}
            \frac{\partial L}{\partial w_{ij}^{(l)}}\text{ for each }l,i,j; \quad
            \frac{\partial L}{\partial b_i^{(l)}}\text{ for each }l,i.\\
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}

\subsubsection{A Simple Example of Back Propagation Algorithm}
We can consider a simple example (one layer, two pathways)
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{2path.png}
    \caption{Two Independent Pathways}
    \label{}
\end{figure}\end{center}
$W^{(1)}=[w_{1,1}^{[1]},w_{2,1}^{[1]}]^T$, $b^{(1)}=[0,0]^T$, $\sigma_1(x)=x$.

$W^{(2)}=[w_{1,1}^{[2]},w_{1,2}^{[2]}]$, $b^{(2)}=0$, $\sigma_2(x)=x$.

\begin{equation}
    \begin{aligned}
        [a_1^{[1]},a_2^{[1]}]^T&=\sigma_1(W^{(1)}x_1+b^{(1)})=[w_{1,1}^{[1]}x_1,w_{2,1}^{[1]}x_1]^T\\
        \hat{y}&=\sigma_2(W^{(2)}[a_1^{[1]},a_2^{[1]}]^T+b^{(2)})=(w_{1,1}^{[1]}w_{1,1}^{[2]}+w_{2,1}^{[1]}w_{1,2}^{[2]})x_1
    \end{aligned}
    \nonumber
\end{equation}

\begin{equation}
    \begin{aligned}
        \frac{\partial J(\hat{y})}{\partial w_{2,1}^{[1]}}=\frac{\partial J(\hat{y})}{\partial \hat{y}}\cdot \frac{\hat{y}}{\partial a_2^{[1]}}\cdot\frac{a_2^{[1]}}{\partial w_{2,1}^{[1]}}
    \end{aligned}
    \nonumber
\end{equation}

\subsubsection{Back Propagation Algorithm}
Recall $y_i^{(m)}=\sigma(x_i^{(m)})$, $x_i^{(m)}=\sum_jw_{ij}^{(m)}y_j^{(m-1)}+b_i^{(m)}$
\begin{equation}
    \begin{aligned}
        \frac{\partial L}{\partial w_{ij}^{(m)}}=\frac{\partial L}{\partial y_i^{(m)}}\cdot \frac{\partial y_i^{(m)}}{\partial w_{ij}^{(m)}}=\frac{\partial L}{\partial y_i^{(m)}}\cdot \frac{\partial y_i^{(m)}}{\partial x_{i}^{(m)}}\cdot \frac{\partial x_{i}^{(m)}}{\partial w_{ij}^{(m)}}
    \end{aligned}
    \nonumber
\end{equation}
\begin{equation}
    \begin{aligned}
        \frac{\partial L}{\partial b_{i}^{(m)}}&=\frac{\partial L}{\partial y_i^{(m)}}\cdot \frac{\partial y_i^{(m)}}{\partial x_{i}^{(m)}}\cdot \frac{\partial x_{i}^{(m)}}{\partial b_{i}^{(m)}}\\
    \end{aligned}
    \nonumber
\end{equation}

\textbf{\underline{For large $M$}},
\begin{enumerate}[$\bullet$]
    \item $\frac{\partial L}{\partial y_i^{(M)}}$ is easy to compute.
    \item $\frac{\partial y_i^{(M)}}{\partial x_{i}^{(M)}}=\frac{\partial \sigma(x_{i}^{(M)})}{\partial x_{i}^{(M)}}=\sigma'(x_{i}^{(M)})$, (assuming $\sigma$ differentiable).
    \item $\frac{\partial x_{i}^{(M)}}{\partial w_{ij}^{(M)}}=y_j^{(M-1)}$
\end{enumerate}
Thus,
\begin{equation}
    \begin{aligned}
        \frac{\partial L}{\partial w_{ij}^{(M)}}=\frac{\partial L}{\partial y_i^{(M)}}\cdot \sigma'(x_{i}^{(M)}) \cdot y_j^{(M-1)}
    \end{aligned}
    \nonumber
\end{equation}
Similarly,
\begin{equation}
    \begin{aligned}
        \frac{\partial L}{\partial b_{i}^{(M)}}&=\frac{\partial L}{\partial y_i^{(M)}}\cdot \frac{\partial y_i^{(M)}}{\partial x_{i}^{(M)}}\cdot \frac{\partial x_{i}^{(M)}}{\partial b_{i}^{(M)}}\\
        &=\frac{\partial L}{\partial y_i^{(M)}}\cdot \sigma'(x_{i}^{(M)})
    \end{aligned}
    \nonumber
\end{equation}

\textbf{\underline{For $1\leq m< M$}}, in this situation $\frac{\partial L}{\partial y_i^{(m)}}$ is not easy to compute. Note that $x^{(m+1)}=W^{(m+1)}y^{(m)}+b^{(m+1)}$.
\begin{equation}
    \begin{aligned}
    \frac{\partial L}{\partial y_i^{(m)}}&=\sum_k \frac{\partial L}{\partial x_{k}^{(m+1)}}\cdot \frac{\partial x_{k}^{(m+1)}}{\partial y_i^{(m)}}\\
    &=\sum_k \frac{\partial L}{\partial y_{k}^{(m+1)}}\cdot\frac{\partial y_{k}^{(m+1)}}{\partial x_{k}^{(m+1)}}\cdot \frac{\partial x_{k}^{(m+1)}}{\partial y_i^{(m)}}\\
    &=\sum_k \frac{\partial L}{\partial y_{k}^{(m+1)}}\cdot\sigma'(x_k^{(m+1)})\cdot w_{ki}^{(m+1)}\\
    \end{aligned}
    \nonumber
\end{equation}

Then use this form to compute,

(We can set $\delta^{(m)}=\frac{\partial L}{\partial y_i^{(m)}}\cdot \sigma'(x_{i}^{(m)})$ to avoid duplicate computation.)
\begin{equation}
    \begin{aligned}
        \frac{\partial L}{\partial w_{ij}^{(m)}}&=\frac{\partial L}{\partial y_i^{(m)}}\cdot \frac{\partial y_i^{(m)}}{\partial x_{i}^{(m)}}\cdot \frac{\partial x_{i}^{(m)}}{\partial w_{ij}^{(m)}}\\
        &=\frac{\partial L}{\partial y_i^{(m)}}\cdot \sigma'(x_{i}^{(m)}) \cdot y_j^{(m-1)}\\
        &=\delta^{(m)}\cdot y_j^{(m-1)}
    \end{aligned}
    \nonumber
\end{equation}

Similarly,
\begin{equation}
    \begin{aligned}
        \frac{\partial L}{\partial b_{i}^{(m)}}&=\frac{\partial L}{\partial y_i^{(m)}}\cdot \frac{\partial y_i^{(m)}}{\partial x_{i}^{(m)}}\cdot \frac{\partial x_{i}^{(m)}}{\partial b_{i}^{(m)}}\\
        &=\frac{\partial L}{\partial y_i^{(m)}}\cdot \sigma'(x_{i}^{(m)})\\
        &=\delta^{(m)}
    \end{aligned}
    \nonumber
\end{equation}

\begin{center}
    \fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{
\textbf{Summary}
\begin{enumerate}
    \item Compute $\frac{\partial L}{\partial y_i^{(M)}}$.
    \item Use $$\frac{\partial L}{\partial y_i^{(m)}}=\sum_k \frac{\partial L}{\partial y_{k}^{(m+1)}}\cdot\sigma'(x_k^{(m+1)})\cdot w_{ki}^{(m+1)}$$ compute $\frac{\partial L}{\partial y_i^{(m)}}$ for $m=1,2...,M-1$.
    \item Compute $$\frac{\partial L}{\partial b_{i}^{(m)}}=\frac{\partial L}{\partial y_i^{(m)}}\cdot \sigma'(x_{i}^{(m)})=\delta^{(m)}$$ for $m=1,2...,M$.
    \item Compute $$\frac{\partial L}{\partial w_{ij}^{(m)}}=\frac{\partial L}{\partial y_i^{(m)}}\cdot \sigma'(x_{i}^{(m)}) \cdot y_j^{(m-1)}=\delta^{(m)}\cdot y_j^{(m-1)}$$ for $m=1,2...,M$.
\end{enumerate}
    }}
\end{center}

\subsubsection{Other Methods}
Stochastic Gradient Descent (SGD)

Subgradient Method

\subsection{Perceptron Algorithm}
\begin{definition}
    \underline{Binary linear classifiers} distinguish between two categories through a linear function of the inputs.
\end{definition}

\begin{definition}
    \underline{Linearly separable} refers to a line that can be drawn to perfectly split the two classes.
\end{definition}

The Perceptron algorithm is an efficient algorithm for learning a \textbf{linear separator} in $d-$dimensional space, with a mistake bound that depends on the margin of separation of the data.
\subsubsection{General Idea}
Given the training data $$D=\left\{\left\langle x^{(i)},y^{(i)}\right\rangle,i=1,...,n\right\}\in \left(\mathbb{R}^m\times\{0,1\}\right)^n$$ we want to know the exact value of $y\in\{0,1\}$.
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.1]{Per_output.png}
    \caption{Perceptron Output}
    \label{}
\end{figure}\end{center}
\begin{center}
    \fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{
    \subsubsection*{General idea:}
    \begin{enumerate}[$\bullet$]
        \item If the perceptron correctly predicts ($\hat{y}=y$):
        \begin{enumerate}[$\cdot$]
            \item Do nothing
        \end{enumerate}
        \item If the perceptron yields an incorrect prediction ($\hat{y}\neq y$):
        \begin{enumerate}[$\cdot$]
            \item If the prediction is $0$ and truth is $1$ ($\hat{y}=0|y=1 \Rightarrow e=y-\hat{y}=1$), add feature vector to weight vector.
            \item If the prediction is $1$ and truth is $0$ ($\hat{y}=1|y=0 \Rightarrow e=y-\hat{y}=-1$), subtract feature vector from the weight vector.
        \end{enumerate}
    \end{enumerate}
    }}
\end{center}

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{per.png}
    \caption{Perceptron}
    \label{}
\end{figure}\end{center}

Since we want the prediction to be either $0$ or $1$, we usually use binary function as the activation function in perceptron.


\subsubsection{Algorithm}
\textbf{Perceptron Algorithm:}
\begin{enumerate}[$\bullet$]
    \item Initialize weights (including a bias term) to zero, e.g. $W=[w,b]=0^{m+1}$.
    \item Under each training epoch: Compute for each sample $\left\langle x^{(i)},y^{(i)}\right\rangle\in D$
    \begin{enumerate}[$\cdot$]
        \item A prediction $\hat{y}^{(i)}=g({x^{(i)}}^TW)$
        \item Prediction error $e^{(i)}=y^{(i)}-\hat{y}^{(i)}$
        \item Weighted update $W=W+\eta e^{(i)}x^{(i)}$
    \end{enumerate}
\end{enumerate}

\subsubsection{Limitations}
\begin{enumerate}[$(1)$]
    \item Only provides a linear classifier boundary.
    \item Only allows for \textbf{binary classifier} between two classes.
    \item \textbf{No convergence possible} if classes are not linearly
    separable.
    \item Perceptron will yield \textbf{multiple boundary/"optimal"
    solutions}.
    \item Boundaries found may \textbf{not} perform \textbf{equally well}.
\end{enumerate}

\subsection{ADAptive LInear NEuron
(ADALINE)}
\subsubsection{General Idea}
Except the activation function in perceptron, we can add a threshold function.

In perceptron, we generate the estimation $\hat{y}$ (after binary function) to help update weight $\{w_i\}_{i=0}^m$. However, in ADALINE, we minimize MSE $z=x^TW$ to update weight $\{w_i\}_{i=0}^m$ before output estimation $\hat{y}$ (before binary function).

Before entering threshold (binary function), we want to minimize a mean-
squared error (MSE) loss
function to estimate weights.

e.g. suppose $g(x)=x$, let $z=x^TW$ be the input of threshold, for each $y$,$$W^*=\argmin_{W}L(z,y)=(y-z)^2$$
\begin{equation}
    \begin{aligned}
        \frac{\partial L(z,y)}{\partial w_i}=-2(y-z)\frac{\partial z}{\partial w_i}=-2(y-z)x_i
    \end{aligned}
    \nonumber
\end{equation}

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{adaline.png}
    \caption{ADALINE}
    \label{}
\end{figure}\end{center}

\subsubsection{Widrow-Hoff Delta Rule}
(Gradient Descent Rule for ADALINE)
\begin{enumerate}[$\bullet$]
    \item \textbf{Original: }$$W=W+\eta(y^{(j)}-z)x^{(j)}$$
    \item \textbf{Unit-norm: }$$W=W+\eta(y^{(j)}-z)\frac{x^{(j)}}{\|x^{(j)}\|}$$
    where $\|x\|=\sqrt{x_1^2+x_2^2+\cdots+x_m^2}$
\end{enumerate}

\textbf{The Perceptron and ADALINE use variants of the delta rule!}
\begin{enumerate}[(1)]
    \item \textbf{Perceptron}: Output used in delta rule is $\hat{y}=g(x^TW)$; $W=W+\eta(y^{(j)}-{\hat{y}}^{(j)})x^{(j)}$
    \item \textbf{ADALINE}: Output used to estimate weights is $z=x^TW$. $W=W+\eta(y^{(j)}-z)x^{(j)}$
\end{enumerate}

\subsection{Logistic Regression (Binary-class Output)}
\subsubsection{Generative and Discriminative Classifiers}
The most important difference between naive Bayes and logistic regression is that \underline{logistic regression} is a \textbf{discriminative classifier} while \underline{naive Bayes} is a \textbf{generative classifier}.

Suppose we want to classify class $A$ (dogs) and class $B$ (cats) (More genearl form: assign a class $c\in C$ to a document $d\in D$):
\begin{enumerate}[(1)]
    \item \underline{\textbf{Generative model:}} A generative model would have the goal of understanding what dogs look like and what cats look like. You might literally ask such a model to ‘generate’, i.e., draw, a dog.\\
    e.g. \textbf{naive Bayes:} we do not directly compute the probability that the document $d$ belongs to each class $c$, $P(c|d)$. We compute likelihood $P(d|c)$ and prior probability $P(c)$ to generate best estimation $\hat{c}$. (i.e., we want to know what should the distribution of a document $d$ in class $c$ be like.) $$\hat{c}=\argmax_{c\in C}P(d|c)P(c)$$
    \item \underline{\textbf{Discriminative model:}} A discriminative model, by contrast, is only trying to learn to distinguish the classes (perhaps without learning much about them). That is we want to directly computing $P(c|d)$.
\end{enumerate}

\subsubsection{Sigmoid function}
The goal of binary logistic regression is to train a classifier that can make a binary decision about the class of a new input observation.

The input observation is $x=[x_1,...,x_m]^T$ and the output $y$ is either $1$ or $0$. Instead of using the optimal weights of each feature $x_i$ and binary activation function (\textbf{threshold}: $\hat{y}=1$ if $z\geq 0$ and $\hat{y}=0$ otherwise) to estimate  in Perceptron and ADALINE, \textbf{we want to estimate the probability $P(y=1|x)$.}

However, \underline{the weighted sum $z=x^TW=\sum_{i=1}^mw_ix_i+b$ ranges $-\infty$ to $\infty$}. We want to \textbf{force the $z$ to be a legal probability}, that is, to lie between $0$ and $1$.

The \textbf{sigmoid function} $\sigma(z)=\frac{1}{1+e^{-z}}$ can be used as acitivation for this purpose, $P(y=1|x)=\sigma(x^TW)$. Since $1-\sigma(x)=\sigma(-x)$, $P(y=0|x)=\sigma(-x^TW)$.

\subsubsection{Cross-entropy Loss Function}
We choose the parameters $W$ that maximize the log probability of the true $y$ labels in the training data given the observations $x$. The conditional probability
\begin{equation}
    \begin{aligned}
        p(y|x)=\left\{\begin{matrix}
            \hat{y},&y=1\\
            1-\hat{y},&y=0
        \end{matrix}\right.=\hat{y}^y(1-\hat{y})^{1-y}
    \end{aligned}
    \nonumber
\end{equation}
To maximize the probability, we log both sides:
\begin{equation}
    \begin{aligned}
        \log p(y|x)=y\log\hat{y}+(1-y)\log(1-\hat{y})
    \end{aligned}
    \nonumber
\end{equation}
Then, we want the $\hat{y}$ to maximize the probability (also the logarithm of the probability):
\begin{equation}
    \begin{aligned}
        \hat{y}^*&=\argmax_{\hat{y}\in[0,1]}\log p(y|x)\\
        &=\argmin_{\hat{y}\in[0,1]}-\log p(y|x)\\
        &=\argmin_{\hat{y}\in[0,1]}-(y\log\hat{y}+(1-y)\log(1-\hat{y}))
    \end{aligned}
    \nonumber
\end{equation}
The right hand side is exactly the \textbf{cross-entropy loss function}:
$$L(y,\hat{y})=-(y\log\hat{y}+(1-y)\log(1-\hat{y}))$$
where $\hat{y}^{(i)}=\sigma(w^Tx^{(i)}+b)$
\begin{equation}
    \begin{aligned}
        \frac{\partial L(y^{(i)},\hat{y}^{(i)})}{\partial w_j}=\left(\sigma(w^Tx^{(i)}+b)-y^{(i)}\right)x_j^{(i)}=(\hat{y}^{(i)}-y^{(i)})x_j^{(i)}
    \end{aligned}
    \nonumber
\end{equation}

The risk (Binary Cross-Entropy Cost) of a weight $W$ is $$J(W)=-\frac{1}{n}\sum_{i=1}^n(y^{(i)}\log\hat{y}^{(i)}+(1-y^{(i)})\log(1-\hat{y}^{(i)}))$$
\begin{equation}
    \begin{aligned}
        \frac{\partial J(w,b)}{\partial w_j}=\frac{1}{n}\sum_{i=1}^n\left(\sigma(w^Tx^{(i)}+b)-y^{(i)}\right)x_j^{(i)}
    \end{aligned}
    \nonumber
\end{equation}

\subsubsection{Algorithm}
\begin{enumerate}[$\bullet$]
    \item Initialize weights (including a bias term) to zero, e.g. $W=[w,b]=0^{m+1}$.
    \item Under each training epoch: Compute for each sample $\left\langle x^{(i)},y^{(i)}\right\rangle\in D$
    \begin{enumerate}[$\cdot$]
        \item A prediction $\hat{y}^{(i)}=g({x^{(i)}}^TW)$
        \item Prediction error $e^{(i)}=y^{(i)}-\hat{y}^{(i)}$
        \item Weighted update $W=W+\eta e^{(i)}x^{(i)}=W-\eta \nabla L(W)$
    \end{enumerate}
\end{enumerate}

\subsection{Softmax Regression (Multi-class Output)}
\subsubsection{Multi-Class Classification and Multi-Label Classification}
\begin{definition}
    \underline{Multi-Class Classification} is a process for assigning each sample exactly one class. In this case, classes are considered mutually exclusive (no intersection).
\end{definition}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.13]{multi-class.png}
    \caption{Multi-Class Classification}
    \label{}
\end{figure}\end{center}
\begin{definition}
    \underline{Multi-Label Classification} or annotation allows for each sample to have 1 or more classes assigned to it. In this case, classes are mutually non-exclusive (one common element).
\end{definition}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.13]{multi-label.png}
    \caption{Multi-Label Classification}
    \label{}
\end{figure}\end{center}

We can show some examples of activation layer and loss choice in different probelms.
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.15]{exampleofact.png}
    \caption{Examples of Activation Layer and Loss Choice}
    \label{}
\end{figure}\end{center}

\subsubsection{One-hot Encoding}
\begin{definition}
    \underline{One-hot encoding} is the process of assigning a single location within a vector to represent a given category.
\end{definition}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.15]{ohe.png}
    \caption{One-hot encoding}
    \label{}
\end{figure}\end{center}
\textbf{Examples:}
\begin{enumerate}
    \item $\mathbf{z}=[4]_{1 \times 1} \rightarrow\left[\begin{array}{lllll}
        0 & 0 & 0 & 0 & 1
        \end{array}\right]_{1 \times 5}$
    \item $\mathbf{y}=\left[\begin{array}{lll}
        3 & 0 & 2
        \end{array}\right]_{1 \times 3} \rightarrow\left[\begin{array}{llll}
        0 & 0 & 0 & 1 \\
        1 & 0 & 0 & 0 \\
        0 & 0 & 1 & 0
        \end{array}\right]_{3 \times 4}$
    \item $\mathbf{v}=\left[\begin{array}{lllll}
        5 & 0 & 4 & 4 & 3
        \end{array}\right]_{1 \times 5} \rightarrow\left[\begin{array}{llllll}
        0 & 0 & 0 & 0 & 0 & 1 \\
        1 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1 & 0 & 0
        \end{array}\right]_{5 \times 6}$
\end{enumerate}
\textbf{Usefulness of Encodings}
\begin{enumerate}
    \item Close to a traditional design matrix for linear regression that uses a codified dummy variable structure. e.g. FALSE (0) or TRUE (1)
    \item Reduce the size of data stored by using numbers instead of
    strings.
    \item Poor if there are too many unique values (e.g. text messages on a phone.)
\end{enumerate}




\subsubsection{Softmax function}
\begin{definition}
    \underline{Softmax function} or \underline{normalized exponential function} converts between real valued numbers to values between 0 and 1
\end{definition}
\textbf{Softmax:} $S_j(\vec{x})=\frac{e^{x_j}}{\sum_{i=1}^ne^{x_i}}$, $$S(\vec{x})=\left[\frac{e^{x_1}}{\sum_{i=1}^ne^{x_i}},\frac{e^{x_2}}{\sum_{i=1}^ne^{x_i}},\cdots, \frac{e^{x_n}}{\sum_{i=1}^ne^{x_i}}\right]^T$$
We can show the softmax function has the following properties:
\begin{enumerate}[(1)]
    \item First-derivative: $$\frac{\partial S_i(\vec{x})}{\partial x_i}=S_i(\vec{x})[1-S_i(\vec{x})];\ \frac{\partial S_i(\vec{x})}{\partial x_j}=-S_i(\vec{x})S_j(\vec{x})$$
    \item Stabilizing softmax: $$S_j(\vec{x}+c)=\frac{e^{x_j+c}}{\sum_{i=1}^ne^{x_i+c}}=\frac{e^{x_j}}{\sum_{i=1}^ne^{x_i}}=S_j(\vec{x})$$
    We can minus $\max_{i} x_i$ to avoid overflow in softmax function. (numerical issue) i.e., $S_j(\vec{x}-(\max_{i} x_i))=S_j(\vec{x})$
\end{enumerate}

\subsubsection{Categorical Cross-entropy Loss Function}
\begin{definition}
    \underline{Categorical Cross-entropy Loss} is a way to quantify the difference between a "true" values $\{y_c\}_{c\in C}$ and an estimated $\{\hat{y}_c\}_{c\in C}$ across $C$ categories.
\end{definition}
Note: $y$ needs one-hot encoding firstly, $\hat{y}$ are estimated probability.
$$L(y,\hat{y})=-\sum_{c\in C}\left(y_c\cdot \log(\hat{y}_c)\right)$$

\begin{definition}
    \underline{Categorical Cross-entropy Cost} is a way of quantifying the cost over multiple points from different categories.
\end{definition}
$$J(W)=\frac{1}{n}\sum_{i=1}^n L(y_i,\hat{y}_i)=-\frac{1}{n}\sum_{i=1}^n\sum_{c\in C}\left(y_{i,c}\cdot \log(\hat{y}_{i,c})\right)$$














\subsection{Deep Feedforward Networks}

\subsubsection{Definition}
In any neural network, a \underline{dense layer} is a layer that is deeply connected with its preceding layer which means the neurons of the layer are connected to \textit{every neuron} of its preceding layer.

\begin{definition}
    \underline{Deep feedforward networks}, \underline{feedforward neural networks}, \underline{multilayer perceptrons (MLPs)}, or \underline{dense neural networks} form the foundations of deep learning models. Learning occurs in only one direction: \textbf{forward}. There are \textbf{no feedback} connections in whichoutputs of the model are fed back into itself. There are no cycles or loops present. Information must flow from the input layer, through one or more hidden layers, before reaching the output.
\end{definition}

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.13]{deep.png}
    \caption{Deep Neural Network}
    \label{}
\end{figure}\end{center}
A deep neural network contains \textit{Input Layer}, \textit{Hidden Layer}, and \textit{Output Layer}.


\subsubsection{Universal Approximation Theorem}
Universal Approximation Theorem, in its lose form, states that a \textbf{feed-forward network} with a \textbf{single hidden layer} containing a finite number of neurons \textbf{can approximate any continuous function}. (Which is also equivalent to having a nonpolynomial activation function)
\begin{theorem}[Universal approximation theorem]
    Fix a continuous function $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ (activation function) and positive integers $d, D\in \mathbb{Z}^+$. The function $\sigma$ is \underline{not a polynomial} $\Leftrightarrow$ for every continuous function $f: \mathbb{R}^d \rightarrow \mathbb{R}^D$ (target function), every compact subset $K$ of $\mathbb{R}^d$, and every $\epsilon>0$ there exists a continuous function $f_\epsilon: \mathbb{R}^d \rightarrow \mathbb{R}^D$ (the layer output) with representation $$f_\epsilon=W_2 \circ \sigma \circ W_1$$
    where $W_2, W_1$ are composable affine maps and o denotes component-wise composition, such that the approximation bound $$\sup _{x \in K}\left\|f(x)-f_\epsilon(x)\right\|<\varepsilon$$
    holds for any $\epsilon$ arbitrarily small (distance from $f$ to $f_\epsilon$ can be infinitely small).
\end{theorem}

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{UAT.png}
    \caption{Universal Approximation Theorem}
    \label{}
\end{figure}\end{center}

\subsection{ Mini-batch Optimization}
\subsubsection{Stochastic Gradient Descent (SGD) and Batch Gradient Descent (BGD)}
\textbf{Stochastic Gradient Descent (SGD)}
\begin{enumerate}
    \item Start with a random guess.
    \item For $n$ epochs:
    \begin{enumerate}[1)]
        \item Reorder data
        \item Retrieve an observation $i=1,2,...$ one by one in reordered data:
        \begin{enumerate}[(1)]
            \item Compute gradient on single data point $i$: $\frac{\partial J_i(W)}{\partial W}$
            \item Update parameters:
            $W=W-\alpha \frac{\partial J_i(W)}{\partial W}$
        \end{enumerate}
    \end{enumerate}
    \item Output parameters
\end{enumerate}
\textbf{Note:} "On-line"/"Stochastic" \textbf{Single} Observation Updates

\textbf{Batch Gradient Descent (BGD)}
\begin{enumerate}
    \item Start with a random guess.
    \item For $n$ epochs:
    \begin{enumerate}[1)]
        \item Compute gradients on
        \textbf{all the data}: $\frac{\partial J(W)}{\partial W}$
        \item Update parameters:
        $W=W-\alpha \frac{\partial J(W)}{\partial W}$
    \end{enumerate}
    \item Output parameters
\end{enumerate}
\textbf{Note:} \textbf{All} data used in update
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.1]{BGDandSGD.png}
    \caption{BGD and SGD}
    \label{}
\end{figure}\end{center}

\subsubsection{Mini-Batch Gradient Descent (MBGD)}
We want a middle ground between SGD and BGD.

\textbf{Mini-Batch Gradient Descent (MBGD)}
\begin{enumerate}
    \item Start with a random guess.
    \item For $n$ epochs:
    \begin{enumerate}[1)]
        \item Reorder data and retrieve a subet of reordered data with size $b$ (batch size)
        \item Compute gradient on subset: $\frac{\partial J(W)}{\partial W}$
        \item Update parameters:
        $W=W-\alpha \frac{\partial J(W)}{\partial W}$
    \end{enumerate}
    \item Output parameters
\end{enumerate}
If $b=n$, the algorithm is exactly BGD; If $b=1$, the algorithm is exactly SGD.
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.1]{SGDandMBGD.png}
    \caption{SGD and MBGD}
    \label{}
\end{figure}\end{center}

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.1]{resource.png}
    \caption{Comparison of Approaches}
    \label{}
\end{figure}\end{center}

\subsection{Weight Initialization}
\subsubsection{Xavier Initialization}
Normal distribution with a scale variance by weights. Used on layers where either $TanH$ or $Sigmoid$ is present.
\begin{center}
    Initialize weights for layer $l$ with: $W^{[l]}=W^{[l]}\sqrt{\frac{1}{n^{[l-1]}}}$
\end{center}
where $n^{[l-1]}$ is the number of weights in the last layer. Each weight is sampled by $W_{j,i}^{[l]}\sim \mathcal{N}(0,1)$

\subsubsection{He Activation}
Weight initialization for $ReLU$-powered network.
\begin{center}
    Initialize weights for layer $l$ with: $W^{[l]}=W^{[l]}\sqrt{\frac{2}{n^{[l-1]}}}$
\end{center}
where $n^{[l-1]}$ is the number of weights in the last layer. Each weight is sampled by $W_{j,i}^{[l]}\sim \mathcal{N}(0,1)$


\section{Adaptive Optimization}
\subsection{Exponentially Weighted Moving Averages}
How can we get an average across time?
\begin{enumerate}
    \item (Simple) (weighted) Moving Average ((S)MA): $$\bar{x}_{MA}=\frac{x_m+x_{m-1}+\cdots+x_{m-(n-1)}}{n}=\frac{1}{n}\sum_{i=0}^{n-1}x_{m-i}$$
    \item Exponentially (weighted) Moving Averages (EMA):
    \begin{equation}
        \begin{aligned}
            {EMA}_{t}=\left\{\begin{matrix}
                Y_1,&t=1\\
                \beta \cdot Y_t+(1-\beta)\cdot {EMA}_{t-1},&t>1
            \end{matrix}\right.
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}
$EMA$ is quicker to react: focuses more on recent events; $SMA$ is slower to react: focuses on long series events.

\subsection{Adaptive Learning Rates}
\textit{Adaptive Learning Rate} is a change to the learning rate while training a model to reduce the training time and improve output
\subsubsection{Momentum}
For a gradient descent with form:
\begin{equation}
    \begin{aligned}
        \theta_{t+1}:=\theta_t-v_t
    \end{aligned}
    \nonumber
\end{equation}
where $v_t$ is the \textbf{velocity} which is amplified gradient speed.
\begin{enumerate}
    \item \textbf{SGD:} $$v_t=\alpha \nabla_\theta J(\theta_t)$$
    \item \textbf{SGD + Momentum:} $$v_t=\rho v_{t-1}+\alpha \nabla_\theta J(\theta_t)$$
    where $\rho$ is the \textbf{friction or momentum} which dampens the amount of the previous gradient included. Default: $0.9$ for $\sim 10$ gradients.
\end{enumerate}
With momentum, the learning rate is \textbf{decreased} if gradient direction changes and \textbf{increased} if gradient direction stays on same path.

\subsubsection{Root Mean Square Propagation (RMSProp)}
Decrease learning rate by EMA using squared gradient.
\begin{equation}
    \begin{aligned}
        g_0&=0\quad (\text{Initial "gain"})\\
        g_t&=\alpha g_{t-1}+(1-\alpha) \nabla_\theta J(\theta_t)^2\quad (\text{MA over gradient squared})\\
        \theta_t:&=\theta_{t-1}-\frac{\varepsilon}{\sqrt{g_t}+1\times 10^{-5}}v_t\quad (1\times 10^{-5} \text{ is used to avoid division by $0$})\\
    \end{aligned}
    \nonumber
\end{equation}

\subsubsection{Adaptive Moment Estimation (ADAM)}
Merges the momentum and RMSProp paradigms.\\
Novelty is a bias correction of 1st/2nd moments.\\
Focus is on learning rate annealing (start fast, decrease).
\begin{enumerate}[(1)]
    \item \textbf{Initial:} $v_0=0,g_t=0$.
    \item \textbf{Momentum-variant:} $v_t=\beta_1 v_{t-1}+(1-\beta_1) \nabla_\theta J(\theta_t)$
    \item \textbf{RMSProp:} $g_t=\beta_2 g_{t-1}+(1-\beta_2) \nabla_\theta J(\theta_t)^2$
    \item \textbf{Bias Correction:} $v'_t=\frac{v_t}{1-\beta_1^t}$, $g'_t=\frac{g_t}{1-\beta_2^t}$ (where $\beta_i^t$ is the $t^{\textnormal{th}}$ power of $\beta_i$)
    \item \textbf{RMSProp + Momentum:} $\theta_t:=\theta_{t-1}-\frac{\varepsilon}{\sqrt{g'_t}+1\times 10^{-5}}v'_t$
\end{enumerate}
\textit{Pytorch Code}
\begin{lstlisting}[language=Python]
    torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08,
                    weight_decay=0, amsgrad=False, *, foreach=None,
                    maximize=False, capturable=False,
                    differentiable=False, fused=False)
\end{lstlisting}
\textit{Pseudocode}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.25]{Adam_code.png}
    \caption{Pseudocode of ADAM}
    \label{}
\end{figure}\end{center}


\section{Convolutional Neural Network (CNN)}
\subsection{Convolution and Cross-correlation}
Cross-correlation and convolution are both operations applied to images. Cross-correlation means sliding a kernel (filter) across an image. Convolution means sliding a flipped kernel across an image. \textbf{Most convolutional neural networks in machine learning libraries are actually implemented using cross-correlation}, but it doesn't change the results in practice because if convolution were used instead, the same weight values would be learned in a flipped orientation.

We have some \textit{source pixels}, we use a \textit{convolution kernel} (filter) to process the \textit{source pixels}. The output is \textit{destination pixels}.

Given the \textit{source pixels} $\{\textbf{Image}(x,y):x\in [-d_X,d_X],y\in [-d_Y,d_Y]\}$ and \textit{convolution kernel} (filter) $\{K(i,j):i,j\in[-d,d]\}$. $n_X=2d_X+1,n_Y=2d_Y+1$ are image dimension, $f=2d+1$ is the filter dimension. We can generate destination pixels in $(n_X-f+1)\times (n_Y-f+1) = (n_X-2d)\times (n_Y-2d)$

For $x\in [d+1,n_X-d],y\in [d+1,n_Y-d]$
\begin{enumerate}
    \item \textbf{Convolution:}
    \begin{equation}
        \begin{aligned}
            \textbf{CONV}(x,y)=\sum_{i=-d}^d\sum_{j=-d}^d \textbf{Image}(x-i,y-j)K(i,j)
        \end{aligned}
        \nonumber
    \end{equation}
    \item \textbf{Cross-Correlation:}
    \begin{equation}
        \begin{aligned}
            \textbf{CrossCorrelation}(x,y)=\sum_{i=-d}^d\sum_{j=-d}^d \textbf{Image}(x+i,y+j)K(i,j)
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.5]{convolution.jpeg}
    \caption{Convolution Used in CNN (actually cross-correlation)}
    \label{}
\end{figure}\end{center}

\subsubsection*{Kernel}
\begin{enumerate}
    \item \textbf{Edge Detection:} vertical $\begin{bmatrix}
        1&0&-1\\
        1&0&-1\\
        1&0&-1
    \end{bmatrix}$; horizontal $\begin{bmatrix}
        1&1&1\\
        0&0&0\\
        -1&-1&-1
    \end{bmatrix}$
    \item {Blur Pixel:} $\frac{1}{9}\begin{bmatrix}
        1&1&1\\
        1&1&1\\
        1&1&1
    \end{bmatrix}$; 
    {Sharpen:} $\begin{bmatrix}
        0&0&0\\
        0&2&0\\
        0&0&0
    \end{bmatrix}$; 
    {Identity:} $\begin{bmatrix}
        0&0&0\\
        0&1&0\\
        0&0&0
    \end{bmatrix}$; 
    {Shift Pixel:} $\begin{bmatrix}
        0&0&0\\
        1&0&0\\
        0&0&0
    \end{bmatrix}$
\end{enumerate}



\subsection{Padding (cover the border)}
\begin{definition}
    \underline{Padding} refers to the extension of the input image by adding a border of pixels the image.
\end{definition}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.15]{Padding.png}
    \caption{Padding: $p=1$}
    \label{}
\end{figure}\end{center}
For image $n\times n$ with filter $f\times f$ and padding $p$, the output has dimension
\begin{equation}
    \begin{aligned}
        (n+2p-f +1)\times (n+2p-f +1)
    \end{aligned}
    \nonumber
\end{equation}
In order for the output dimensions to be equivalent to the image dimension the padding value must be $p=\frac{f-1}{2}$

\subsection{Stride}
\begin{definition}
    \underline{Stride} refers to the sliding distance of the filter/kernel over spatial locations.
\end{definition}
The default stride or strides in two dimensions is (1,1) for the height and the width movement.

For example, the stride can be changed to (2,2). This has the effect of moving the filter two pixels right for each horizontal movement of the filter and two pixels down for each vertical movement of the filter when creating the feature map.

The new dimension of the output would be
\begin{equation}
    \begin{aligned}
        \left\lfloor \frac{n_X+2p-f}{s}+1\right\rfloor\times\left\lfloor \frac{n_Y+2p-f}{s}+1\right\rfloor
    \end{aligned}
    \nonumber
\end{equation}

\subsection{Other Layer Types}
\subsubsection{Pooling}
\begin{definition}
    \underline{Pooling} refers to the process of downsampling features by aggregating values at places in the feature map.
\end{definition}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.1]{maxpool.png}
    \caption{Example: maxpool}
    \label{}
\end{figure}\end{center}

\subsubsection{Unpooling}
\begin{definition}
    \underline{Unpooling} refers to the process of upsampling features by recreating the dimensions of feature map pooled and placing the pooled values into their original location.
\end{definition}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.1]{unpool.png}
    \caption{Example: maxpool+unpool}
    \label{}
\end{figure}\end{center}

\subsection{3D Convolution}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{3DConv.png}
    \caption{3D Convolution}
    \label{}
\end{figure}\end{center}
Note the default filter has dimension $f\times f\times n'$ ($n'$ is the number of filters in the previous layer)
\subsubsection*{Parameters Computing:}
\begin{enumerate}
    \item \textbf{CONV layer:} (shape of width of the filter * shape of height of the filter * number of filters in the previous layer+1)*number of filters\\
    (added 1 because of the bias term for each filter.)
    $$(5\times 5\times 3+1)\times 8=608$$
    \item \textbf{Fully Connected Layer (FC):} (current layer neurons number * previous layer neurons number)+1* current layer neurons number
    $$120\times 400+ 1\times 120=48120$$
\end{enumerate}






\section{Generative model}
\subsection{Autoencoders}
\textbf{Definition:} \textit{Self-supervised learning (SSL)} is a machine learning process where the model trains itself to learn one part of the input from another part of the input. It is a technique similar in scope to how humans learn to classify objects. SSL relies on unlabeled data to solve a task by splitting the task into at least two halves:
\begin{enumerate}
    \item A decomposition into pseudo-labels by withholding some training data (self-supervised task/pretext task); and,
    \item Reconstruction using either supervised or unsupervised learning.
\end{enumerate}
For example, in natural language processing, if we have a few words, using self-supervised learning we can complete the rest of the sentence. Similarly, in a video, we can predict past or future frames based on available video data.

\subsubsection{Basics}
\begin{definition}
    \textbf{Autoencoders} are designed to take the input data, say $x$, and, then, predict the very same input $x$! In other words, the network trains itself to imitate its input so that its output is the same.
\end{definition}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{Autoencoders.png}
    \caption{Autoencoders}
    \label{}
\end{figure}\end{center}
Undercomplete autoencoders are defined as having a bottleneck layer dimension that is less than that of the input. e.g. $$\text{dim}(z)<\text{dim}(x)$$
The lower dimention of bottleneck (hidden unit) avoids overfitting.

If the dimension is equal, the $x$ will be completely transformed into $x'$ which is often the same as the model learns nothing and may also be overfitted. Therefore, some other conditions are usually added to make the model only approximate its input, so that the model can really learn the hidden vector expression of the input samples and prevent overfitting.

\subsubsection{PCA and Autoencoders}

Principal Component Analysis (PCA) is using orthogonal basis to reduce dimensionality.
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{PCA.png}
    \caption{Principal Component Analysis (PCA)}
    \label{}
\end{figure}\end{center}
Consider a single hidden layer linear autoencoder network with linear activations and MSE loss:
\begin{equation}
    \begin{aligned}
        \vec{z}&=W^{[1]}\vec{x}+\vec{b}^{[1]}\\
        \vec{x}'&=W^{[2]}\vec{z}+\vec{b}^{[2]}\\
        L(\vec{x},\vec{x}')&=\|\vec{x}-\vec{x}'\|^2_2
    \end{aligned}
    \nonumber
\end{equation}
If we have the ${dim}(m) < {dim}(n)$, then the problem will be a PCA without an orthogonality restriction on the weights.

\textbf{Why bother with autoencoders if PCA exists?} Dimensional Reductions (Addressing curse of dimensionality)

- Rarely are we seeking to use an auto encoder for solely a dimension reduction.\\
- In such cases, we probably would be better off with:
\begin{enumerate}[$\bullet$]
    \item Principal Component Analysis (PCA): If linear and desire global structure under a deterministic algorithm.
    \item T-distributed stochastic neighbour embedding (t-SNE): If non-linear and desire local structure under a randomized algorithm with dense structures.
    \item Uniform Manifold Approximation and Projection (UMAP): If non-linear and desire local structure under a randomized algorithm with sparse structures.
\end{enumerate}

\subsubsection{Transposed Convolutions (upscale method)}
\textbf{Definition:} \textit{Transposed Convolutions}, \textit{uncov}, or \textit{fractionally striped convolution} is a technique to upscale the feature map so that it matches in dimension with the input feature map.\\
(Sometimes erroneously called "deconvolution".)
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{Transposed Convolutions.png}
    \caption{Transposed Convolutions}
    \label{}
\end{figure}\end{center}
\subsubsection*{Comparison between Convolution and Transposed Convolution}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{Comparison_conv.png}
    \caption{Comparison}
    \label{}
\end{figure}\end{center}
\subsubsection*{Transposed Convolution with Stride}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{Transposed Convolution With stride .png}
    \caption{Transposed Convolution with Stride}
    \label{}
\end{figure}\end{center}
For a image $n\times n$ with filter $f\times f$, padding $p$ and stride $s$, the dimension of the output is
\begin{equation}
    \begin{aligned}
        (s(n-1)+f-2p)\times (s(n-1)+f-2p)
    \end{aligned}
    \nonumber
\end{equation}



















































\section{Statistical Inference}

\subsection{Basics}
Given an observation $x\in X$, we want to estimate an unknown state $\theta \in S$ (not necessarily random). The $\theta$ can form $x$ with $P_\theta(x)$. We use decision rule $\delta (x)$ to form an action (estimation of $\theta$) $a=\hat{\theta}$.

\textbf{Example:}
\begin{enumerate}[(1)]
    \item Binary hypothesis testing (detection) when $S=\{0,1\}$ e.g. $P_0\sim N(0,\sigma^2), P_1\sim N(\mu,\sigma^2)$
    \item Multiple hypothesis testing (classification) when $S=\{1,2,...,n\}$
    \item (Estimation) when $S=\mathbb{R}$ e.g. $P_\theta\in N(\theta,\sigma^2)$
\end{enumerate}

\subsection{Decision Rule Examples}
\subsubsection*{Binary HT Example}
For the example Binary HT, $P_0\sim N(0,\sigma^2), P_1\sim N(\mu,\sigma^2)$: decision rule $\delta: \mathbb{R} \rightarrow \{0,1\}$

We can find a $\tau$ such that $\delta(x)=\left\{\begin{matrix}
    1,&x\ge \tau\\
    0,& else
\end{matrix}\right.=\mathbf{1}_{x\geq \tau}$. Howe to choose $\tau$?

Type-I error probability: probability that $\theta$ is $0$ but receive $\delta(x)=1$. $$P_I=P_0\{\delta(x)=1\}=P_0\{x\geq \tau\}=Q\left(\frac{\tau}{\sigma}\right)$$
Type-II error probability: probability that $\theta$ is $1$ but receive $\delta(x)=0$. $$P_{II}=P_1\{\delta(x)=0\}=P_1(x<\tau)=Q(\frac{\mu-\tau}{\sigma})$$

Both $P_I$ and $P_{II}$ depends on $\tau$. $Q(t)=\int_t^\infty\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}dx$

For $\tau=\frac{\mu}{2}$, $P_I=P_{II}=Q\left(\frac{\mu}{2\sigma}\right)$

\subsubsection*{Multiple HT Example}
Consider three state $S=\{1,2,3\}$.
We can find a $\tau$ such that $\delta(x)=\left\{\begin{matrix}
    1,&x< \tau_1\\
    2,& \tau_1\leq x\leq \tau_2\\
    3,& x>\tau_2
\end{matrix}\right.=\mathbf{1}_{x\geq \tau}$.

\textit{Conditional Error Probabilities:} probability that $\theta$ is $i$ but receive $\delta(x)=j$ (6 types in this example) $$P_i\{\delta(x)=j\}, \forall i\neq j$$

\subsubsection*{Estimation Example}
Ex: $P_\theta\sim N(\theta,\sigma^2)$. Perform $\delta(x)=\hat{\theta}$ by using mean-squared error (MSE):
$$MSE= \mathbb{E}_\theta \left[(\delta(x)-\theta)^2\right],\theta\in \mathbb{R}$$

\subsection{Maximum-Likelihood Principle (state is norandom)}
Maximum-Likelihood Principle $$\hat{\theta}=\argmax_{\theta\in S}P_{\theta}(x)=\argmax_{\theta\in S}\ln P_{\theta}(x)$$
Applied to the binary example: $P_0\sim N(0,\sigma^2), P_1\sim N(\mu,\sigma^2)$.

$P_0(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^2}{2\sigma^2}}, P_1(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$. $\ln P_0(x)=c-\frac{x^2}{2\sigma^2}$, $\ln P_1(x)=c-\frac{(x-\mu)^2}{2\sigma^2}$.

Then, the rule can become $$\hat{\theta}=\left\{\begin{matrix}
    0,&x^2<(x-\mu)^2\\
    1,&else
\end{matrix}\right.=\mathbf{1}_{x^2\geq (x-\mu)^2}=\mathbf{1}_{x\geq \frac{\mu}{2}}$$

\subsubsection*{Vector Observations}
Observations $X=\left(x_1,x_2,...,x_n\right)$, where i.i.d. $x_i\sim P_\theta$. Then $$P_\theta(X)=\prod_{i=1}^n P_\theta(x_i),\ \ln P_\theta(X)=\sum_{i=1}^n\ln P_\theta(x_i)$$
$\ln P_0(x)=cn-\frac{\sum_{i=1}^n x_i^2}{2\sigma^2}$, $\ln P_1(x)=cn-\frac{\sum_{i=1}^n(x_i-\mu)^2}{2\sigma^2}$.

Then, the rule can become $$\hat{\theta}=\left\{\begin{matrix}
    0,&\sum_{i=1}^nx_i^2<\sum_{i=1}^n(x_i-\mu)^2\\
    1,&else
\end{matrix}\right.=\mathbf{1}_{\sum_{i=1}^nx_i^2\geq \sum_{i=1}^n(x_i-\mu)^2}=\mathbf{1}_{\bar{x}\geq \frac{\mu}{2}}$$
where $\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i$. Under both $H_0$ and $H_1$, $\bar{x}\sim N(0,\frac{\sigma^2}{n})$.

Then, type I error prob and type II error prob are the same $$P_I=P_0\{\bar{x}\geq \frac{\mu}{2}\}=P_{II}=P_1\{\bar{x}< \frac{\mu}{2}\}=Q\left(\frac{\mu\sqrt{n}}{2\sigma}\right)$$

\subsubsection*{Estimation $S=\mathbb{R}$}
To estimate $\theta$ when $S=\mathbb{R}$
\begin{equation}
    \begin{aligned}
        &\max_{\theta\in \mathbb{R}}\sum_{i=1}^n\ln P_\theta(x_i)\\
        &\Leftrightarrow \max_{\theta\in \mathbb{R}} \left[cn-\frac{\sum_{i=1}^n(x_i-\theta)^2}{2\sigma^2}\right]\\
        &\Leftrightarrow \max_{\theta\in \mathbb{R}} \sum_{i=1}^n(x_i-\theta)^2 \Rightarrow \hat{\theta}=\bar{x}
    \end{aligned}
    \nonumber
\end{equation}

Then, with $\bar{x}\sim N(\theta,\frac{\sigma^2}{n})$, the $$MSE_\theta=\mathbb{E}_\theta\left(\bar{x}-\theta\right)^2=\frac{\sigma^2}{n}$$

\subsection{Bayesian Decision Rule (state is random)}
\subsubsection{Rules}
Prior probability distribution $\pi(\theta)$,

\underline{Loss/cost function} with action (estimation) $a$ is $l(a,\theta)$. e.g.
\begin{enumerate}
    \item (binary HT) Hamming/zero-one loss $l(a=\hat{\theta},\theta)=\mathbf{1}_{a\neq \theta}$
    \item (estimation) Squared error loss $l(a=\hat{\theta},\theta)=(a-\theta)^2$; Absolute error loss $l(a,\theta)=|a-\theta|$.
\end{enumerate}

\textbf{Risk of decision rule $\delta$:} $$R(\delta)=\mathbb{E}\left(l(\delta(X),\theta)\right)$$ where $(X,\theta)$ are random with prob $\pi(\theta),P_\theta$

\textbf{Note: to help be consistent with machine learning notations, we use $y$ to substitute $\theta$.}

The joint probability $P(x,y)=\pi(y)P_y(x)=P(x)\pi(y|x)$.

\begin{example}
    The risk of decision $\delta(x)$ in Hamming/zero-one loss $l(a=\hat{y},y)=\mathbf{1}_{a\neq y}$
    \begin{equation}
        \begin{aligned}
            R(\delta)&=\mathbb{E}(\mathbf{1}_{\delta(x)\neq y})=\mathbb{E}[\delta(x)\neq y]\\
            &=P(y=0)P[\delta(x)\neq 0|y=0]+P(y=1)P[\delta(x)\neq 1|y=1]\\
            &=P(y=0)P[\delta(x)=1|y=0]+P(y=1)P[\delta(x)=0|y=1]
        \end{aligned}
        \nonumber
    \end{equation}
\end{example}

\textbf{Bayes rule}
$$\delta_B=\argmin_\delta R(\delta)$$
\underline{Derive Bayes rule}
\begin{equation}
    \begin{aligned}
        R(\delta)&=\int_x\int_y P(x,y) l(\delta(x),y) dy dx\\
        &=\int_x P(x)\int_y \pi(y|x) l(\delta(x),y) dy dx\\
    \end{aligned}
    \nonumber
\end{equation}
Solve optimization problem:
\begin{equation}
    \begin{aligned}
        \min_\delta \int_x P(x)\int_y \pi(y|x) l(\delta(x),y) dy dx
    \end{aligned}
    \nonumber
\end{equation}
\begin{center}
    \fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{this problem can be transformed into optimization problems for each $x\in S$
    \begin{equation}
        \begin{aligned}
            \min_{\delta(x)} \int_y \pi(y|x) l(\delta(x),y) dy
        \end{aligned}
        \nonumber
    \end{equation}}}
\end{center}
\underline{The problem becomes to compute $\pi(y|x)$}. From $P(x,y)=\pi(y)P_y(x)=P(x)\pi(y|x)$, we know $$\pi(y|x)=\frac{\pi(y)P_y(x)}{P(x)}$$

\subsubsection{Maximum A Posteriori (MAP) Decision Rule (Binary example)}
\begin{example}
    Hamming/zero-one loss $l(a,y)=\mathbf{1}_{a\neq y}$
\end{example}

\textbf{Maximum A Posteriori (MAP) Decision Rule}:

Optimization problem is $$\delta(x)=\argmin_{a} \sum_{y=0,1} \pi(y|x) \mathbf{1}_{a\neq y} dy=\argmax_{y\in\{0,1\}}\pi(y|x)$$
$$\Rightarrow \sum_{y=0,1} \pi(y|x) \mathbf{1}_{\delta(x)\neq y} dx=\min_{a} \sum_{y=0,1} \pi(y|x) \mathbf{1}_{a\neq y} dy=\min\{\pi(1|x),\pi(0|x)\}$$

Likelihood ratio: $L(x)=\frac{P_1(x)}{P_0(x)}$

Likelihood ratio test: threshold $\tau=\frac{\pi(0)}{\pi(1)}$. If $L(x)>\tau$ accept $H_1$ (equivalent to $P_1(x)\pi(1)>P_0(x)\pi(0)$ which is also equivalent to comparing $\pi(y|x)$).

In this rule the whole optimization problem also goes to
\begin{equation}
    \begin{aligned}
        R(\delta_{MAP})&=\int_x P(x)\sum_{y=0,1} \pi(y|x) \mathbf{1}_{\delta(x)\neq y} dx\\
        &=\int_x P(x)\min\{\pi(1|x),\pi(0|x)\}dx
    \end{aligned}
    \nonumber
\end{equation}

\subsubsection{Minimum Mean Squared Error (MMSE) Rule ($\mathbb{R}^n$ example)}
\begin{example}
    (estimation) Squared error loss $l(a,y)=(a-y)^2$.
\end{example}
\subsubsection*{Minimum Mean Squared Error (MMSE) Rule:}
Optimization problem is $\delta(x)=\argmin_{a} \int_{y} \pi(y|x) (a-y)^2 dy$
\begin{equation}
    \begin{aligned}
        0=\int_{y} \pi(y|x) (\delta_B(x)-y) dy=\delta_B(x)-\mathbb{E}\left[Y|X=x\right]
    \end{aligned}
    \nonumber
\end{equation}
\begin{equation}
    \begin{aligned}
        \Rightarrow \delta_B(x)=\mathbb{E}\left[Y|X=x\right]
    \end{aligned}
    \nonumber
\end{equation}
which is called \textbf{conditional mean estimation}.

In this rule the whole optimization problem also goes to
$$R(\delta_{MMSE})=\int_x P(x)\int_y \pi(y|x) (y-\mathbb{E}\left[Y|X=x\right])^2 dy dx=\mathbb{E}_X Var\left[Y|X=x\right]$$

\textbf{Gaussian case:}
If $X\in \mathbb{R}^n$ and $(Y,X)$ are jointly Gaussian, then the conditional mean is a linear function of x, also called linear MMSE estimator.
\begin{equation}
    \begin{aligned}
        \mathbb{E}\left[Y|X=x\right]=\mathbb{E}[Y]+Cov(Y,X)Cov(X)^{-1}(x-\mathbb{E}[X])
    \end{aligned}
    \nonumber
\end{equation}
and the posterior risk is independent of $x$:
\begin{equation}
    \begin{aligned}
        Var\left[Y|X=x\right]=Var[Y]-Cov(Y,X)Cov(X)^{-1}Cov(X,Y)
    \end{aligned}
    \nonumber
\end{equation}
\underline{\textbf{Note:}} MMSE estimator coincides with the MAP estimator for Gaussian Variables.

\subsection{Comparison}
Maximum-Likelihood Principle (state is nonrandom): $\delta_{ML}(x)=\argmax_y P_y(x)$.

Maximum A Posteriori (MAP) Decision Rule (state is random): $\delta_{MAP}(x)=\argmax_y \pi(y|x)=\argmax_y \{\pi(y|x),P_y(x)\}$


\section{Machine Learning in Inference}
Instead of given a prior distribution of $Y$, we are given a \textbf{training set} $T=(X_i,Y_i)_{i=1}^n$ where i.i.d. $(X_i,Y_i)\sim P$. (Distribution $P$ is unknown).

Risk: $R(\delta)=\mathbb{E}_P\left[l(\delta(X),Y)\right]$

The ture optimal decision rule is $$\delta_B=\argmin_\delta \mathbb{E}_P\left[l(\delta(X),Y)\right]$$
which is can't be computed since we don't know how actually $P$ is.

\subsection{Empirical Risk Minimization (ERM)}
Instead of computing optimal decision rule with $P$, we compute the optimal decisioin rule in the training set:
$$\hat{\delta}_n=\argmin_\delta \frac{1}{n}\sum_{i=1}^nl(\delta(X_i),Y_i)$$
The corresponding risk is $R(\hat{\delta}_n)=\mathbb{E}_P\left[l(\hat{\delta}_n(X),Y)\right]$. $\Delta R(\hat{\delta}_n)=R(\hat{\delta}_n)- R(\delta)>0$ always holds.

\textbf{Consistency:} if $\Delta R(\hat{\delta}_n) \rightarrow 0$ as $n \rightarrow \infty$.

\subsubsection{Example: Linear MMSE (LMMSE) estimator}
Use the decision role in the class of $\delta(x)=wx$. To find the linear MMSE (LMMSE) estimation $\delta^*(x)=w^*x$:
\begin{equation}
    \begin{aligned}
        w^*=\argmin_{w}\mathbb{E}_P\left[(wX-Y)^2\right]=\frac{\mathbb{E}[XY]}{\mathbb{E}[X^2]}
    \end{aligned}
    \nonumber
\end{equation}
The rule that minimizes the \textbf{empirical risk} is
\begin{equation}
    \begin{aligned}
        \hat{w}=\argmin_{w}\frac{1}{n}\sum_{i=1}^n(wX_i-Y_i)^2=\frac{\frac{1}{n}\sum_{i=1}^nX_iY_i}{\frac{1}{n}\sum_{i=1}^nX_i^2}
    \end{aligned}
    \nonumber
\end{equation}
The risk of the optimal rule $\delta^*=w^*x$ is $R(\delta^*)$ and the empirical risk under rule $\hat{\delta}(x)=\hat{w}x$ is $R(\hat{\delta}(x))$. $R(\hat{\delta})>R(\delta^*)$ always holds, and $$R(\hat{\delta})\rightarrow R(\delta^*)\text{ as }n \rightarrow \infty$$
According to CLT:
\begin{equation}
    \begin{aligned}
        \sqrt{n}\left(\frac{1}{n}\sum_iX_iY_i- \mathbb{E}(XY)\right) &\stackrel{d}{\longrightarrow} N(0,\sigma^2)\\
        \sqrt{n}\left(\frac{1}{n}\sum_iX_i^2- \mathbb{E}(X^2)\right) &\stackrel{d}{\longrightarrow} N(0,\sigma^2)
    \end{aligned}
    \nonumber
\end{equation}
Then,
\begin{equation}
    \begin{aligned}
        \frac{1}{n}\sum_iX_i^2=\mathbb{E}(X^2)+O(\frac{1}{\sqrt{n}})\\
        \frac{1}{n}\sum_iX_iY_i=\mathbb{E}(XY)+O(\frac{1}{\sqrt{n}})\\
    \end{aligned}
    \nonumber
\end{equation}
which means the error of estimators
\begin{equation}
    \begin{aligned}
        \hat{w}=\frac{\mathbb{E}(X^2)+O(\frac{1}{\sqrt{n}})}{\mathbb{E}(XY)+O(\frac{1}{\sqrt{n}})}=w^*+O(\frac{1}{\sqrt{n}})
    \end{aligned}
    \nonumber
\end{equation}
$$\hat{w}-w^*=O(\frac{1}{\sqrt{n}})$$
and the error of the risks:
\begin{equation}
    \begin{aligned}
        R(\hat{\delta})-R(\delta^*)&=\mathbb{E}_P[\hat{w}X-Y]^2-\mathbb{E}_P[w^*X-Y]^2\\
        &=\mathbb{E}_P[(\hat{w}-w^*)X+w^*X-Y]^2-\mathbb{E}_P[w^*X-Y]^2\\
        &=\mathbb{E}_P[(\hat{w}-w^*)X]^2+2(\hat{w}-w^*)\mathbb{E}_P[X(w^*X-Y)]\\
        &=(\hat{w}-w^*)\mathbb{E}_P(X^2)=O\left(\frac{1}{n}\right)
    \end{aligned}
    \nonumber
\end{equation}

\begin{center}
    \fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\textbf{\underline{Complexity}:}
    \begin{definition}
        A sequence $f(n)$ is $O(1)$ if $\lim_{n \rightarrow \infty}f(n)<\infty$.
    \end{definition}
    
    \begin{definition}
        A sequence $f(n)$ is $O(g(n))$ if $\frac{f(n)}{g(n)}$ is $O(1)$.
    \end{definition}
    
    \begin{definition}
        A sequence $f(n)$ is $o(1)$ if $\lim_{n \rightarrow \infty}\sup f(n)=0$.
    \end{definition}
    
    \begin{definition}
        A sequence $f(n)$ is $o(g(n))$ if $\lim_{n \rightarrow \infty}\sup \frac{f(n)}{g(n)}=0$.
    \end{definition}
    
    \begin{definition}
        A sequence $f(n)$ is \underline{asymptotic} to $g(n)$ if $\lim_{n \rightarrow \infty} \frac{f(n)}{g(n)}=1$. (This is denoted by $f(n)\sim g(n)$ as $a \rightarrow \infty$)
    \end{definition}
    }}
\end{center}

\subsubsection{Penalized ERM}
$\delta(x)=\sum_{j=1}^Jw_jx^j$

Pick $J=d$ and use ERM with $d$ dimensional $w$: $$\argmin_{w\in \mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n[w^TX_i-Y_i]^2$$

Approach 1: Fix $d<<n$, use ERM.

Approach 2: (\textbf{Penalized ERM}) $$\min_\delta [R_{emp}(\delta)+J(\delta)]$$ ($J(\delta)$ is regularization (penalty) term)

\subsection{Stochastic Approximation}
Robbins and Monro (95)

\underline{Problem:} Find a root of function $h(x)$. ($f(x)=0$)

We do not observe $h(x)$ directly, but we observe $Y\sim P_x$ with
\begin{enumerate}[(1)]
    \item $\mathbb{E}[Y|X=x]=h(x)$
    \item $(Y|X=x)-h(x)$ is bounded
\end{enumerate}
\begin{example}
    $Y=X+Z$ with $\mathbb{E}[Z]=0$ and $Z$ is bounded.
\end{example}
Assumptions: 1. $h'(x^*)>0$; 2. $x^*$ is the unique root of $h$.

\textbf{SA Algorithm}
\begin{enumerate}[$\bullet$]
    \item Pick Sequence $\{a_n\}$ such that $\sum_{n=1}^\infty a_n=\infty$ and $\sum_{n=1}^\infty a^2_n<\infty$ (should converge to $0$ but not too quick) e.g. $a_n=n^{-\alpha}$ when $\alpha\in (\frac{1}{2},1]$.
    \item Initialize $X_1$
    \item Update for $n=1,2,...$, $Y_n\sim P(\cdot|X=X_n)$ $$X_{n+1}=X_n-a_n Y_n$$ until convergence.
\end{enumerate}

\begin{theorem}
    Under these assumptions $$X_n \stackrel{m.s.}{\longrightarrow} x^*\text{ as }n \rightarrow \infty$$ i.e., $\mathbb{E}(X_n-x^*)^2 \rightarrow 0$ as $n \rightarrow \infty$.
\end{theorem}

\begin{enumerate}[$\bullet$]
    \item \textbf{\underline{Performance Measure}} (Convergence rate): the root mean squared error (RMSE) $e_n=\sqrt{\mathbb{E}[(X_n-x^*)^2]}$.
    \item \underline{Projections}: If $x$ is constrained to live in an interval $I$, the update rule becomes $$X_{n+1}=\text{Proj}_x[X_n-a_nY_n]$$
    \item \underline{Averaging}: $$\bar{X}_n=\frac{1}{n}\sum_{i=1}^nX_i=\frac{X_n}{n}+\bar{X}_{n-1}\frac{n-1}{n}$$(nicer graph) (The benefits of this smoothing operation are mostly seen in the initial stages of the SA recursion, and \underline{do not improve the convergence rate}.)
\end{enumerate}

\begin{example}
    Let $h(x) = x$, in which case $x^* = 0$. $Y_n=h(X_n)+Z_n=X_n+Z_n$ where noise $Z_n$ is independent of $Y_n$ with $\mathbb{E}[Z_n]=0,Var(Z_n)=1$ and $Z_n$ is bounded.
\end{example}
Then,
    \begin{equation}
        \begin{aligned}
            X_{n+1}&=X_n-a_n(X_n+Z_n)\\
            &=(1-a_n)X_n-a_n Z_n
        \end{aligned}
        \nonumber
    \end{equation}
The MSE,
\begin{equation}
    \begin{aligned}
        e_{n+1}^2&=\mathbb{E}(X_{n+1}-x^*)^2=\mathbb{E}(X_{n+1})^2\\
        &=\mathbb{E}[(1-a_n)X_n-a_n Z_n]^2\\
        &=(1-a_n)^2\mathbb{E}X_n^2+a_n^2\mathbb{E}Z_n^2\\
        &=(1-a_n)^2e_n^2+a_n^2
    \end{aligned}
    \nonumber
\end{equation}
Pick $a_n=n^{-\alpha}$, where $\alpha\in (\frac{1}{2},1]$
\begin{equation}
    \begin{aligned}
        \Rightarrow e_{n+1}^2=(1-n^{-\alpha})^2e_n^2+n^{-2\alpha}
    \end{aligned}
    \nonumber
\end{equation}
\begin{center}
    \fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\center{Guess: $e_n=\sqrt{c}n^{-\beta}+H.O.T$}
    \begin{equation}
        \begin{aligned}
            c(n+1)^{-2\beta}+H.O.T=(1-n^{-\alpha})^2cn^{-2\beta}+n^{-2\alpha}+H.O.T
        \end{aligned}
        \nonumber
    \end{equation}
    (where $(n+1)^{-2\beta}=n^{-2\beta}(1+\frac{1}{n})^{-2\beta}=n^{-2\beta}[1-2\beta n^{^{-1}}+O(n^{-2})]$, by Taylor)
    \begin{equation}
        \begin{aligned}
            cn^{-2\beta}-2c\beta n^{^{-1-2\beta}}+H.O.T&=(1-n^{-\alpha})^2cn^{-2\beta}+n^{-2\alpha}+H.O.T\\
            -2c\beta n^{^{-1-2\beta}}+H.O.T&=-2cn^{-\alpha-2\beta}+n^{-2\alpha}+H.O.T
        \end{aligned}
        \nonumber
    \end{equation}
    \textbf{(For $\alpha<1$)}, $-2c\beta n^{^{-1-2\beta}}$ is not dominant term.
    $$H.O.T=-2cn^{-\alpha-2\beta}+n^{-2\alpha}+H.O.T$$
    Identify Power: $2\alpha=\alpha+2\beta$ $\Rightarrow$ $\beta=\frac{\alpha}{2}$ and $c=\frac{1}{2}$

    \textbf{(For $\alpha=1$)}, there are three dominant terms.
    $$-2c\beta n^{^{-1-2\beta}}+H.O.T=-2cn^{-1-2\beta}+n^{-2}+H.O.T$$
    Identify Power: $2=1+2\beta$ $\Rightarrow$ $\beta=\frac{1}{2}$ and $-2c\beta=-2c+1\Rightarrow c=1$

    $$e_n^2\sim c n^{-2\beta}$$
    To let the convergence rate as fast as possible, we want the $\beta$ to be as large as possible. Since $\beta=\frac{\alpha}{2}$, we pick the highest $\alpha=1 \Rightarrow \beta=\frac{1}{2},c=1$.
    $$e_n=O(n^{-\frac{1}{2}})\text{ with }a_n\sim\frac{1}{n}$$

    }}
\end{center}

\begin{example}
    Let $h(x) = x^3$, in which case $x^* = 0$. $Y_n=h(X_n)+Z_n=X_n^2+Z_n$ where noise $Z_n$ is independent of $Y_n$ with $\mathbb{E}[Z_n]=0,Var(Z_n)=1$ and $Z_n$ is bounded.
\end{example}
Then,
    \begin{equation}
        \begin{aligned}
            X_{n+1}&=X_n-a_n(X^3_n+Z_n)\\
            &=X_n-a_nX^3_n-a_n Z_n
        \end{aligned}
        \nonumber
    \end{equation}
Pick $a_n=n^{-\alpha}$, $\alpha\in (\frac{1}{2},1]$ $\Rightarrow \beta=\frac{1}{6},\alpha=\frac{2}{3}$ $\Rightarrow e_n\sim O(n^{-\frac{1}{6}})$

\subsection{Stochastic Gradient Descent (SGD)}
Solve $\min_{x\in\mathbb{R}^n}f(x)$.

We only use a \textbf{noisy version} $g(x,z)$ of $f(x)$, where $\mathbb{E}_z[g(x,z)]=f(x)$.
\begin{equation}
    \begin{aligned}
        \mathbb{E}_z[\nabla_x g(x,z)]=\nabla_x \mathbb{E}_z[g(x,z)]=\nabla f(x)
    \end{aligned}
    \nonumber
\end{equation}
Also pick sequence $\{a_n\}$ such that $\sum_{n=1}^\infty a_n=\infty$ and $\sum_{n=1}^\infty a^2_n<\infty$.

\textbf{SGD}
\begin{enumerate}[$\bullet$]
    \item Initialize $X_1$
    \item Update for $n=1,2,...$, $$X_{n+1}=X_n-a_n \nabla g(X_n,Z_n)$$
\end{enumerate}

\begin{example}
    $f(x)=\frac{1}{2}x^2,x\in \mathbb{R}$. Let $Z$ be a random variable with $\mathbb{E}(Z)=0,Var(Z)=1$. $$g(x,Z)=\frac{1}{2}(x+Z)^2-\frac{1}{2}$$
    \begin{equation}
        \begin{aligned}
            \mathbb{E}[g(x,Z)]&=\frac{1}{2}x^2=f(x)\\
            \nabla_x g(x,Z)&=x+Z \Rightarrow 
            \mathbb{E}[\nabla_xg(x,Z)]=\nabla f(x)
        \end{aligned}
        \nonumber
    \end{equation}
    $$X_{n+1}=X_n-a_n(X_n+Z_n)$$
    which is the same as the stochastic approximation.
\end{example}
\textbf{Main Results:} (Suppose the unique minimum is $x^*$)
\begin{enumerate}[(1)]
    \item \underline{Convergence:} $e_n \rightarrow 0$ as $n \rightarrow \infty$.
    \item \underline{Convergence Rate:} To achieve $\mathbb{E}[f(X_n)]-f(x^*)<\varepsilon$, we need $n=O(\frac{1}{\varepsilon})$ if $f$ is twice continuously differentiable and strongly convex.
\end{enumerate}

GD has linear convergence $\Rightarrow$ $e_n=O(e^{-cn})$; Solve $\varepsilon=O(e^{-cn}) \Rightarrow n=O(\ln \frac{1}{\varepsilon})$. (\textbf{SGD is much worse than GD}, cost more.)

\subsection{SGD Application to Empirical Risk Minimization (ERM)}
ERM problem is
$$\min_{w\in \mathbb{R}^d}\frac{1}{n}\sum_{i=1}^nL(\delta_w(X_i),Y_i)$$
$R_{emp}(w)=\frac{1}{n}\sum_{i=1}^nL(\delta_w(X_i),Y_i)$ is the empirical risk (e.g. $\delta_w(x)=w^Tx$, $L(\hat{y},y)=(\hat{y}-y)^2$)
To make $w$ more visible, we can write
\begin{equation}
    \begin{aligned}
        R_{emp}(w)=\frac{1}{n}\sum_{i=1}^nL(\delta_w(X_i),Y_i)
        =\frac{1}{n}\sum_{i=1}^nQ(X_i,Y_i,w)
    \end{aligned}
    \nonumber
\end{equation}
For penalized ERM we would similarly have $$\min_{w\in \mathbb{R}^d}\frac{1}{n}\sum_{i=1}^nQ(X_i,Y_i,w)+J(w)$$ $J(w)$ is the penalty (regularization) term.

In the problem $\min_{W\in \mathbb{R}^n}R_{emp}(w)=\frac{1}{n}\sum_{i=1}^nQ(X_i,Y_i,w)$

\subsubsection{Different Gradient Descent for ERM}
\begin{enumerate}
    \item[\textbf{GD}]
    \begin{enumerate}[$\bullet$]
        \item Initialize $W_1$
        \item Update for $k\geq 1$,
        Update: $W_{k+1}=W_k-a_k \frac{1}{n}\sum_{i=1}^n\nabla Q(X_i,Y_i,W_k)$
    \end{enumerate}
    \textbf{Computational cost:} The computational cost of GD is $O(dn)$ operations per iteration. Since GD has exponential convergence, the number of iterations needed to reach an optimization error of $\rho$ is $O(\log\frac{1}{\rho})$. Hence GD incurs a \underline{total computational cost of $O(dn\log\frac{1}{\rho})$} to reach a solution $W_k$ such that $R_{emp}(W_k)\leq \min_WR_{emp}(W)+\rho$
    \item[\textbf{SGD}]
    \begin{enumerate}[$\bullet$]
        \item Initialize $W_1$
        \item Update for $k\geq 1$,
        \begin{enumerate}[{Step} 1:]
            \item Pick $i$ uniformly over $\{1,...,n\}$
            \item $W_{k+1}=W_k-a_k \nabla Q(X_i,Y_i,W_k)$
        \end{enumerate}
    \end{enumerate}
    \textbf{Computational cost:} After $k$ iterations, $\mathbb{E}[R_{emp}(W_k)]\leq \min_W R_{emp}(W)+\rho$, for $k=O(\frac{1}{\rho})$ and $f$ twice differentiable and strongly convex. The cost per iteration is $O(d)$ (independent of $n$), so the \underline{total computational cost is $O(\frac{d}{\varepsilon})$}.
\end{enumerate}

\subsubsection{Constraints on Learning Problem}
\textbf{Why achieving a low value of $\rho$ is useful (low error of $R_{emp}(\cdot)$), since the cost function $R_{emp}(\cdot)$ is only a surrogate for the actual risk $R(\cdot)$?}


Typically, $d=O(n^b)$ where $0<b<1$.

For any numerical algorithm producing a decision rule $\tilde{\delta_n}$, the excess risk (compared to Bayes rule $\delta_B$) can be expressed as the sum of three terms:
\begin{equation}
    \begin{aligned}
        \Delta R(\tilde{\delta_n})&=R(\tilde{\delta_n})-R(\delta_B)\\&=[R(\tilde{\delta_n})-R(\hat{\delta_n})]+[R(\hat{\delta_n})-R(\delta^*)]+[R(\delta^*)-R(\delta_B)]
    \end{aligned}
    \nonumber
\end{equation}

where
\begin{equation}
    \begin{aligned}
        \delta_B&=\text{ Bayes rule}\\
        \delta^*&=\text{ best rule in } D=\argmin_{\delta\in D} R(\delta)\\
        \hat{\delta_n}&=\argmin_{\delta\in D}R_{emp}(\delta)\\
        \tilde{\delta_n}&=\text{solution of the algorithm after $k$ iterations}
    \end{aligned}
    \nonumber
\end{equation}
(Note: $D$ is the set of all available decision rule in approximation (e.g. all linear parameters $\{W,b\}$), which can't be better than Bayes rule)

The expected excess risk
\begin{equation}
    \begin{aligned}
        \epsilon =\mathbb{E}[\Delta R(\tilde{\delta_n})]=\underbrace{\mathbb{E}[R(\tilde{\delta_n})-R(\hat{\delta_n})]}_{\text{Comp. Error}=\rho}+\underbrace{\mathbb{E}[R(\hat{\delta_n})-R(\delta^*)]}_{\text{Est. Error}=O(\frac{d}{n})}+\underbrace{\mathbb{E}[R(\delta^*)-R(\delta_B)]}_{\text{Approx. Error}=O(d^{-\beta})}
    \end{aligned}
    \nonumber
\end{equation}

\textit{Estimation error} increases as $d$ increases, but \textit{approximation error} decreases as $d$ increases. To minimize the excess risk, we want to balance the last two items, that is $O(\frac{d}{n})=O(d^{-\beta})$: solve $$\frac{d}{n}=d^{-\beta} \Rightarrow d^{1+\beta}=n \Rightarrow d=n^{\frac{1}{1+\beta}}$$
$$\Rightarrow\text{ the last two items } O(\frac{d}{n})=O(d^{-\beta})=O(n^{-\gamma})$$
where $\gamma=\frac{\beta}{1+\beta}\in(0,1]$ is a constant.

To balance the three items, we want
\begin{center}
    $\rho=O(n^{-\gamma})\Rightarrow n=O(\rho^{-\frac{1}{\gamma}})$ and $d=O(n^{\frac{1}{1+\beta}})$
\end{center}

The update rule $W_{k+1}=W_k-a_k \nabla Q(X_k,Y_k,W_k)$, where $i\sim \text{Uniform}\{1,2,...,n\}$

\textbf{Relation to Online Learning:} When the training data are made available sequentially (instead of in a batch as assumed here), online learning can be used to sequentially learn the decision rules (or the weights that parameterize the decision rule).

\textbf{Variations on Basic SGD:} mini batch: replace $S$ by a subset $B$ and $n$ by $|B|$
$$\frac{1}{|B|}\sum_{i\in B}Q(X_i,Y_i,W_k)$$

\textbf{Averaging SGD:} $$\bar{W}_n=\frac{1}{n}\sum_{i=1}^nW_i=\frac{W_n}{n}+\bar{W}_{n-1}\frac{n-1}{n}$$

\textbf{SVRG (Stochastic Variance Randomed Gradient):} R. Johnson and T. Zhang, “Accelerating Stochastic Gradient Descent using Predictive Variance Reduction,” \textit{Proc. NIPS} 2013.

\textbf{Unsupervised learning:} If no explanatory variable $X$ is present, the problem reduces to
$$
\min _w \frac{1}{n} \sum_{i=1}^n Q\left(Y_i, w\right)
$$
which finds applications to various unsupervised learning problems. For instance the $k$-means clustering algorithm partitions $n$ data points $y_i, 1 \leq i \leq n$ in $\mathbb{R}^d$ into $k$ clusters with centroids $w_j, 1 \leq j \leq k$, in a way that minimizes the within-cluster sum-of-squares: WCSS $=\frac{1}{n} \sum_{i=1}^n \min _{1 \leq j \leq k}\left\|y_i-w_j\right\|^2$. Using the formalism of (7), we have $Q(y, w)=\min _{1 \leq j \leq k}\left\|y-w_j\right\|^2$ where $y \in \mathbb{R}^d$ and $w=\left\{w_j\right\}_{j=1}^k \in \mathbb{R}^{d \times k}$ is a matrix whose $k$ columns are the centroid vectors.


\section{Stochastic Integration Methods}
Integral $I=\int_Xf(x)dx$.

\subsection{Deterministic Methods (Better in Low Dimension)}
\subsubsection{Riemann Integration}
Riemann integral: approximation integral $I$ of $f(x)$ in $[a,b]$ with $$\hat{I}_n=\sum_{i=1}^n(\underbrace{x_i-x_{i-1}}_{\frac{b-a}{n}})f(x_i)$$
where $x_i=a+\frac{b-a}{n}i$. We can also denote $\hat{f}(x)=f(x_i)$ if $x\in (x_{i-1},x_i]$

The error $|\hat{I}_n-I|=\int_a^b|\hat{f}(x)-f(x)|dx$

Assume $f$ is differentiable and $\max_x|f'(x)|=c<\infty$, then $|\hat{f}(x)-f(x)|\leq \frac{b-a}{n}c$
$$\Rightarrow |\hat{I}_n-I|\leq \int_a^b\frac{b-a}{n}c\ dx=\frac{(b-a)^2}{n}c$$
That is $n\sim O\left(\varepsilon^{-1}\right)$

\subsubsection{Trapezoidal Rule}
Using average can be better. $$\hat{I}_n=\sum_{i=1}^n(\underbrace{x_i-x_{i-1}}_{\frac{b-a}{n}})\frac{f(x_i)+f(x_{i-1})}{2}$$
The upper bound of error is $|\hat{I}_n-I|\leq \frac{C}{n^2}$ for some constant $C$.

That is $n\sim O\left(\varepsilon^{-\frac{1}{2}}\right)$

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{Riemann and Trapezoidal.png}
    \caption{(a) Riemann approximation; (b) Trapezoidal approximation.}
    \label{}
\end{figure}\end{center}

\subsubsection{Multidimensional Integration}
When we want to do intergral in high dimention, it will be really hard.

For $d-$dimensional integrals, the trapezoidal rule yields an approximation error $|\hat{I}_n-I|\leq \frac{C}{n^\frac{2}{d}}$ for some constant $C$. That is $n\sim O\left(\varepsilon^{-\frac{d}{2}}\right)$. $n$ needs to increase exponentially with $d$ to achieve a target approximation error $\varepsilon$. This phenomenon is known as \underline{the curse of dimensionality}.
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.3]{two-dim int.png}
    \caption{Two-dimensional integration using regular grid.}
    \label{}
\end{figure}\end{center}



\subsection{Stachastic Methods (Better in High Dimension)}
\subsubsection{Classical Monte Carlo Integration}

Compute the expetation $$\xi=\mathbb{E}_p[h(x)]=\int_X \underbrace{p(x)h(x)}_{f(x)}dx$$

The methods described below can be used to solve the following problems: (1) General $\int_X f$; (2) Compute the probability of falling into a subset $a\subset X :P(a)=\int_a p(x)dx$, where $h(x)=\mathbf{1}_{x\in a}$

The Monte Carlo approach is as follows: Given $X_1, X_2, \cdots, X_n$ drawn i.i.d from the pdf
$p$, estimate $\xi $ by the empirical average $$\hat{\xi}_n=\frac{1}{n}\sum_{i=1}^nh(X_i)$$

$\mathbb{E}_p[\hat{\xi}_n]=\mathbb{E}_p[h(X)]=\xi$. $\hat{\xi}_n \stackrel{a.s.}{\longrightarrow}\xi$ as $n \rightarrow \infty$ by SLLW.

$Var(\hat{\xi}_n-\xi)=Var(\hat{\xi}_n)=\frac{1}{n}Var[h(x)]=O\left(\frac{1}{n}\right) \Rightarrow sd(\hat{\xi}_n)=\frac{\sqrt{Var[h(x)]}}{\sqrt{n}}$

That is $n\sim O\left(n^{-\frac{1}{2}}\right)$

The stochastic methods \textbf{outperform} when the deterministic ones for dimensions \underline{$d >4$} and are \textbf{worse} for \underline{$d<4$}.

\subsubsection{Importance Sampling}
Draw $X_i,i=1,...,n$ i.i.d from pdf $q$ $$\hat{\xi}_n=\frac{1}{n}\sum_{i=1}^n\frac{p(X_i)}{q(X_i)}h(X_i)$$
It is an unbiased estimator of $\xi$
\begin{equation}
    \begin{aligned}
        \mathbb{E}[\hat{\xi}_n]=\mathbb{E}_q[\frac{p(X_i)}{q(X_i)}h(X_i)]=\int_X p(x)h(x)dx=\xi
    \end{aligned}
    \nonumber
\end{equation}
$\hat{\xi}_n \stackrel{a.s.}{\longrightarrow}\xi$ as $n \rightarrow \infty$ by SLLW.

Its variance is
\begin{equation}
    \begin{aligned}
        {Var}_q(\hat{\xi}_n)&=\frac{1}{n}{Var}_q\left[\frac{p(X_i)}{q(X_i)}h(X_i)\right]\\
        &=\frac{1}{n}\left(\int_X\frac{p^2(x)}{q(x)}h^2(x)dx-\xi^2\right)
    \end{aligned}
    \nonumber
\end{equation}
The idea of importance sampling is to find a good $q$ such that $${Var}_q(\hat{\xi}_n)<{Var}_p(\hat{\xi}_n)$$

\subsubsection*{Error Meausre}
The \textit{relative error} of the importance-sampling estimator is defined as
$$
\delta_{\mathrm{rel}}\left(\hat{\xi}_n\right) \triangleq \frac{\sqrt{\operatorname{Var}_q\left(\hat{\xi}_n\right)}}{\xi}=\sqrt{\frac{\operatorname{Var}_q\left[\frac{p(X)}{q(X)} h(X)\right]}{\xi^2 n}} .
$$
The \textit{number} of simulations needed to achieve a relative error of $\delta$ is
$$
n_{I S}(\delta)=\frac{\operatorname{Var}_q\left[\frac{p(X)}{q(X)} h(X)\right]}{\xi^2 \delta^2} .
$$
The \textit{gain} \underline{relative to a Monte Carlo simulation} is defined as
$$
\Gamma=\frac{n_{M C}(\delta)}{n_{I S}(\delta)}=\frac{\operatorname{Var}_p[h(X)]}{\operatorname{Var}_q\left[\frac{p(X)}{q(X)} h(X)\right]}
$$

\textbf{In the example of $\xi=P(a), h(x)=\mathbf{1}_{x\in a}$:} Suppose $\xi=P(a)\approx 10^{-9}$ (small), $\hat{\xi}_n=\frac{1}{n}\sum_{i=1}^n\mathbf{1}_{X_i\in a}$. $\mathbf{1}_{X_i\in a}$ is $Bernoulli(\xi)$. We have $Var(\hat{\xi}_n)=\frac{\xi(1-\xi)}{n}$.

We can use relative error to measure $$\delta_{\mathrm{rel}}\left(\hat{\xi}_n\right)=\frac{\sqrt{Var(\hat{\xi}_n)}}{\xi}=\sqrt{\frac{1-\xi}{n\xi}}$$
and the number of simulation need to get relative error $\delta$ is $$
n_{I S}(\delta)=\frac{1-\delta} {\xi \delta^2} .
$$

\textbf{Find the optimal $q$:}
$$\min_q\int_X\frac{p^2(x)}{q(x)}h^2(x)dx-\xi^2$$
write $$\int_X\frac{p^2(x)}{q(x)}h^2(x)dx-\xi^2=\mathbb{E}_q\left[\left(\underbrace{\frac{p(x)}{q(x)}h(x)}_{Z}\right)^2\right]$$
Since $x^2$ is convex function, by Jensen's inequality
$$\mathbb{E}_q\left[\left(\underbrace{\frac{p(x)}{q(x)}h(x)}_{Z}\right)^2\right]\geq \left(\mathbb{E}_q\left[\underbrace{\frac{p(x)}{q(x)}h(x)}_{Z}\right]\right)^2$$
This equality holds if and only if $\frac{p(x)}{q(x)}h(x)=\alpha,\forall x\in X$, $\alpha$ is a constant.

Since $q$ is pdf., we can infer $$q(x)=\frac{p(x)h(x)}{\int_Xp(x)h(x)dx}$$
which is as hard as the original problem. In practice, one is content to find a “good” $q$ that assigns high probability to the important region where $p(x)h(x)$ is large. Ideally the ratio $\frac{p(x)}{q(x)}h(x)$ would be roughly constant over $X$.


\section{Bootstrap (not enough data)}
\underline{Problem:} analyze the performance of an estimator $\hat{\theta}_n(\vec{Y})$, $\vec{Y}=(Y_1,Y_2,...,Y_n)$ taken i.i.d. from distribution $P$. e.g. $P_{\theta}=N(0,1),\hat{\theta}_n=\frac{1}{n}\sum_{i=1}^nY_i$

Assume $\theta$ is a scalar parameter. Performance: (1) Bias $\mathbb{E}_{\theta}[\hat{\theta}_n(\vec{Y})]-\theta$; (2) Variance $\mathbb{E}_{\theta}[\hat{\theta}_n^2(\vec{Y})]-\mathbb{E}_{\theta}^2[\hat{\theta}_n(\vec{Y})]$; (3) CDF $G_{n}(t)=P(\hat{\theta}_n(\vec{Y})<t),\forall t$

\subsubsection*{Approach \#1 Monte-Carlo Simulations}
Generate $k$ vectors $\vec{Y}^{(i)},i=1,2,...,k$ (total $kn$ random variables)
(1) Bias $\frac{1}{k}\sum_{j=1}^k \hat{\theta}_n(\vec{Y}^{(j)})-\theta$; (2) Variance $\frac{1}{k}\sum_{j=1}^k \hat{\theta}^2_n(\vec{Y}^{(j)})-\left(\frac{1}{k}\sum_{j=1}^k \hat{\theta}_n(\vec{Y}^{(j)})\right)^2$; (3) CDF $\hat{G}_{n}(t)=\frac{1}{k}\sum_{j=1}^k \mathbf{1}_{\hat{\theta}_n(\vec{Y}^{(j)})<t},\forall t$

\subsubsection*{Approach \#2 Bootstrap}(When data is not enough)
Suppose we only have one data $\vec{Y}=(Y_1,...,Y_n)$

Reuse $Y_1,...Y_n$ to obtain resamples $\vec{Y}^*=(Y_1^*,...,Y_n^*)$. Do this $k$ times $\Rightarrow$ $k$ resamples ${\vec{Y^*}}^{(1)},...,{\vec{Y^*}}^{(k)}$

(1) Bias $\frac{1}{k}\sum_{j=1}^k \hat{\theta}_n(\vec{Y^*}^{(j)})-\theta$; (2) Variance $\frac{1}{k}\sum_{j=1}^k \hat{\theta}^2_n(\vec{Y^*}^{(j)})-\left(\frac{1}{k}\sum_{j=1}^k \hat{\theta}_n(\vec{Y^*}^{(j)})\right)^2$; (3) CDF $\hat{G}_{n}(t)=\frac{1}{k}\sum_{j=1}^k \mathbf{1}_{\hat{\theta}_n(\vec{Y^*}^{(j)})<t},\forall t$

\textbf{Example:} $\theta=med\{P\}$, $P$ is an unknown distribution over $\{0,1,...,9\}$. $\vec{Y}=(4,8,9,6,2)$.



\subsection{Residual Bootstrap}
The bootstrap principle is quite general and may also be used in problems where the data $Y_i$, $1\leq i\leq n$, \textbf{are not i.i.d}.

\subsubsection*{Example: Linear}
Observation $Y_i=a+b\frac{i}{n}+Z_i$, where $Z_i\sim N(0,\sigma^2)$ (i.i.d.) for $i=1,2,...,n$

Parameter $\theta=(a,b)$. Linear Least Square Estimator:
$$(\hat{a}_n,\hat{b}_n)=\argmin_{(a,b)}\sum_{i=1}^n(Y_i-a-b\frac{i}{n})^2$$
Given $\vec{Y}$, the residual (not i.i.d.)$$E_i=Y_i=\hat{a}_n-\hat{b}_n\frac{i}{n}\approx Z_i$$
Generate $k$ resamples of $\vec{E}=(E_1,E_2,...,E_n)$\\
$\Rightarrow$ obtain $\vec{E^*}^{(1)},\vec{E^*}^{(2)},...,\vec{E^*}^{(k)}$ by resampling\\
$\Rightarrow$ Compute pseudo-data ${Y_i^*}^{(j)}=\hat{a}_n+\hat{b}_n\frac{i}{n}+{E_i^*}^{(j)}$\\
$\Rightarrow$ Compute LS estimator $$\hat{\theta}_n^{(j)}=(\hat{a}_n^{(j)},\hat{b}_n^{(j)})=\argmin_{(a,b)}\sum_{i=1}^n({Y_i^*}^{(j)}-a-b\frac{i}{n})^2$$
$\Rightarrow$ Evaluate bias $$\widehat{Bias}=\frac{1}{k}\sum_{j=1}^k \hat{\theta}_n^{(j)}-\theta$$

\subsubsection*{Example: Nonlinear Markov Process}
Observation $Y_i=F_{\theta}(Y_{i-1})+Z_i$, where $Z_i\sim N(0,\sigma^2)$ (i.i.d.) for $i=1,2,...,n$

Parameter $\theta=(a,b)$. Linear Least Square Estimator:
$$\hat{\theta}_n(\vec{Y})=\argmin_{\theta}\sum_{i=1}^n(Y_i-F_{\theta}(Y_{i-1}))^2$$
Given $\vec{Y}$, the residual (not i.i.d.)$$E_i=Y_i=\hat{a}_n-F_{\hat{\theta}_n}(Y_{i-1})\approx Z_i$$
Generate $k$ resamples of $\vec{E}=(E_1,E_2,...,E_n)$\\
$\Rightarrow$ obtain $\vec{E^*}^{(1)},\vec{E^*}^{(2)},...,\vec{E^*}^{(k)}$ by resampling\\
$\Rightarrow$ Fix ${Y_0^*}^{(j)}=Y_0$, compute pseudo-data ${Y_i^*}^{(j)}=F_{\hat{\theta}_n}({Y^*_{i-1}}^{(j)})+{E_i^*}^{(j)}$\\
$\Rightarrow$ Compute LS estimator $$\hat{\theta}_n^{(j)}=\argmin_{(a,b)}\sum_{i=1}^n({Y_i^*}^{(j)}-F_{\hat{\theta}_n}({Y^*_{i-1}}^{(j)}))^2$$
$\Rightarrow$ Evaluate bias $$\widehat{Bias}=\frac{1}{k}\sum_{j=1}^k \hat{\theta}_n^{(j)}-\theta$$


\section{Particle Filtering}
Kalman filtering is used in tracking problems (dynamic models). Particle Filtering is an extension of Kalman filtering.
\subsection{Kalman Filtering (Linear Dynamic System)}
\begin{enumerate}
    \item Unknown state sequence $X_t\in \mathbb{R}^m,t=0,1,2,...$
    \item Observations $Y_t\in \mathbb{R}^k,t=0,1,2,...$
    \item $X_{t+1}=F_tX_t+U_t$, $F_t\in \mathbb{R}^{m\times m}, U_t\sim P_{U_t}$
    \item $Y_t=H_tX_t+V_t$, $H_t\in \mathbb{R}^{k\times m}, V_t\sim P_{V_t}$
\end{enumerate}

We want to solve two problems
\begin{enumerate}
    \item \textbf{Estimation Problem:} Evaluate Linear MMSE (LMMSE) of $X_t$ given $Y_{0:t}$. $$\hat{X}_{t|t}=WY_{0:t}+b$$
    \item \textbf{Prediction Problem:} Predict Linear MMSE (LMMSE) of $X_{t+1|t}$ given $Y_{0:t}$. (Really hard)
\end{enumerate}
We can solve closed-form solutions.

\subsection{Particle Filtering (Nonlinear Dynamic System)}
Particle filtering is a nonlinear form of Kalman filtering, which doesn't have closed-from solutions.

We consider a \underline{Nonlinear Dynamic System}
\begin{equation}
    \begin{aligned}
        X_{t+1}\sim q(\cdot|X_t)\\
        Y_{t}\sim r(\cdot|X_t)\\t=0,1,2,...
    \end{aligned}
    \nonumber
\end{equation}
where $q(X_{t+1}|X_t)$ is the transition probability distribution, and $r(Y_t|X_t)$ is the conditional probability distribution for the observations. Hence, $X_t$ is a Markov process and $Y_t$ follows a Hidden Markov Model (HMM).
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{HMM.png}
    \caption{Hidden Markov Model}
    \label{}
\end{figure}\end{center}
We also consider these two probelms.
\begin{enumerate}
    \item \textbf{Estimation Problem:} Evaluate $X_t$ given $Y_{0:t}$.
    \item \textbf{Prediction Problem:} Predict $X_{t+1|t}$ given $Y_{0:t}$.
\end{enumerate}

\subsubsection{Bayesian Recursive Filtering}
In this section we use Bayesian approach and use MMSE estimation $l(\hat{x}_t,x_t)=\|x_t-\hat{x}_t\|^2$

Estimation and prediction in conditional forms are:
\begin{equation}
    \begin{aligned}
        \hat{X}_{t|t}&=\mathbb{E}[X_{t}|Y_{0:t}]=\int_{\mathbb{R}^m}x_{t}P(X_{t}|Y_{0:t})dx_{t}\\
        \hat{X}_{t+1|t}&=\mathbb{E}[X_{t+1}|Y_{0:t}]=\int_{\mathbb{R}^m}x_{t+1}P(X_{t+1}|Y_{0:t})dx_{t+1}
    \end{aligned}
    \nonumber
\end{equation}

Apparently the posterior p.d.f cannot be evaluated due to the curse of dimensionality as $t$ increases. However, they can in principle be evaluated \textit{recursively} using the following two-step procedure.

\begin{enumerate}[\textbf{Step}]
    \item \textbf{ 1: Prediction.} $P(X_{t+1}|Y_{0:t})$ can be expressed in term of $P(X_t|Y_{0:t})$:
    \begin{equation}
        \begin{aligned}
            P(X_{t+1}|Y_{0:t})&=\int_{\mathbb{R}^m}P(X_{t+1},X_t|Y_{0:t})dx_t\\
            &=\int_{\mathbb{R}^m}P(X_{t+1}|X_t, Y_{0:t})P(X_t|Y_{0:t})dx_t\\
            &=\int_{\mathbb{R}^m}q(X_{t+1}|X_t)P(X_t|Y_{0:t})dx_t\\
        \end{aligned}
        \nonumber
    \end{equation}
    \item \textbf{ 2: Update.} We can also express $P(X_t|Y_{0:t})$ in terms of $P(X_t|Y_{0:t-1})$
    \begin{equation}
        \begin{aligned}
            P(X_{t}|Y_{0:t})&=P(X_t|Y_{t},Y_{0:t-1})\\
            &=\frac{P(Y_t|X_t,Y_{0:t-1})P(X_t|Y_{0:t-1})}{P(Y_t|Y_{0:t-1})}\\
            &=\frac{r(Y_t|X_t)P(X_t|Y_{0:t-1})}{\int_{\mathbb{R}^m}r(Y_t|X_t)P(X_t|Y_{0:t-1})dx_t}
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}

\subsubsection{Particle Filter (bootstrap filter)}
Suppose we have $n$ i.i.d. samples of $X_t$ drawn from $p(x_t|Y_{0:t})$: $X_t(1),X_t(2),...,X_t(n)$.
\begin{equation}
    \begin{aligned}
        X_{t}(i)\sim p(\cdot|Y_{0:t}),1\leq i\leq n
    \end{aligned}
    \tag{\text{Sample 1}}
\end{equation}

We can use above recursive filtering method to generate estimation of $X_{t+1}$.
\begin{enumerate}[\textbf{Step}]
    \item \textbf{ 1: Prediction.} Using the transition probability $q(\cdot|X_t(i)),1\leq i\leq n$ to generate $n$ independent random variables
    \begin{equation}
        \begin{aligned}
            X^*_{t+1}(i)\sim q(\cdot|X_t(i)),\ 1\leq i\leq n
        \end{aligned}
        \tag{\text{Sample 2}}
    \end{equation}
    \item \textbf{ 2: Update.} Upon \textbf{receiving a new measurement $y_{t+1}$}, evaluate the \textit{importance weights} (nonnegative and summing to $1$)
    \begin{equation}
        \begin{aligned}
            w_i=\frac{r(y_{t+1}|X_{t+1}^*(i))}{\sum_{j=1}^n r(y_{t+1}|X_{t+1}^*(j))},\ 1\leq i\leq n
        \end{aligned}
        \nonumber
    \end{equation}
    Then we resample $n$ times from the set $\{X_{t+1}^*(i)\}_{i=1}^n$ with respective probabilities $\{w_i\}_{i=1}^n$, obtaining i.i.d samples $\{X_{t+1}(j)\}_{j=1}^n$ with probabilities
    \begin{equation}
        \begin{aligned}
            Pr[X_{t+1}(j)=X_{t+1}^*(i)]=w_i,\ 1\leq i,j\leq n
        \end{aligned}
        \tag{\text{Sample 3}}
    \end{equation}
\end{enumerate}
By the \textbf{weighted bootstrap theorem}, as $n \rightarrow \infty$, the distribution of the
resampled $\{X_{t+1}(j)\}_{j=1}^n$ converges to the desired posterior.


Potential issues: 1. $n$ is not large enough. 2. Sample impoverishment





\section{EM Algorithm}

The ML estimator: $\hat{\theta}_{ML}=\argmax_{\theta\in S}\ln p_\theta(y)$. Numerical evaluation of maximum-likelihood (ML) estimates is often difficult. The likelihood function may have multiple extreme and the parameter $\theta$ may be multidimensional, all of which are problematic for any numerical algorithm.

\subsubsection*{Maximum-Likelihood (ML) Estimation}
Given a vector $\vec{y}$, find the $\theta$ that maximizes $p_\theta(\vec{y})=\prod_{i=1}^n P(y_i|\theta)$
$$\hat{\theta}_{ML}=\argmax_{\theta\in S}\ln p_\theta(\vec{y})=\argmax_{\theta\in S}\sum_{i=1}^n \ln P(y_i|\theta)$$
Solving the closed-form solution is quite hard sometimes, so we may use EM algorithm.

\subsection{General Structure of the EM Algorithm}

\subsubsection*{What we want to estimate:}
$\theta\in S$ is an unknown parameter that we want to estimate.
\subsubsection*{What we know:}
To help us solve the solution, we construct an unobservable vector $\vec{z}$ corresponding to $\vec{y}$.
\begin{enumerate}
    \item There is a complete data space $Z$ and an incomplete data space $Y$.
    \item The reality is $z\in Z$, which has p.d.f $P(z|\theta)$. ($\ln P(z|\theta)$'s derivative should be constructed to be easy.)
    \item \textbf{Instead of observing the $z$ directly, we can observe $y=h(z)\in Y$ which has p.d.f $P(y|\theta)$.}
    \item $h(z)=y$ is a many-to-one mapping. $$P(y,z|\theta)=P(z|\theta),\quad \forall z\in h^{-1}(y)$$
    \item We can infer that the relationship between $P(z|\theta)$ and $P(y|\theta)$ is
    \begin{equation}
        \begin{aligned}
            P(z|\theta)&=P(z|y,\theta)P(y|\theta),\quad \forall z\in h^{-1}(y)\\
            P(y|\theta)&=\sum_{z\in h^{-1}(y)}P(z|\theta),\quad \forall y
        \end{aligned}
        \nonumber
    \end{equation}
    \item For any function $f$, $\mathbb{E}_z[f(z)|y]$ depends on the p.d.f.
    \begin{equation}
        \begin{aligned}
            \mathbb{E}_{z|\theta}[f(z)|y]=\sum_{z\in h^{-1}(y)}P(z|y,\theta)f(z)
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{EM1.png}
    \caption{Complete and incomplete data spaces $Z$ and $Y$.}
    \label{}
\end{figure}\end{center}

%The incomplete-data and complete-data loglikelihood functions are respectively denoted by
%\begin{equation}
%    \begin{aligned}
        %l_{id}(\theta)\triangleq \ln p_\theta(y);\ l_{cd}(\theta)\triangleq \ln q_\theta(z)
%    \end{aligned}
%    \nonumber
%\end{equation}
\subsubsection*{EM Algorithm}
Instead of computing the $P(\vec{y}|\theta)$ directly, we use the relationship $h(z)=y$ and $P(\vec{z}|\theta)$ to estimate $\theta$.

Suppose we have a prior belief of the relationship between $y$ and $z$: $P(z|y,\theta^{(k)})$. Given $\vec{y}$, since maximizing $\ln P(\vec{y}|\theta)$ is hard, we maximize the expected value of $\ln P(\vec{z}|\theta)|\vec{y}$ under the prior belief (i.e., finding the $\theta$ that can properly represent the relationship between $\vec{y}$ and $\vec{z}$), that is
\begin{equation}
    \begin{aligned}
        \theta&=\argmax_\theta \mathbb{E}_{z|y,\theta^{(k)}} [\ln P(\vec{z}|\theta)|\vec{y}]\\
        &=\argmax_\theta\sum_{i=1}^n \sum_{z_i\in h^{-1}(y_i)}P(z_i|y_i,\theta^{(k)})\ln P(z_i|\theta)
    \end{aligned}
    \nonumber
\end{equation}

EM algorithm alternates between Expectation (E) and Maximization (M) steps:
\begin{enumerate}
    \item Initialize $\hat{\theta}^{(0)}$
    \item For $k=0,1,2,...$\\
    \textbf{Expectation (E)-Step:}
        Compute
        \begin{equation}
            \begin{aligned}
                Q(\theta|\hat{\theta}^{(k)})
                &=\mathbb{E}_{z|y,\hat{\theta}^{(k)}}[\ln P(\vec{z}|\theta)|\vec{y}]\\
                &=\sum_{i=1}^n \sum_{z_i\in h^{-1}(y_i)}P(z_i|y_i,\theta^{(k)})\ln P(z_i|\theta)
            \end{aligned}
            \nonumber
        \end{equation}
    \textbf{Maximization (M)-Step} $$\hat{\theta}^{(k+1)}=\argmax_{\theta\in S}Q(\theta|\hat{\theta}^{(k)})$$
\end{enumerate}

\begin{definition}
    $\theta^*$ is a stable point of the EM algorithm if $\exists$ subsequence that converges to $\theta^*$.

    e.g. $1,3,\frac{1}{2},3,\frac{1}{3},3,...\frac{1}{n},3,...$
\end{definition}

\subsection{Example 1: Variance Estimation}
Observation $Y=S+N$, $S\sim \mathcal{N}(0,\theta)$ is independent of $N\sim \mathcal{N}(0,\theta)$ $\Rightarrow$ $Y\sim \mathcal{N}(0,\theta+1)$. $p_\theta(y)=\frac{1}{\sqrt{2\pi(\theta+1)}}e^{-\frac{y^2}{2(\theta+1)}}$. We want to estimate $\theta$.
\subsubsection{Maximum-Likelihood (ML) Estimation}
\begin{equation}
    \begin{aligned}
        \ln p_\theta(y)=-\frac{1}{2}\ln (2\pi)-\frac{1}{2}\ln(\theta+1)-\frac{y^2}{2(\theta+1)}
    \end{aligned}
    \nonumber
\end{equation}
take derivation of $\theta$ to be equal to $0$
\begin{equation}
    \begin{aligned}
        -\frac{1}{2(\theta+1)}+\frac{y^2}{2(\theta+1)^2}=0
    \end{aligned}
    \nonumber
\end{equation}
We can get $$\hat{\theta}=y^2-1$$
Then, $$\hat{\theta}_{ML}=\left\{\begin{matrix}
    0,&y^2\leq 1\\
    y^2-1,&y^2>1
\end{matrix}\right.$$

\subsubsection{EM Algorithm}
Let $Z=(S,N)$, $y=h(z)=s+n$.
\begin{equation}
    \begin{aligned}
        q_\theta(z)=q_\theta(s,n)=\frac{1}{\sqrt{2\pi\theta}}e^{-\frac{s^2}{2\theta}}\frac{1}{\sqrt{2\pi}}e^{-\frac{n^2}{2}}
    \end{aligned}
    \nonumber
\end{equation}
Then $$\ln q_\theta(z)=\ln \frac{1}{\sqrt{2\pi}}e^{-\frac{n^2}{2}}-\frac{1}{2}\ln (2\pi)-\frac{1}{2}\ln(\theta)-\frac{s^2}{2\theta}$$
\textbf{E-Step:}
        Compute
        \begin{equation}
            \begin{aligned}
                Q(\theta|\hat{\theta}^{(k)})
                &=\mathbb{E}_{z|\hat{\theta}^{(k)}}[\ln q_\theta(z)|Y=y]\\
                &=\sum_{z\in h^{-1}(y)}q_{\hat{\theta}^{(k)}}(z)\ln q_\theta(z)\\
                &=\ln \frac{1}{\sqrt{2\pi}}e^{-\frac{n^2}{2}}-\frac{1}{2}\ln (2\pi)-\frac{1}{2}\ln(\theta)-\frac{\mathbb{E}_{z|\hat{\theta}^{(k)}}(s^2)}{2\theta}
            \end{aligned}
            \nonumber
        \end{equation}
\textbf{M-Step}
\begin{equation}
    \begin{aligned}
        \hat{\theta}^{(k+1)}&=\argmax_{\theta\in S}Q(\theta|\hat{\theta}^{(k)})\\
        0&=-\frac{1}{2\hat{\theta}^{(k+1)}}+\frac{\mathbb{E}_{z|\hat{\theta}^{(k)}}(s^2)}{2(\hat{\theta}^{(k+1)})^2}\\
        \hat{\theta}^{(k+1)}&=\mathbb{E}_{z|\hat{\theta}^{(k)}}(s^2)=\frac{\hat{\theta}^{(k)}}{\hat{\theta}^{(k)}+1}\left(\frac{\hat{\theta}^{(k)}}{\hat{\theta}^{(k)}+1}y^2+1\right)
    \end{aligned}
    \nonumber
\end{equation}
Then we can solve the stable point
\begin{equation}
    \begin{aligned}
        \hat{\theta}^{*}=\frac{\hat{\theta}^{*}}{\hat{\theta}^{*}+1}\left(\frac{\hat{\theta}^{*}}{\hat{\theta}^{*}+1}y^2+1\right)\\
        \Rightarrow \hat{\theta}^{*}=0, \hat{\theta}^{*}=y^2-1
    \end{aligned}
    \nonumber
\end{equation}
According to the relation between $\hat{\theta}^{(k)}$ and $\hat{\theta}^{(k+1)}$, we can infer
$$\hat{\theta}^*=\left\{\begin{matrix}
    0,&y^2\leq 1\\
    y^2-1,&y^2>1
\end{matrix}\right.$$







\subsection{Example 2: Estimation of Gaussian Mixtures}
Assume the data $\boldsymbol{Y}=\left\{Y_i, 1 \leq i \leq n\right\} \in \mathbb{R}^n$, are drawn iid from a pdf $p_\theta(y)$ which is the mixture of $m$ univariate Gaussians with respective probabilities $\pi(j)$, means $\mu_j$, and variances $\sigma_j^2$, for $1 \leq j \leq m$ :
$$p_\theta(y|j)=\phi\left(y ; \mu_j, \sigma_j^2\right)$$
$$
p_\theta(y)=\sum_{j=1}^m \pi(j) \phi\left(y ; \mu_j, \sigma_j^2\right), \quad y \in \mathbb{R}
$$
where
$$\phi\left(y ; \mu, \sigma^2\right) \triangleq \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left\{-\frac{(y-\mu)^2}{2 \sigma^2}\right\}$$
denotes the Gaussian pdf with mean $\mu$ and variance $\sigma^2$.
\subsubsection{Unknown Means: ML estimation is hard}
We initially assume that $\{\pi(j)\}$ and $\left\{\sigma_j^2\right\}$ are given and that we only need to estimate the means $\left\{\mu_j\right\}$. Thus, $\theta=\mu \in \mathbb{R}^m$.

Unfortunately the ML estimator cannot be derived in closed form. Indeed, the loglikelihood function for $\theta$ is
$$
\ln \prod_{i=1}^n p_\theta(y_i)=\sum_{i=1}^n \ln p_\theta\left(y_i\right)=\sum_{i=1}^n \ln \sum_{j=1}^m \pi(j) \phi\left(y_i ; \mu_j, \sigma_j^2\right)
$$
and maximizing it is a $m$-dimensional, nonconcave maximization problem.

Taking the derivative of $\mu_j, j=1,...,m$,
\begin{equation}
    \begin{aligned}
        0&=\frac{1}{\sigma^2_j}\sum_{i=1}^n(y_i-\mu_j)\frac{\pi(j)\phi(y_i ; \mu_j, \sigma_j^2)}{\sum_{j=1}^m\pi(j)\phi(y_i ; \mu_j, \sigma_j^2)}\\
        &=\frac{1}{\sigma^2_j}\sum_{i=1}^n(y_i-\mu_j)\pi_\theta(j|y_i)
    \end{aligned}
    \nonumber
\end{equation}
where $\pi_\theta(j|y_i)\triangleq \frac{\pi(j)\phi(y_i ; \mu_j, \sigma_j^2)}{\sum_{j=1}^m\pi(j)\phi(y_i ; \mu_j, \sigma_j^2)}$.
The system may have multiple solutions corresponding to local maxima or even local minima or saddle points of the likelihood function.

\subsubsection{Unknown Means: EM Algorithm}

There is a complete data $Z_i=(J_i,Y_i),=1,...,n$, where $J_i$ is the random label that was drawn to produce $Y_i$. $z=\{j_i,y_i\}_{i=1}^n$ is the sample.
\begin{equation}
    \begin{aligned}
        q_\theta(z)&=\prod_{i=1}^n\left(\pi(j_i)p_\theta(y_i|j_i)\right)
    \end{aligned}
    \nonumber
\end{equation}
$$\ln q_{\theta}(z)=\sum_{i=1}^n[\ln \pi(j_i)+\ln p_{\theta}(y_i|j_i)]$$

Initialize $\hat{\theta}^{(0)}$

Iteration:
\begin{equation}
    \begin{aligned}
        Q(\theta|\hat{\theta}^{(k)})
        &=\sum_{i=1}^n\mathbb{E}_{\hat{\theta}^{(k)}}[\ln \pi(j_i)+\ln p_{\theta}(y_i|j_i)|Y_i=y_i]\\
        &=\sum_{i=1}^n\sum_{j=1}^m\pi_{\hat{\theta}^{(k)}}(j|y_i)[\ln \pi(j)+\ln p_{\theta}(y_i|j)]\\
        &=cst-\sum_{i=1}^n\sum_{j=1}^m\pi_{\hat{\theta}^{(k)}}(j|y_i)\frac{(y_i-\mu_j)^2}{2\sigma_j^2}\\
        &=cst-\sum_{i=1}^n\sum_{j=1}^m\frac{\pi(j)\phi(y_i ; \hat{\mu}_j^{(k)}, \sigma_j^2)}{\sum_{j=1}^m\pi(j)\phi(y_i ; \hat{\mu}_j^{(k)}, \sigma_j^2)}\frac{(y_i-\mu_j)^2}{2\sigma_j^2}
    \end{aligned}
    \nonumber
\end{equation}
where $\ln p_{\theta}(y_i|j)=-\frac{1}{2}\ln(2\pi\sigma_j^2)-\frac{(y_i-\mu_j)^2}{2\sigma_j^2}$.

Take derivative of $\mu_j$,
\begin{equation}
    \begin{aligned}
        0&=\frac{\partial Q(\theta|\hat{\theta}^{(k)})}{\partial \mu_j}=\sum_{i=1}^n\pi_{\hat{\theta}^{(k)}}(j|y_i)\frac{(y_i-\mu_j)}{\sigma_j^2}\\
        \hat{\mu}_j^{(k+1)}&=\frac{\sum_{i=1}^n\pi_{\hat{\theta}^{(k)}}(j|y_i)y_i}{\sum_{i=1}^n\pi_{\hat{\theta}^{(k)}}(j|y_i)}
    \end{aligned}
    \nonumber
\end{equation}

Recall the $\hat{\theta}_{ML}$
\begin{equation}
    \begin{aligned}
        \hat{\theta}_{ML,j}=\frac{\sum_{i=1}^n\pi_{\hat{\theta}_{ML}}(j|y_i)y_i}{\sum_{i=1}^n\pi_{\hat{\theta}_{ML}}(j|y_i)}
    \end{aligned}
    \nonumber
\end{equation}
$\hat{\theta}_{ML}$ is the stable point. (if exist)

\subsubsection{Unknown Mixture Probabilities, Means and Variances}
\textbf{ML Estimation:}

If $\theta \triangleq\left\{\pi(j), \mu_j, \sigma_j^2, 1 \leq j \leq m\right\}$ is unknown, the ML estimator $\hat{\theta}_{\mathrm{ML}}$ satisfies the following nonlinear system of equations:
$$
\begin{aligned}
\hat{\mu}_{\mathrm{ML}, j} &=\frac{\sum_{i=1}^n y_i \pi_{\hat{\theta}^{(k)}}\left(j \mid y_i\right)}{\sum_{i=1}^n \pi_{\hat{\theta}^{(k)}}\left(j \mid y_i\right)} \\
\hat{\sigma}_{\mathrm{ML}, j}^2 &=\frac{\sum_{i=1}^n\left(y_i-\hat{\mu}_{\mathrm{ML}, j}\right)^2 \pi_{\hat{\theta}^{(k)}}\left(j \mid y_i\right)}{\sum_{i=1}^n \pi_{\hat{\theta}(k)}\left(j \mid y_i\right)} \\
\hat{\pi}_{\mathrm{ML}}(j) &=\frac{1}{n} \sum_{i=1}^n \pi_{\hat{\theta}^{(k)}}\left(j \mid y_i\right) \quad 1 \leq j \leq m
\end{aligned}
$$
where
$$
\pi_\theta\left(j \mid y_i\right)=\frac{\pi(j) \phi\left(y_i ; \mu_j, \sigma_j^2\right)}{\sum_{j=1}^m \pi(j) \phi\left(y_i ; \mu_j, \sigma_j^2\right)}, \quad 1 \leq j \leq m
$$
\textbf{E-step:}
\begin{equation}
    \begin{aligned}
        Q(\theta|\hat{\theta}^{(k)})
        &=cst-\sum_{i=1}^n\sum_{j=1}^m\pi_{\hat{\theta}^{(k)}}(j|y_i)\frac{(y_i-\mu_j)^2}{2\sigma_j^2}\\
        &=cst-\sum_{i=1}^n\sum_{j=1}^m\frac{\hat{\pi}^{(k)}(j)\phi(y_i ; \hat{\mu}_j^{(k)}, \hat{\sigma_j^2}^{(k)})}{\sum_{j=1}^m\hat{\pi}^{(k)}(j)\phi(y_i ; \hat{\mu}_j^{(k)}, \hat{\sigma_j^2}^{(k)})}\frac{(y_i-\mu_j)^2}{2\sigma_j^2}
    \end{aligned}
    \nonumber
\end{equation}
\textbf{M-Step:}
\begin{equation}
    \begin{aligned}
        \hat{\mu}_j^{(k+1)} &=\frac{\sum_{i=1}^n y_i \pi_{\hat{\theta}^{(k)}}\left(j \mid y_i\right)}{\sum_{i=1}^n \pi_{\hat{\theta}(k)}\left(j \mid y_i\right)} \\
        \left(\hat{\sigma}_j^2\right)^{(k+1)} &=\frac{\sum_{i=1}^n\left(y_i-\hat{\mu}_j^{(k+1)}\right)^2 \pi_{\hat{\theta}^{(k)}}\left(j \mid y_i\right)}{\sum_{i=1}^n \pi_{\hat{\theta}^{(k)}}\left(j \mid y_i\right)} \\
        \hat{\pi}^{(k+1)}(j) &=\frac{1}{n} \sum_{i=1}^n \pi_{\hat{\theta}^{(k)}}\left(j \mid y_i\right), \quad 1 \leq j \leq m .
    \end{aligned}
    \nonumber
\end{equation}


\subsection{Convergence of EM Algorithm}
\begin{theorem}
    The likelihood sequence $p_{\hat{\theta}^{(k)}}(y)$, $k = 0, 1, 2, ...$ is nondecreasing.
\end{theorem}
\begin{proof}
    Assume for notational simplicity that the random variables $Y$ and $Z$ are discrete. Hence, their joint distribution is given by
    \begin{equation}
        \begin{aligned}
            P_\theta(y,z)&=q_\theta(z)p_\theta(y|z)=q_\theta(z)\mathbf{1}_{\{y=h(z)\}}\\
            &=p_\theta(y)q_\theta(z|y)
        \end{aligned}
        \nonumber
    \end{equation}
    Given $y$, the following identity holds for all $z \in h^{-1}(y)$:
    \begin{equation}
        \begin{aligned}
            p_\theta(y)&=\frac{q_\theta(z)}{q_\theta(z|y)}
        \end{aligned}
        \nonumber
    \end{equation}
    Taking the logarithm,
    \begin{equation}
        \begin{aligned}
            \ln p_\theta(y)&= \ln q_\theta(z)-\ln q_\theta(z|y),\ \forall z\in h^{-1}(y)
        \end{aligned}
        \nonumber
    \end{equation}
    Taking the conditional expectation with respect to $q_{\hat{\theta}}(z|y)$,
    \begin{equation}
        \begin{aligned}
            \ln p_\theta(y)&= \sum_{z\in h^{-1}(y)}q_{\hat{\theta}}(z|y)\ln q_\theta(z)-\sum_{z\in h^{-1}(y)}q_{\hat{\theta}}(z|y)\ln q_\theta(z|y)
        \end{aligned}
        \tag{1}
    \end{equation}
    \textbf{Expectation (E)-Step:}
        Compute
        \begin{equation}
            \begin{aligned}
                Q(\theta|\hat{\theta}^{(k)})
                &=\sum_{z\in h^{-1}(y)}q_{\hat{\theta}^{(k)}}(z|y)\ln q_\theta(z)
            \end{aligned}
            \nonumber
        \end{equation}
    \textbf{Maximization (M)-Step} $$\hat{\theta}^{(k+1)}=\argmax_{\theta\in S}Q(\theta|\hat{\theta}^{(k)})$$

    According to $(1)$,
    \begin{equation}
        \begin{aligned}
            \ln p_\theta (y)&=Q(\theta|\hat{\theta}^{(k)})-H(q_{\hat{\theta}^{(k)}},q_\theta)\\
            \ln p_{\hat{\theta}^{(k+1)}} (y)-\ln p_{\hat{\theta}^{(k)}} (y)&=(Q(\hat{\theta}^{(k+1)}|\hat{\theta}^{(k)})-Q(\hat{\theta}^{(k)}|\hat{\theta}^{(k)}))-(H(q_{\hat{\theta}^{(k)}},q_{\hat{\theta}^{(k+1)}})-H(q_{\hat{\theta}^{(k)}},q_{\hat{\theta}^{(k)}}))
        \end{aligned}
        \nonumber
    \end{equation}
    Since $\hat{\theta}^{(k+1)}=\argmax_{\theta\in S}Q(\theta|\hat{\theta}^{(k)})$, $Q(\hat{\theta}^{(k+1)}|\hat{\theta}^{(k)})-Q(\hat{\theta}^{(k)}|\hat{\theta}^{(k)})\geq 0$.
    \begin{equation}
        \begin{aligned}
            H(q_{\hat{\theta}^{(k)}},q_{\hat{\theta}^{(k+1)}})-H(q_{\hat{\theta}^{(k)}},q_{\hat{\theta}^{(k)}})=D(q_{\hat{\theta}^{(k)}}\| q_{\hat{\theta}^{(k+1)}})\geq 0
        \end{aligned}
        \nonumber
    \end{equation}
    Hence, we can conclude $\ln p_{\hat{\theta}^{(k+1)}} (y)-\ln p_{\hat{\theta}^{(k)}} (y)\geq 0$. Then $p_{\hat{\theta}^{(k)}}$ should be nondecreasing in $k$.
\end{proof}

\begin{corollary}
    Assume that $S$ is a closed, bounded subset of Euclidean space, the functions $Q(\theta|\theta')$ and $H(\theta|\theta')$ are continuously differentiable, and the loglikelihood function $\ln p_{\hat{\theta}^{(k)}}$ is differentiable and bounded. Then the sequence $\ln p_{\hat{\theta}^{(k)}}$ converges, and any limit point $\theta^*\in \text{interior}(S)$ of the EM sequence is a solution of the likelihood equation $\nabla \ln p_{\theta}=0$.
\end{corollary}


\subsection{EM As an Alternating Maximization Algorithm}
Define an \textit{auxiliary cost function} $L(q,\theta)$.

Incomplete data $Y$; Complete data $Z$. Still $h(z): Z \rightarrow Y$.

$\mathcal{Q}_y=\{q: q(z)=0, \forall z\in h^{-1}(y)\}$

\underline{EM updates}
\begin{enumerate}
    \item E-Step:
\end{enumerate}













\section{Hidden Markov model (HMM)}
A Markov chain ${X_t}_{t\geq 1}$ is observed as $\{Y_t\}_{t\geq 1}$. The state sets are finite sets $S_x, S_y$. Suppose the initial state distribution is $\pi$. The \textit{transition probability matrix} of the MC is
\begin{equation}
    \begin{aligned}
        A(i,j)=P(X_{t+1}=j|X_{t}=i),\ i,j\in S_x
    \end{aligned}
    \nonumber
\end{equation}
and the \textit{emission probability matrix} is
\begin{equation}
    \begin{aligned}
        B(i,j)=P(Y_t=j|X_t=i),\ i\in S_x, j\in S_y
    \end{aligned}
    \nonumber
\end{equation}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.15]{HDM.png}
    \caption{Hidden Markov Model (HMM)}
    \label{}
\end{figure}\end{center}

Relative problems include
\begin{enumerate}[\textbf{Problem} 1:]
    \item Estimate $X_t$ given $Y_{1:t}$ (Using MAP or MMSE criterion: particle filtering)
    \item Estimate $X_{t+1}$ given $Y_{1:t}$ (Using MAP or MMSE prediction: particle filtering)
    \item Estimate $X_{1:t}$ given $Y_{1:t}$ (MAP, MMSE)
    \item Estimate the HMM parameters $\theta=(\pi,A,B)$ given $Y_{1:t}$ (learning)
\end{enumerate}

\subsection{Viterbi Algorithm: (MAP) estimate $X_{1:t}$ given $Y_{1:t}$}
\subsubsection{MAP estimation problem}
The MAP estimation problem arises in a variety of applications, and Viterbi derived a remarkable algorithm for solving it exactly. The probability of state $\vec{x}\in S_x^n$ is given by
\begin{equation}
    \begin{aligned}
        P(\vec{x})=\pi(x_1)\prod_{t=1}^{n-1}A(x_t,x_{t+1})
    \end{aligned}
    \nonumber
\end{equation}
and the conditional probability of the observed sequence $\vec{y}$ given the state sequence $\vec{x}$ is
\begin{equation}
    \begin{aligned}
        P(\vec{y}|\vec{x})=\prod_{t=1}^n B(x_t,y_t)
    \end{aligned}
    \nonumber
\end{equation}
Hence, the joint probability of $\vec{x}$ and $\vec{y}$ is
\begin{equation}
    \begin{aligned}
        P(\vec{x},\vec{y})=P(\vec{x})P(\vec{y}|\vec{x})=\pi(x_1)\prod_{t=1}^{n-1}A(x_t,x_{t+1})\prod_{t=1}^n B(x_t,y_t)
    \end{aligned}
    \nonumber
\end{equation}
Then the MAP estimation problem is
\begin{equation}
    \begin{aligned}
        \vec{x}^*&=\argmax_{\vec{x}}P(\vec{x}|\vec{y})=\argmax_{\vec{x}}\frac{P(\vec{x},\vec{y})}{P(\vec{y})}=\argmax_{\vec{x}}P(\vec{x},\vec{y})\\
        &=\argmax_{\vec{x}}\ \pi(x_1)\prod_{t=1}^{n-1}A(x_t,x_{t+1})\prod_{t=1}^n B(x_t,y_t)\\
        &=\argmax_{\vec{x}}\ \ln\pi(x_1)+\sum_{t=1}^{n-1}\ln A(x_t,x_{t+1})+\sum_{t=1}^n\ln B(x_t,y_t)
    \end{aligned}
    \nonumber
\end{equation}

\subsubsection{Viterbi Algorithm}
Let $f(x_1)=\ln\pi(x_1)+\ln B(x_1,y_1)$, $g_t(x_t,x_{t+1})=\ln A(x_t,x_{t+1})+\ln B(x_{t+1},y_{t+1})$. Then the estimation problem is written in the form
\begin{equation}
    \begin{aligned}
        \vec{x}^*=\argmax_{\vec{x}}\ \left[\varepsilon (\vec{x})=f(x_1)+\sum_{u=1}^{n-1}g_u(x_u,x_{u+1})\right]
    \end{aligned}
    \nonumber
\end{equation}
Let $V(1,x)=f(x)$ and
\begin{equation}
    \begin{aligned}
        V(t,x_t=x)\triangleq \max_{x_1,x_2,...,x_{t-1}}\left[\varepsilon([x_1,...,x_t])=f(x_1)+\sum_{u=1}^{t-1}g_u(x_u,x_{u+1})\right]
    \end{aligned}
    \nonumber
\end{equation}
\begin{equation}
    \begin{aligned}
        V(t,x_t=x)=\max_{x'} \left[V(t-1,x_{t-1}=x')+g_{t-1}(x',x)\right],\ t\geq 2
    \end{aligned}
    \nonumber
\end{equation}
Then, when $t=n$ we have
\begin{equation}
    \begin{aligned}
        \max_{\vec{x}}\varepsilon(\vec{x})=\max_{x} V(n,x_n=x)
    \end{aligned}
    \nonumber
\end{equation}

The complexity of the algorithm is $O(n|S_x|)$ storage and $O(n|S_x|^2)$ computation.

\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{dia.png}
    \caption{(a) Trellis diagram; (b)—(e) evolution of the Viterbi algorithm, showing surviving paths and values $V(t,x)$ at times $t = 2,3,4,5$; (f) optimal path $\vec{x}^* = (0,2,0,2,0)$ and its value $\varepsilon(\vec{x}^*) = 11$.}
    \label{}
\end{figure}\end{center}

\subsection{Bayesian Estimation of a Sequence: Need (MMSE) estimate $X_{1:t}$ given $Y_{1:t}$}
Consider Bayesian estimation under an additive squared-error loss function: $$L(\vec{x},\hat{\vec{x}})=\sum_{t=1}^nL(x_t,\hat{x}_t)=\sum_{t=1}^n\left(x_t-\hat{x}_t\right)^2$$
The Bayesian estimator $\hat{\vec{x}}$ achieves
\begin{equation}
    \begin{aligned}
        \min_{\hat{\vec{x}}}\sum_{\vec{x}\in X^n}L(\vec{x},\hat{\vec{x}})P(\vec{x}|\vec{y})=\sum_{t=1}^n\min_{\hat{\vec{x}}_t}\sum_{x_t\in X}L(x_t,\hat{x}_t)P(x_t|\vec{y})
    \end{aligned}
    \nonumber
\end{equation}
In particular, under squared-error loss, we obtain the conditional mean estimator
\begin{equation}
    \begin{aligned}
        \hat{\vec{x}}_t=\sum_{x_t\in X}x_t P\left(X_t=x|Y=\vec{y}\right),\quad 1\leq t\leq n
    \end{aligned}
    \nonumber
\end{equation}

\subsection{Forward-Backward Algorithm: (MMSE) estimate $X_{1:t+1}$ given $Y_{1:t}$}
\begin{enumerate}
    \item Evaluate $P\left(X_t=x|Y=\vec{y}\right)$ for $t=1,2,...,n$ and $x\in \mathcal{X}$. (Used to (MMSE) estimate $X_{1:t}$ given $Y_{1:t}$)
    \item Evaluate $P\left(X_t=x,X_{t+1}=x'|Y=\vec{y}\right)$ for $t=1,2,...,n$ and $x,x'\in \mathcal{X}$ (Used to learn parameters $\theta=(\pi,A,B)$)
\end{enumerate}
Define the shorthands
$$
\begin{aligned}
\gamma_t(x) & \triangleq \mathrm{P}\left\{X_t=x \mid \boldsymbol{Y}=\boldsymbol{y}\right\} \\
\xi_t\left(x, x^{\prime}\right) & \triangleq \mathrm{P}\left\{X_t=x, X_{t+1}=x^{\prime} \mid \boldsymbol{Y}=\boldsymbol{y}\right\}, \quad x, x^{\prime} \in \mathcal{X}
\end{aligned}
$$
Hence $\gamma_t$ is the first marginal of $\xi_t$. The forward-backward algorithm allows efficient computation of these probabilities.
\subsubsection{$\gamma_t(x) \triangleq \mathrm{P}\left\{X_t=x \mid \boldsymbol{Y}=\boldsymbol{y}\right\}$}
We begin with
$$
\gamma_t(x)=\mathrm{P}\left\{X_t=x \mid \boldsymbol{Y}=\boldsymbol{y}\right\}=\frac{\mathrm{P}\left\{X_t=x, \boldsymbol{Y}=\boldsymbol{y}\right\}}{\sum_{x \in \mathcal{X}} \mathrm{P}\left\{X_t=x, \boldsymbol{Y}=\boldsymbol{y}\right\}}, \quad 1 \leq t \leq n
$$
Write the numerator as a product of two conditional distributions,
$$
\begin{aligned}
\mathrm{P}\left\{\boldsymbol{Y}=\boldsymbol{y}, X_t=x\right\} & \stackrel{(a)}{=} \underbrace{\mathrm{P}\left\{Y_{1: t}=y_{1: t}, X_t=x\right\}}_{\mu_t(x)} \underbrace{\mathrm{P}\left\{Y_{t+1: n}=y_{t+1: n} \mid X_t=x\right\}}_{\nu_t(x)} \\
&=\mu_t(x) \nu_t(x), \quad 1 \leq t<n
\end{aligned}
$$
where (a) follows from the Markov chain $Y_{1: t} \rightarrow X_t \rightarrow Y_{t+1: n}$. For $t=n$, we let $\nu_n(x) \equiv 1$. Combining above two equations we have
$$
\gamma_t(x)=\frac{\mu_t(x) \nu_t(x)}{\sum_{x \in \mathcal{X}} \mu_t(x) \nu_t(x)} .
$$
\begin{enumerate}[(1)]
    \item The first factor in the product of $\mathrm{P}\left\{\boldsymbol{Y}=\boldsymbol{y}, X_t=x\right\}$ is
    $$
    \mu_t(x)=\mathrm{P}\left\{Y_{1: t}=y_{1: t}, X_t=x\right\}, \quad x \in \mathcal{X}, 1 \leq t \leq n,
    $$
    for which we derive a \textbf{forward recursion}. The recursion is initialized with
    $$
    \mu_1(x)=\mathrm{P}\left\{Y_1=y_1, X_1=x\right\}=\pi(x) B\left(x, y_1\right) .
    $$
    For $t \geq 1$ we express $\mu_{t+1}$ in terms of $\mu_t$ as follows:
    $$
    \begin{aligned}
    \mu_{t+1}(x) &=\mathrm{P}\left\{Y_{1: t+1}=y_{1: t+1}, X_{t+1}=x\right\} \\
    & \stackrel{(a)}{=} \mathrm{P}\left\{Y_{1: t}=y_{1: t}, X_{t+1}=x\right\} \mathrm{P}\left\{Y_{t+1}=y_{t+1} \mid X_{t+1}=x\right\} \\
    &=B\left(x, y_{t+1}\right) \sum_{x^{\prime} \in \mathcal{X}} \mathrm{P}\left\{Y_{1: t}=y_{1: t}, X_{t+1}=x, X_t=x^{\prime}\right\} \\
    & \stackrel{(b)}{=} B\left(x, y_{t+1}\right) \sum_{x^{\prime} \in \mathcal{X}} \mathrm{P}\left\{Y_{1: t}=y_{1: t}, X_t=x^{\prime}\right\} \mathrm{P}\left\{X_{t+1}=x \mid X_t=x^{\prime}\right\} \\
    &=B\left(x, y_{t+1}\right) \sum_{x^{\prime} \in \mathcal{X}} \mu_t\left(x^{\prime}\right) \mathrm{A}\left(x^{\prime}, x\right), \quad t=1,2, \cdots, n-1
    \end{aligned}
    $$
    where (a) holds because $Y_{1: t} \rightarrow X_{t+1} \rightarrow Y_{t+1}$ forms a Markov chain, and (b) because $Y_{1: t} \rightarrow X_t \rightarrow X_{t+1}$ forms a Markov chain.
    \item The second factor in the product of $\mathrm{P}\left\{\boldsymbol{Y}=\boldsymbol{y}, X_t=x\right\}$ is
    $$
    \nu_t(x)=\mathrm{P}\left\{Y_{t+1: n}=y_{t+1: n} \mid X_t=x\right\}, \quad x \in \mathcal{X}, 1 \leq t<n .
    $$
    Starting from $\nu_n(x) \equiv 1$, we have the following \textbf{backward recursion}, expressing $\nu_{t-1}$ in terms of $\nu_t$ for $2 \leq t \leq n$ :
    $$
    \begin{aligned}
    \nu_{t-1}(x) &=\mathrm{P}\left\{Y_{t: n}=y_{t: n} \mid X_{t-1}=x\right\} \\
    &=\sum_{x^{\prime} \in \mathcal{X}} \mathrm{P}\left\{Y_{t: n}=y_{t: n}, X_t=x^{\prime} \mid X_{t-1}=x\right\} \\
    & \stackrel{(a)}{=} \sum_{x^{\prime} \in \mathcal{X}} \mathrm{P}\left\{Y_{t: n}=y_{t: n} \mid X_t=x^{\prime}\right\} \mathrm{P}\left\{X_t=x^{\prime} \mid X_{t-1}=x\right\} \\
    & \stackrel{(b)}{=} \sum_{x^{\prime} \in \mathcal{X}} \mathrm{P}\left\{Y_{t+1: n}=y_{t+1: n} \mid X_t=x^{\prime}\right\} \mathrm{P}\left\{Y_t=y_t \mid X_t=x^{\prime}\right\} \mathrm{P}\left\{X_t=x^{\prime} \mid X_{t-1}=x\right\} \\
    &=\sum_{x^{\prime} \in \mathcal{X}} \nu_t\left(x^{\prime}\right) \mathrm{B}\left(x^{\prime}, y_t\right) \mathrm{A}\left(x, x^{\prime}\right), \quad t=n, n-1, \cdots, 2
    \end{aligned}
    $$
    where (a) holds because $X_{t-1} \rightarrow X_{t} \rightarrow Y_{t:n}$ forms a Markov chain, and (b) because $Y_{t+1: n} \rightarrow X_t \rightarrow Y_t$ forms a Markov chain.
\end{enumerate}

\subsubsection{$\xi_t\left(x, x^{\prime}\right) \triangleq \mathrm{P}\left\{X_t=x, X_{t+1}=x^{\prime} \mid \boldsymbol{Y}=\boldsymbol{y}\right\}$}
Next we derive an expression for
$$
\xi_t\left(x, x^{\prime}\right)=\mathrm{P}\left\{X_t=x, X_{t+1}=x^{\prime} \mid \boldsymbol{Y}=\boldsymbol{y}\right\}=\frac{\mathrm{P}\left\{\boldsymbol{Y}=\boldsymbol{y}, X_t=x, X_{t+1}=x^{\prime}\right\}}{\sum_{x, x^{\prime} \in \mathcal{X}} \mathrm{P}\left\{\boldsymbol{Y}=\boldsymbol{y}, X_t=x, X_{t+1}=x^{\prime}\right\}}
$$
We have
\begin{equation}
    \begin{aligned}
        \mathrm{P}&\left\{\boldsymbol{Y}=\boldsymbol{y}, X_t=x, X_{t+1}=x^{\prime}\right\}\\
        &\stackrel{(a)}{=} \mathrm{P}\left\{Y_{1: t+1}=y_{1: t+1}, X_t=x, X_{t+1}=x^{\prime}\right\} \mathrm{P}\left\{Y_{t+2: n}=y_{t+2: n} \mid X_{t+1}=x^{\prime}\right\}\\
        &\stackrel{(b)}{=} \mathrm{P}\left\{Y_{1: t}=y_{1: t}, X_t=x\right\} \mathrm{P}\left\{X_{t+1}=x^{\prime} \mid X_t=x\right\} \mathrm{P}\left\{Y_{t+1}=y_{t+1} \mid X_{t+1}=x^{\prime}\right\} \nu_{t+1}\left(x^{\prime}\right)\\
        &=\mu_t(x) \mathrm{A}\left(x, x^{\prime}\right) \mathrm{B}\left(x^{\prime}, y_{t+1}\right) \nu_{t+1}\left(x^{\prime}\right)
    \end{aligned}
    \nonumber
\end{equation}
where (a) holds because $\left(Y_{1: t+1}, X_t\right) \rightarrow X_{t+1} \rightarrow Y_{t+2: n}$ forms a Markov chain, and (b) because $Y_{1: t} \rightarrow X_t \rightarrow X_{t+1} \rightarrow Y_{t+1}$ forms a Markov chain. Hence
$$
\xi_t\left(x, x^{\prime}\right)=\frac{\mu_t(x) \mathrm{A}\left(x, x^{\prime}\right) \mathrm{B}\left(x^{\prime}, y_{t+1}\right) \nu_{t+1}\left(x^{\prime}\right)}{\sum_{x, x^{\prime} \in \mathcal{X}} \mu_t(x) \mathrm{A}\left(x, x^{\prime}\right) \mathrm{B}\left(x^{\prime}, y_{t+1}\right) \nu_{t+1}\left(x^{\prime}\right)}, \quad 1 \leq t \leq n, x, x^{\prime} \in \mathcal{X}
$$

\subsubsection{Scaling Factors}
Unfortunately the recursions above are numerically unstable for large $n$ because the probabilities $\mu_t(x)$ and $\nu_t(x)$ vanish exponentially with $n$ and are sums of many small terms of different sizes. The following approach is more stable. Define
$$
\begin{aligned}
\alpha_t(x) &=\mathrm{P}\left\{X_t=x \mid Y_{1: t}=y_{1: t}\right\}, \\
\beta_t(x) &=\frac{\mathrm{P}\left\{Y_{t+1: n}=y_{t+1: n} \mid X_t=x\right\}}{\mathrm{P}\left\{Y_{t+1: n}=y_{t+1: n} \mid Y_{1: t}=y_{1: t}\right\}}, \\
c_t &=\mathrm{P}\left\{Y_t=y_t \mid Y_{1: t-1}=y_{1: t-1}\right\}
\end{aligned}
$$
Then
$$
\begin{aligned}
\gamma_t(x) &=\alpha_t(x) \beta_t(x) \\
\xi_t\left(x, x^{\prime}\right) &=c_t \alpha_t(x) \mathrm{B}\left(x, y_t\right) \mathrm{A}\left(x, x^{\prime}\right) \beta_t\left(x^{\prime}\right)
\end{aligned}
$$
A forward recursion can be derived for $\alpha_t$ and $c_t$, and a backward recursion for $\beta_t$.

The time and storage complexity of the algorithm is $O\left(n|\mathcal{X}|^2\right)$.











\section{Graphic Models}
When we want to compute $P(x_1,...,x_d)$, we use chain rule $P(x_1,...,x_d)=P(x_1)\prod_{i=2}^d P(x_i|x_{1:i-1})$. The computation cost is huge when the dimension $d$ is large.

If there exists conditionally independent relations between variables e.g. $x_A \bot x_C | x_B$. The computation cost can be reduced.

In this section we can use graphic models to represent probabilistic relationships between variables when conditionally independent relations exist.
\subsection{Graph Theory}
\begin{enumerate}
    \item A graph $(V,E)$, $V$ is a set of \textit{vertices}, $E\subseteq V\times V$ is a set of ordered pairs of vertices, called \textit{edges}.
    
    An edge $(i,j)\in E$ is \textit{directed} if $(i,j)\notin E$; otherwise the edge is \textit{undirected}. We denote directed and undirected edges by the symbols $i \rightarrow j$ and $i\sim j$, respectively.
    \item \textbf{Directed and Undirected Graphs:} Graphs in which \textit{all} edges are directed (resp. undirected).
    \item \textbf{Subgraph:} a subgraph $(S,E_S)$ of $(V,E)$ is a subset $S\subset G$ with edges that have both endpoints in $S$.
    \item \textbf{Clique:} A set $C$ of vertices in an undirected graph is a \underline{clique} if either $C$ is a singleton, or \textbf{each pair of vertices in $C$ is linked by an edge}.\\
    That is, all vertices in $C$ are neighbors. The clique is \underline{maximal} if there is no larger clique that contains $C$.
    \item \textbf{Parent, Child:} Vertex $i$ is a \underline{parent} of vertex $j$ if $i \rightarrow j$, in which case $j$ is also called a \underline{child} of $i$. We denote by $\pi(j)$ the set of parents of $j$.
    \item \textbf{Path:} A \textit{path} of length $n$ from $i$ to $j$ is a sequence $i = k_0,k_1,...,k_n = j$ of distinct vertices such that ($k_{m-1},k_m) \in E$ for all $m = 1,... ,n$. We designate such a path by $i \rightarrow j$.
    \item \textbf{Connected Graph:} An undirected graph is \underline{connected} if there is a path between any pair of nodes. In general, the connected components of a graph are those subgraphs which are connected.
    \item \textbf{Cycle/Loop:} An $n-$cycle, or loop, is a path of length $n$ $i \rightarrow j$ with $i = j$.
    
    A directed graph without cycle is also called Directed Acyclic Graph (DAG)
    \item \textbf{Tree:} A tree is a connected, undirected graph without cycles; \textbf{it has a unique path between any two vertices}.
    \item \textbf{Rooted Tree:} A rooted tree is the directed acyclic graph obtained from a tree by choosing as vertex as root and directing all edges away from this root. Each vertex of a rooted tree has at most one parent.
    \item \textbf{Forest:} A forest is an undirected graph where all connected components are trees.
\end{enumerate}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.25]{graph.png}
    \caption{(a) Directed and (b) Undirected graph.}
    \label{}
\end{figure}\end{center}

\subsection{Bayesian Networks}
A Bayesian network (or belief network) is a joint probability distribution associated with a \textit{directed acyclic graph} $(V,E)$ whose nodes $X_v, v \in V$ are random variables. The joint distribution is of the form
\begin{equation}
    \begin{aligned}
        p(\vec{x})=\prod_{v\in V}p(x_v|\pi(x_v))
    \end{aligned}
    \nonumber
\end{equation}
$\pi(x_v)$ is the set of parents of vertices.

For instance a Markov chain is a chain-type directed acyclic graph where $V = \{1, 2,..., n\}$, and $\pi(v) = v - 1$ for $v \geq 2$. The pmf for the sequence $\vec{x}$ is obtained from the chain rule
\begin{equation}
    \begin{aligned}
        p(\vec{x})=p(x_1)p(x_2|x_1)\cdots p(x_n|x_{n-1})
    \end{aligned}
    \nonumber
\end{equation}

\subsection{Markov Networks}
\subsubsection{General Form}
We can use undirected graph to represent conditionally independent.
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{BM.png}
    \caption{(a) (b) Two Bayesian networks and (c) a Markov network.}
    \label{}
\end{figure}\end{center}
More generally, if two nodes $X_u$ and $X_v$ in a Markov network are not connected by an edge, then the random variables $X_u$ and $X_v$ are conditionally independent given all the other random variables (denoted by $X_u \perp X_v \mid X_{\mathcal{V} \backslash\{u, v\}}$).

A Markov network is an undirected graph $G = (V,E)$ together with a collection $X = \{X_v, v \in V\}$ of random variables indexed by the nodes of $G$.

Since there is no direction, we use \textbf{clique} to help use represent probabilities. (\textit{Review: \underline{clique} is a set of vertices that each pair of vertices is linked})

We use $\Omega$ let be collection of cliques in the graph and the functions $\psi_C(\cdot)$ be the \textbf{\textit{clique potentials}, or \textit{compatibility functions}}.

The pmf of $X$ takes the form
\begin{equation}
    \begin{aligned}
        p(\vec{x})=\frac{\prod_{C\in \Omega}\psi_C(\vec{x}_C)}{\sum_{\vec{x}}\prod_{C}\psi_C(\vec{x}_C)}=\frac{1}{Z}\prod_{C\in \Omega}\psi_C(\vec{x}_C)
    \end{aligned}
    \nonumber
\end{equation}
where $Z=\sum_{\vec{x}}\prod_{C}\psi_C(\vec{x}_C)$ is a normalization constant.

\textbf{Note:} this is a form of factorization that can represent conditionally independent relationship among variables. $\psi_C(\cdot)$ are undefined functions.x

\subsubsection{Hammersley-Clifford theorem}
\begin{theorem}[Hammersley-Clifford theorem]
    Assume that $p\left(x_1, \ldots, x_n\right)>0$ (positivity condition). Then,
    $$
    p(\vec{x})=\frac{1}{Z} \prod_{\substack{C \in \Omega}} \phi_C\left(\vec{x}_C\right)
    $$
    Thus, the following are equivalent (given the positivity condition):
    \begin{enumerate}
        \item \textbf{Local Markov property:} $p\left(x_i \mid \vec{x} \backslash\left\{x_i\right\}\right)=p\left(x_i \mid \mathcal{N}\left(x_i\right)\right)$, where $\mathcal{N}\left(x_i\right)$ is the neighboring set of $x_i$.
        \item \textbf{Factorization property:} The probability factorizes according
        to the cliques of the graph.
        \item \textbf{Global Markov property:} $p\left(\vec{x}_A \mid \vec{x}_B, \vec{x}_S\right)=p\left(\vec{x}_A \mid \vec{x}_S\right)$
        whenever $\vec{x}_A$ and $\vec{x}_B$ are separated by $\vec{x}_S$ in $G$
    \end{enumerate}
\end{theorem}

\subsubsection{Form of Gibbs distribution (Boltzmann distribution)}
The factorization is not unique.
We let $\psi(\vec{x}_C)=e^{-V_C(\vec{x}_C)}$, where $V_C(\cdot)$ are the so-called potential energy functions. In a pairwise Markov network, $p(\vec{x})$ can be expressed as a product of clique potentials involving either one or two random variables.
\begin{equation}
    \begin{aligned}
        p(\vec{x})=\frac{1}{Z}e^{-\sum_CV_C(x_C)}
    \end{aligned}
    \nonumber
\end{equation}
This probability follows \textbf{Gibbs distribution (Boltzmann distribution)}. This distribution follows exponential families.

\subsection{Conversion of directed graph to undirected graph}
We can use a step known as \textit{moralization}.
Moralization of graph: connect two unmarried parents.

This is the process of “marrying” the parents of each node, i.e., adding an edge connecting any pair of parents if one did not exist. The figure illustrates this process for a node with three parents. In this case the undirected graph consists of a clique of size 4.
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.25]{moralization.png}
    \caption{Graph moralization}
    \label{}
\end{figure}\end{center}

\subsection{ Inference and Learning}
\subsubsection{Inference on Trees}
Consider the tree of the figure, which has 5 nodes and edges $1\sim 2\sim 3$ and $4\sim 3\sim 5$. We have
\begin{equation}
    \begin{aligned}
        p(\vec{x})=\frac{1}{Z}\psi_{12}(x_1,x_2)\psi_{23}(x_2,x_3)\psi_{34}(x_3,x_4)\psi_{35}(x_3,x_5)
    \end{aligned}
    \nonumber
\end{equation}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{inftree.png}
    \caption{Example 1}
    \label{}
\end{figure}\end{center}
\begin{enumerate}
    \item \textbf{Marginal Inference:} As a first example of inference on trees, consider the problem of evaluating the marginal pmf $p(x_5)$. We explore two approaches: the \textbf{direct approach}, which is computationally infeasible for large graphs (the number of items in the sum is $|\mathcal{X}|^4$);
    $$p(x_5)=\sum_{x_1,...,x_4}\frac{1}{Z}\psi_{12}(x_1,x_2)\psi_{23}(x_2,x_3)\psi_{34}(x_3,x_4)\psi_{35}(x_3,x_5)$$
    and the \textbf{sum-product algorithm}, which exploits the graph structure.
    $$
    \begin{aligned}
    p\left(x_5\right) &=\frac{1}{Z} \sum_{x_3} \psi_{35}\left(x_3, x_5\right) \sum_{x_4} \psi_{34}\left(x_3, x_4\right) \sum_{x_2} \psi_{23}\left(x_2, x_3\right) \underbrace{\sum_{x_1} \psi_{12}\left(x_1, x_2\right)}_{m_{1 \rightarrow 2}\left(x_2\right)} \\
    &=\frac{1}{Z} \sum_{x_3} \psi_{35}\left(x_3, x_5\right) \sum_{x_4} \psi_{34}\left(x_3, x_4\right) \underbrace{\sum_{x_2} \psi_{23}\left(x_2, x_3\right) m_{1 \rightarrow 2}\left(x_2\right)}_{m_{2 \rightarrow 3}\left(x_3\right)} \\
    &=\frac{1}{Z} \sum_{x_3} \psi_{35}\left(x_3, x_5\right) m_{2 \rightarrow 3}\left(x_3\right) \underbrace{\sum_{x_4} \psi_{34}\left(x_3, x_4\right)}_{m_{4 \rightarrow 3}\left(x_3\right)} \\
    &=\frac{1}{Z} \underbrace{\sum_{x_3} \psi_{35}\left(x_3, x_5\right) m_{2 \rightarrow 3}\left(x_3\right) m_{4 \rightarrow 3}\left(x_3\right)}_{m_{3 \rightarrow 5}\left(x_5\right)} .
    \end{aligned}
    $$
    In this derivation, nodes $1,2,4,3$ are eliminated in that order. We think of each term $m_{i \rightarrow j}\left(x_j\right)$ as a message conveyed from node $i$ to node $j$, just before elimination of $j$. Computing $m_{i \rightarrow j}\left(x_j\right)$ involves a summation over all possible values of $x_i$. This interpretation will be helpful in more complex problems.
    \begin{center}\begin{figure}[htbp]
        \centering
        \includegraphics[scale=0.2]{belief.png}
        \caption{Belief propagation in a tree}
        \label{}
    \end{figure}\end{center}
    As illustrated in the Figure, a node can send a message to a neighbor once it has received messages from all of its other neighbors. For a general tree, upon choosing an elimination order, we evaluate the following messages in the corresponding order:
    $$
    m_{i \rightarrow j}\left(x_j\right)=\sum_{x_i} \psi_{i j}\left(x_i, x_j\right) \prod_{k \in \mathcal{N}(i) \backslash\{j\}} m_{k \rightarrow i}\left(x_i\right)
    $$
    The marginal probability at any node $i$ is the product of all incoming messages:
    $$
    p\left(x_i\right)=\frac{1}{Z} \prod_{k \in \mathcal{N}(i)} m_{k \rightarrow i}\left(x_i\right) .
    $$
    We can also evaluate the 2D marginal $p(x_2,x_5)$
    $$
    p\left(x_2, x_5\right)=\frac{1}{Z} \sum_{x_3} \psi_{23}\left(x_2, x_3\right) \psi_{35}\left(x_3, x_5\right) \underbrace{\sum_{x_4} \psi_{34}\left(x_3, x_4\right)}_{m_{4 \rightarrow 3}\left(x_3\right)} \underbrace{\sum_{x_1} \psi_{12}\left(x_1, x_2\right)}_{m_{1 \rightarrow 2}\left(x_2\right)} .
    $$
    Finally, a conditional marginal such as $p\left(x_1 \mid x_5\right)$ is obtained as $p\left(x_1, x_5\right) / p\left(x_5\right)$, hence the problem is reduced to evaluating unconditional marginals.

    The computational cost of the algorithm is $O\left(n|\mathcal{X}|^2\right)$ when the $n$ random variables are defined over the same alphabet $\mathcal{X}$.

    \item \textbf{Maximization:} A closely related problem is to find the most likely configuration, possibly by fixing some coordinates. For instance, evaluate
    $$
    M\left(x_5\right)=\max _{x_1, x_2, x_3, x_4} p\left(x_1, x_2, x_3, x_4, x_5\right)
    $$
    for the above Markov network. Direct calculation has exponential complexity. However, the more efficient max-product algorithm has the same structure as the sum-product algorithm:
    \begin{equation}
        \begin{aligned}
            M\left(x_5\right)&=\max _{x_1, x_2, x_3, x_4} \frac{1}{Z} \psi_{12}\left(x_1, x_2\right) \psi_{23}\left(x_2, x_3\right) \psi_{34}\left(x_3, x_4\right) \psi_{35}\left(x_3, x_5\right)\\
            &=\frac{1}{Z} \max _{x_3} \psi_{35}\left(x_3, x_5\right) \max _{x_4} \psi_{34}\left(x_3, x_4\right) \max _{x_2} \psi_{23}\left(x_2, x_3\right) \underbrace{\max _{x_1} \psi_{12}\left(x_1, x_2\right)}_{m_{1 \rightarrow 2}\left(x_2\right)}\\
            &=\frac{1}{Z} \max _{x_3} \psi_{35}\left(x_3, x_5\right) \underbrace{\max _{x_4} \psi_{34}\left(x_3, x_4\right)}_{m_{4 \rightarrow 3}\left(x_3\right)} \underbrace{m_{x_2} \psi_{23}\left(x_2, x_3\right) m_{1 \rightarrow 2}\left(x_2\right)}_{m_{2 \rightarrow 3}\left(x_3\right)}\\
            &=\frac{1}{Z} \underbrace{\max _{x_3} \psi_{35}\left(x_3, x_5\right) m_{4 \rightarrow 3}\left(x_3\right) m_{2 \rightarrow 3}\left(x_3\right)}_{m_{3 \rightarrow 5}\left(x_5\right)}
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}




















\section{Variational Inference, Mean-Field Techniques}
Approximate complicated p.d.f. $p(\vec{x})$ with tractable $q(\vec{x})$, where $q\in Q=$ tractable set of distributions.

Use divergence to measure:
\begin{equation}
    \begin{aligned}
        \min_{q\in Q}\quad D(q\| p)
    \end{aligned}
    \nonumber
\end{equation}
note that $D$ is convex in $q$.

\subsection{Naive Mean-Field Methods}
The \textit{naive mean field method} approximates a distribution by a product distribution.

Assume the $q$ has the form $q(\vec{x})=\prod_{i=1}^nq_i (x_i)$. Assume $x_i\in X=$ finite set.
\begin{equation}
    \begin{aligned}
        D(q\| p)&=\mathbb{E}_q\left[\ln\frac{q(\vec{X})}{p(\vec{X})}\right]\\
        &=\sum_{i=1}^n \mathbb{E}_{q}\left[\ln q_i(X_i)\right]- \mathbb{E}_q\left[\ln p(\vec{X})\right]\\
        &=\sum_{i=1}^n\sum_{x_i\in X}q_i(x_i)\ln q_i(x_i)-\sum_{\vec{x}\in X^n}\left(\prod_{i=1}^nq_i (x_i)\right)\ln p(\vec{x})
    \end{aligned}
    \nonumber
\end{equation}

Solve 
\begin{equation}
    \begin{aligned}
        \min_{\{q_i\}}&\quad D(\prod_{i=1}^nq_i\| p)\\
        s.t.&\sum_{x_i\in X}q_i(x_i)=1,i=1,...,n
    \end{aligned}
    \nonumber
\end{equation}
Using Lagrangian method:
\begin{equation}
    \begin{aligned}
        L(q,\vec{\lambda})&=D(q\| p)+\sum_{i=1}^n\lambda_i\left(\sum_{x\in X}q_i(x_i)-1\right)\\
        0=\frac{\partial L(q,\vec{\lambda})}{\partial q_i(x_i)}&=1+\ln q_i(x_i)-\sum_{\vec{x}':x'_i=x_i}\left(\prod_{j\neq i}^nq_j (x'_j)\right)\ln p(\vec{x}')+\lambda_i
    \end{aligned}
    \nonumber
\end{equation}
Hence, $q_i(x_i)$ should in the form:
\begin{equation}
    \begin{aligned}
        q_i(x_i)&=\frac{1}{e^{1+\lambda_i}}e^{\sum_{\vec{x}':x'_i=x_i}\left(\prod_{j\neq i}^nq_j (x'_j)\right)\ln p(\vec{x}')}\\
        &=\frac{1}{Z_i}exp\left({\mathbb{E}_{\prod_{j\neq i}^nq_j}[\ln p(X_{1:i-1},x_i,X_{i+1:n})]}\right)
    \end{aligned}
    \nonumber
\end{equation}
Iteration Algorithm:
\begin{equation}
    \begin{aligned}
        q_i^{(k+1)}(x_i)=\frac{1}{Z_i}exp\left({\mathbb{E}_{\prod_{j\neq i}^nq_j^{(k)}}[\ln p(X_{1:i-1},x_i,X_{i+1:n})]}\right)
    \end{aligned}
    \nonumber
\end{equation}

\subsubsection{Graphical Models}
Consider $P=$ pairwise Markov model
\begin{equation}
    \begin{aligned}
        p(\vec{x})&=\frac{1}{Z}\Pi_{(i,j)\in E}\psi_{ij}(x_i,x_j)\\
        \ln p(\vec{x})&=-\ln{Z}+\sum_{(i,j)\in E}\ln\psi_{ij}(x_i,x_j)\\
    \end{aligned}
    \nonumber
\end{equation}
\begin{equation}
    \begin{aligned}
        {\mathbb{E}_{\prod_{j\neq i}^nq_j}[\ln p(X_{1:i-1},x_i,X_{i+1:n})]}=\mathbb{E}_{\prod_{j\neq i}^nq_j}[-\ln{Z}+\sum_{(i,j)\in E}\ln\psi_{ij}(x_i,x_j)]
    \end{aligned}
    \nonumber
\end{equation}

\subsubsection{Ising Model}
Consider a 2-D torus $V$ with $|V| = n$ nodes, and $X = \{\pm 1\}$. Each node is connected to its upper, lower, right, and left neighbors. The distribution is of the form
\begin{equation}
    \begin{aligned}
        p(\vec{x})=\frac{1}{Z}exp\left(\beta\sum_{(i,j)\in E}x_ix_j\right)
    \end{aligned}
    \nonumber
\end{equation}
with $\beta \geq 0$. The parameter $\beta$ represents the inverse of a temperature. For $\beta = 0$ the distribution is uniform, hence fully factorized. For large positive values of $\beta$, configurations $\vec{x}$ with strong correlations are favored.

\textbf{2-D Ising Model}
$$\psi_{ij}(x_i,x_j)=e^{\beta x_ix_j}$$
\begin{equation}
    \begin{aligned}
        \Rightarrow q_i(x_i)=\frac{1}{Z_i}\prod_{j\in N(i)}exp\left(-\sum_{x_j=\pm 1}q_j(x_j)\beta x_i x_j\right)
    \end{aligned}
    \nonumber
\end{equation}

Since each $X_i$ is a Bernoulli random variable, $q_i$ can be represented by a single parameter which we choose to be the mean $m_i = q_i(1) - q_i(-1) \in [-1, 1]$. Equivalently,
\begin{equation}
    \begin{aligned}
        q_i(1)=\frac{1+m_i}{2},\ q_i(-1)=\frac{1-m_i}{2}
    \end{aligned}
    \nonumber
\end{equation}
\begin{equation}
    \begin{aligned}
        \Rightarrow q_i(x_i)=\frac{1}{Z_i}\prod_{j\in N(i)}exp\left(-\beta x_i m_j\right)
    \end{aligned}
    \nonumber
\end{equation}
The normalization constant is given by
\begin{equation}
    \begin{aligned}
        Z_i=2{cosh}\left(\beta x_i\sum_{j\in N(i)}m_j\right)
    \end{aligned}
    \nonumber
\end{equation}
Hence, $$m_i=q_i(1)-q_i(-1)={tanh}\left(\beta\sum_{j\in N(i)}m_j\right)$$
\textbf{Convergence}. We show that the algorithm always converges if a uniform initialization is used, i.e., $m_i^{(0)} = m^{(0)}$ for all $i \in V$. Then the (simultaneous) update equation for the means is
\begin{equation}
    \begin{aligned}
        m^{(k+1)}={tanh}\left(4\beta m^{(k)}\right)
    \end{aligned}
    \nonumber
\end{equation}
\begin{enumerate}[(1)]
    \item \textbf{Case I: $\beta < \frac{1}{4}$:} The mapping is a contraction mapping for $\beta<\frac{1}{4}$, and so the fixed point of this mapping is $\lim_{k \rightarrow \infty} m^{(k)} = 0$, for any initialization $m^{(0)}$. Hence, the variational approximation is uniform: $q(\vec{x}) = 2^{-n}$ for all $x \in \{\pm 1\}^V$.
    \item \textbf{Case II: $\beta > \frac{1}{4}$:} In this case, the equation $m=\tanh (4 \beta m)$ has three possible solutions 0 and $\pm m^*$ where $m^*>0$. If the algorithm is initialized with $m^{(0)}=0$, then subsequent iterations do not change this value. If the algorithm is initialized with $m^{(0)}>0$, it converges to $m^*$. Finally, if the algorithm is initialized with $m^{(0)}<0$, it converges to $-m^*$. In the latter two cases (convergence to either $m^*$ or $-m^*$), the variational approximations $q_i$ are nonuniform.
    \item \textbf{Case III: $\beta = \frac{1}{4}$:} phase transition
\end{enumerate}
The case $\beta>\frac{1}{4}$ is related to percolation theory in statistical physics. It may be shown that the distribution $p$ favors configurations featuring large homogeneous regions. The correlation between any two nodes is significant, even for large graphs. This behavior is completely different from the case $\beta<\frac{1}{4}$, where the correlation between distant nodes dies out with distance (similarly to a homogeneous, irreducible Markov chain). The case $\beta=\frac{1}{4}$ is known as a phase transition.

\subsection{Exponential Families of Probability Distributions}
In canonical form: $d-$dimensional exponential families
$$p_\theta(y)=e^{\sum_{k=1}^d\theta_k T_k(y)-A(\theta)}=e^{\theta^T T(y)-A(\theta)}$$
$T_k (\cdot)$ are called sufficient statistics.\\
It can also be written
$$p_\theta(y)=\frac{h(y)}{Z(\theta)} e^{\sum_{k=1}^d\theta_k T_k(y)}$$

\textbf{Example 1:} $P_\theta=N(\theta,1)$, $$p_\theta(y)=\frac{1}{\sqrt{2\pi}}e^{-\frac{(y-\theta)^2}{2}}=\frac{e^{\frac{y^2}{2}}}{\sqrt{2\pi}e^{-\frac{\theta^2}{2}}}e^{-\theta y}$$

\textbf{Example 2:} $P_\theta=N(0,\theta^{-1})$, $\theta$ is the inverse covariance matrix. $$p_\theta(y)=\frac{|\theta|^{\frac{1}{2}}}{\sqrt{2\pi}}e^{-\frac{1}{2}y^T\theta y}$$

\textbf{Example 3:} 2D-Ising Model $$p_\theta(y)=\frac{1}{Z(\theta)}e^{\theta y_iy_j},\theta>0$$
Generalized 2D-Ising Model $$p_\theta(y)=\frac{1}{Z(\theta)}e^{\sum_{i\in V}\theta_i y_i +\sum _{i\sim j}\theta_{ij} y_iy_j}$$

The natural parameter set:
$$\Theta=\{\theta:\int_X e^{\theta^T T(y)}dy<\infty\}$$

The divergence in exponential form
\begin{equation}
    \begin{aligned}
        D(P_\theta\| P_{\theta'})&=\mathbb{E}_\theta \left[\ln \frac{P_\theta (Y)}{P_{\theta'} (Y)}\right]\\
        &=\mathbb{E}_\theta[\theta^T T(Y)-A(\theta)-(\theta')^T T(Y)+A(\theta')]\\
        &=-[A(\theta)-A(\theta')]+(\theta-\theta')^T \mathbb{E}_\theta [T(Y)]
    \end{aligned}
    \nonumber
\end{equation}

\textbf{Cumulant-generating function} (cgf)
\begin{equation}
    \begin{aligned}
        \kappa(u)=\ln \mathbb{E}(e^{u^TT(Y)})=\ln \int e^{u^TT(y)}(e^{\theta^T T(y)-A(\theta)}) dy
    \end{aligned}
    \nonumber
\end{equation}
\begin{equation}
    \begin{aligned}
    \nabla \kappa(0)&=\mathbb{E}_\theta[T(Y)] \\
    \nabla^2 \kappa(0)&=\operatorname{Cov}_\theta[T(Y)]
    \end{aligned}
    \nonumber
\end{equation}

We can compute
\begin{equation}
    \begin{aligned}
        \int e^{\theta^T T(y)-A(\theta)} dy&=1\\
        \int [T(y)-\nabla A(\theta)] e^{\theta^T T(y)-A(\theta)} dy&=0\\
        \int[T(y)-\nabla A(\theta)]p_\theta(y) dy&=0\\
        \mathbb{E}_\theta [T(Y)]&=\nabla A(\theta)
    \end{aligned}
    \nonumber
\end{equation}
Hence,
\begin{equation}
    \begin{aligned}
    \nabla A(\theta) &=\nabla \kappa(0)=\mathbb{E}_\theta[T(X)] \\
    \nabla^2 A(\theta) &=\nabla^2 \kappa(0)=\operatorname{Cov}_\theta[T(X)]
    \end{aligned}
    \nonumber
\end{equation}

Definition. The set of realizable mean parameters $\mathcal{M}$ is the set of $\mu$ that are the expected value of $T(X)$ under some distribution $p$ (not necessarily in the exponential family). Thus
$$
\mathcal{M} \triangleq\left\{\mu \in \mathbb{R}^d: \exists p: \mathbb{E}_p[T(X)]=\mu\right\}
$$
which is a convex set.\\

\textbf{Example 6.} Consider Generalized 2D-Ising Model $$p_\theta(y)=\frac{1}{Z(\theta)}e^{\sum_{i\in V}\theta_i y_i +\sum _{i\sim j}\theta_{ij} y_iy_j}$$
For $y\in\{0,1\}$
\begin{equation}
    \begin{aligned}
        \mu_i&=\mathbb{E}_\theta \left[T_i(Y)\right]=P_\theta (Y_i=1)\\
        \mu_{ij}&=\mathbb{E}_\theta \left[T_i(Y)T_j(Y)\right]=P_\theta (Y_i=Y_j=1)\\
    \end{aligned}
    \nonumber
\end{equation}
\textbf{Example 7.} If $T(x)=x x^{\top} \in \mathbb{R}^{n \times n}$ then $\mu$ is a correlation matrix, and so $\mathcal{M}$ is the set of all $n \times n$ symmetric nonnegative definite matrices.

\subsection{ML Estimation}
Consider $n$ iid samples $X^{(i)}, 1 \leq i \leq n$ drawn from the exponential distribution $p_\theta$. The ML estimator of $\theta$ given these $n$ samples is obtained by solving
$$
\begin{aligned}
\hat{\theta}_{ML}&=\max _\theta \frac{1}{n} \sum_{i=1}^n \ln p_\theta\left(X^{(i)}\right)\\
&=\max _\theta \frac{1}{n} \sum_{i=1}^n\left[\theta^{\top} T\left(X^{(i)}\right)-A(\theta)\right] \\
&=\max _\theta\left[\theta^{\top} \hat{\mu}-A(\theta)\right]
\end{aligned}
$$
where $\hat{\mu}$ is the mean parameter
$$
\hat{\mu} \triangleq \frac{1}{n} \sum_{i=1}^n T\left(X^{(i)}\right)
$$
$A(\theta)$ is a convex function, we can solve optimal solution by solving critical point.
\begin{equation}
    \begin{aligned}
        \nabla A(\hat{\theta}_{ML})=\hat{\mu}
    \end{aligned}
    \nonumber
\end{equation}
The gradient mapping could be hard to invert, however. For instance, for the Ising model example we easily obtain
$$
\hat{\mu}=\frac{1}{n} \sum_{i=1}^n \sum_{j \sim k} X_j^{(i)} X_k^{(i)}
$$
but inverting the gradient mapping is a hard problem. Such is generally the case if $p$ is a distribution over a Markov network with cycles.

\subsection{Maximum Entropy}
Consider a random variable $X$ over a finite set $\mathcal{X}$. Its probability distribution $p$ is unknown, however we are given the expected value $\mu_k=\mathbb{E}_p\left[T_k(X)\right]$ of $d$ statistics $T_k(X), 1 \leq k \leq d$. A classical problem, which originates from statistical physics, is to find $p$ that maximizes the entropy $H(p)=-\sum_x p(x) \ln p(x)$ subject to the $d$ constraints above. Assuming the feasible set is nonvoid, the resulting distribution is called the maximum-entropy (or maxent()) distribution.

Since $H(p)$ is concave, the constraints are linear in $p$, and the probability simplex is a convex set, the maxent problem is concave. Its solution is obtained by introducing $d$ Lagrange multipliers $\lambda_k, 1 \leq k \leq d$ associated with the mean constraints, and a Lagrange multiplier $\lambda_{d+1}$ associated with the constraint $\sum_x p(x)=1$. Ignoring momentarily the nonnegativity constraints, we maximize the Lagrangian
$$
\mathcal{L}(p, \lambda) \triangleq-\sum_{x \in \mathcal{X}} p(x) \ln p(x)+\sum_{k=1}^d \lambda_k\left(\sum_{x \in \mathcal{X}} p(x) T_k(x)-\mu_k\right)+\lambda_{d+1}\left(\sum_{x \in \mathcal{X}} p(x)-1\right)
$$
over $p$, subject to the $d+1$ equality constraints

The first-order optimality conditions are given by
\begin{equation}
    \begin{aligned}
        0=\frac{\partial \mathcal{L}(p, \lambda)}{\partial p(x)}=-\ln p(x)-1+\sum_{k=1}^d\lambda_kT_k(x)+\lambda_{d+1}
    \end{aligned}
    \nonumber
\end{equation}
Hence, $$p(x)=\frac{1}{Z}e^{\sum_{k=1}^d\lambda_kT_k(x)}$$
where $Z=e^{1-\lambda_{d+1}}$

\begin{equation}
    \begin{aligned}
        H(p)=\mathbb{E}_p[\ln p(X)]=-\theta^T \mathbb{E}_p[T(X)]+A(\theta)=-\max_\theta(\theta^T \mu-A(\theta))
    \end{aligned}
    \nonumber
\end{equation}

\textbf{Example.} Let $X=\left(X_1, X_2\right) \in\{0,1\}^2$ and consider maximizing entropy subject to the constraint $\mathbb{E}\left[X_1 X_2\right]=\mu$ where $\mu \in(0,1)$. We obtain $p(x)=\frac{1}{Z} \exp \left\{\lambda x_1 x_2\right\}$. Since $\sum_x p(x)=1$, the normalization constant is obtained as $Z=e^\lambda+3$. We obtain $\lambda$ from the constraint
$$
\mu=\mathbb{E}_P\left[X_1 X_2\right]=\frac{e^\lambda}{e^\lambda+3} \quad \Rightarrow \quad \lambda=\ln \frac{3 \mu}{1-\mu}
$$
The maxent solution takes the form
$$
p(x)= \begin{cases}\mu & :\left(x_1, x_2\right)=(1,1) \\ \frac{1-\mu}{3} & : \text { else }\end{cases}
$$
and has entropy is $H(p)=-\mu \ln \mu-(1-\mu) \ln \frac{1-\mu}{3}$.\\
A similar version of the maxent problem exists for continuous random variables. The entropy function is replaced with the differential entropy functional $h(p) \triangleq-\int p \ln p$, and the maxent solution again takes an exponential form.


\subsection{}
\begin{equation}
    \begin{aligned}
        \min_{q\in Q}\ D(q\| p)&=\min_q \mathbb{E}_q\left[\ln\frac{q(x)}{p_\theta(x)}\right]\\
        &=\min_q \left[A(\theta)-\theta^T \mathbb{E}_q[T(x)]-H(q)\right]\\
        &=A(\theta)-\max_\mu \max_{q: \mathbb{E}_q[T(x)]=\mu}\left[\theta^T \mathbb{E}_q [T(x)]+H(q)\right]\\
        &=A(\theta)-\max_{\mu\in M}\left[\theta^T \mu +\max_{q: \mathbb{E}_q[T(x)]=\mu}H(q)\right]
    \end{aligned}
    \nonumber
\end{equation}
Since $\max_{q: \mathbb{E}_q[T(x)]=\mu}H(q)$ is exactly an entropy maximum problem, we let $A^*(\mu)=\max_{q: \mathbb{E}_q[T(x)]=\mu}H(q)$.

As we showed: (1). $A^*(\mu)=\max_\theta [\theta^T\mu- A(\theta)]$, $A(\theta)=\max_\mu [\theta^T\mu-A^*(\mu)]$
\begin{equation}
    \begin{aligned}
        \min_{q\in Q}\ D(q\| p)&=A(\theta)-\max_{\mu\in M}\left[\theta^T \mu +A^*(\mu)\right]
    \end{aligned}
    \nonumber
\end{equation}


\subsection{Connection between Exponential Families and Graphic Models}

Pairwise Markov network over $G(V,E)$
\begin{equation}
    \begin{aligned}
        p(\vec{x})&=\frac{1}{Z}\left(\prod_{i\in V}\psi_i(x_i)\right)\left(\prod_{(i,j)\in E}\psi_{ij}(x_ix_j)\right)\\
        &=\frac{1}{Z}e^{\sum_{i\in V}\ln \psi_i(x_i)+\sum_{(i,j)\in E}\ln \psi_{ij}(x_ix_j)}\\
        &=\frac{1}{Z}e^{\sum_{i\in V}\sum_{x\in X}\ln \psi_i(x_i) \mathbf{1}_{x=i}+\sum_{(i,j)\in E}\sum_{x,x'\in X}\ln \psi_{ij}(x_ix_j)\mathbf{1}_{x=i,x'=j}}
    \end{aligned}
    \nonumber
\end{equation}
Let $T_{ix}(x)=\mathbf{1}_{x_i=x},T_{ijxx'}(x,x')=\mathbf{1}_{x_i=x,x_i=x'}$, $\theta_i(x)=\ln\psi_i(x), \theta_{ij}(x)=\ln\psi_{ij}(x,x')$. The probability can be transformed into exponential families.

The dimension of this family is $d = |V||X| + |E||X|^2$.

\textbf{Binary Example:} Consider the case $X=\{0,1\}$ binary pairwise markov network.

$|V|=2$,
\begin{equation}
    \begin{aligned}
        p(x_1,x_2)&=\frac{1}{Z}\psi_1(x_1)\psi_2(x_2)\psi_{12}(x_1,x_2)\\
        &=\frac{1}{Z}e^{\ln \psi_1(x_1)+\ln \psi_2(x_2) +\theta (x_1\land x_2)}
    \end{aligned}
    \nonumber
\end{equation}
In this form, if $\theta>0$ $\Rightarrow$ $12$ is attractive edge; if $\theta<0$ $\Rightarrow$ $12$ is repulsive edge.

$|V|=3$,
\begin{equation}
    \begin{aligned}
        p(x_1,x_2,x_3)&=\frac{1}{Z}\psi_1(x_1)\psi_2(x_2)\psi_3(x_3)\psi_{12}(x_1,x_2)\psi_{23}(x_2,x_3)\psi_{13}(x_1,x_3)\\
        &=\frac{1}{Z}\psi_1(x_1)\psi_2(x_2)\psi_3(x_3)e^{\sum_{(i,j)\in E}\theta_{ij} (x_i\land x_j)}
    \end{aligned}
    \nonumber
\end{equation}

More general:
\begin{equation}
    \begin{aligned}
        p(\vec{x})=\frac{1}{Z}e^{\sum_{i\in V}\theta_i x_i+\sum_{(i,j)\in E}\theta_{ij} (x_i\land x_j)}
    \end{aligned}
    \nonumber
\end{equation}
In this form, if $\theta_{ij}>0$ $\Rightarrow$ $ij$ is attractive edge; if $\theta_{ij}<0$ $\Rightarrow$ $ij$ is repulsive edge.




\subsubsection{Marginal polytope}
\begin{equation}
    \begin{aligned}
        \mu_{ix}&=\mathbb{E}_\theta[T_{ix}(x)]=P_\theta(x_i=x)=\text{marginal distribution of }X_i\\
        \mu_{ijxx'}&=\mathbb{E}_\theta[T_{ijxx'}(x,x')]=P_\theta(x_i=x,x_j=x')=\text{2D marginal distribution of }(X_i,X_j)\\
    \end{aligned}
    \nonumber
\end{equation}


\subsubsection{Locally Consistent Marginal Distributions}
Given a graph $G = (V,E)$, consider the set of marginal distributions $\tau_i$ on individual nodes $i \in V$ and pairwise marginals $\tau_{ij}$ on edges $(i, j) \in E$ that are locally consistent in the sense

\begin{equation}
    \begin{aligned}
        \sum_x \tau_i(x)=1,\quad \tau_i(x)\geq 0,\ \forall i,x\\
        \sum_{x,x'}\tau_{ij}(x,x')=1,\quad \tau_{ij}(x,x')\geq \forall i,j,x,x'\\
        \sum_{x_j\in X}\tau_{ij}(x_i,x_j)=\tau_i(x_i), \forall (i,j)\in E, x_i\in X\\
        \sum_{x_i\in X}\tau_{ij}(x_i,x_j)=\tau_j(x_j), \forall (i,j)\in E, x_j\in X
    \end{aligned}
    \nonumber
\end{equation}

\begin{definition}
    The local marginal polytope $\mathcal{L}(G)$ is the set of $\tau=\left(\left\{\tau_i\right\}_{i \in V},\left\{\tau_{i j}\right\}_{(i, j) \in E}\right)$ that satisfy the above consistency conditions.
\end{definition}

This is a fairly simple polytope defined by $|V|+\left(2|X|+|X|^2\right)|E|$ linear constraints. Clearly the marginal polytope $\mathcal{M}(G)$ is a subset of $\mathcal{L}(G)$, but is the converse true?

\begin{proposition}
    If $G=(V,E)$ is a forest then $\mathcal{M}(G)=\mathcal{L}(G)$. Any probability distribution on $G$ can be expressed as follows in terms of its $1-D$ and pairwise marginals:
    $$
    p(\boldsymbol{x})=\left(\prod_{i \in V} \mu_i\left(x_i\right)\right)\left(\prod_{(i, j) \in E} \frac{\mu_{i j}\left(x_i, x_j\right)}{\mu_i\left(x_i\right) \mu_j\left(x_j\right)}\right)
    $$
\end{proposition}


\subsubsection{Entropy on Tree Graphs}
Any distribution $p$ defined on a tree graph is of the form $p(\boldsymbol{x})=\left(\prod_{i \in V} \mu_i\left(x_i\right)\right)\left(\prod_{(i, j) \in E} \frac{\mu_{i j}\left(x_i, x_j\right)}{\mu_i\left(x_i\right) \mu_j\left(x_j\right)}\right)$. Hence, its entropy is given by
$$
\begin{aligned}
H(p) &=\mathbb{E}_p[-\ln p(\boldsymbol{X})] \\
&=\sum_{i \in V} \mathbb{E}_p\left[-\ln \mu_i\left(X_i\right)\right]-\sum_{(i, j) \in E} \mathbb{E}_p\left[\ln \frac{\mu_{i j}\left(X_i, X_j\right)}{\mu_i\left(X_i\right) \mu_j\left(X_j\right)}\right] \\
&=\sum_{i \in V} H\left(\mu_i\right)-\sum_{(i, j) \in E} I\left(\mu_{i j}\right)
\end{aligned}
$$
where
$$
\begin{aligned}
I\left(\mu_{i j}\right) & \triangleq \mathbb{E}_{\mu_{i j}}\left[\ln \frac{\mu_{i j}\left(X_i, X_j\right)}{\mu_i\left(X_i\right) \mu_j\left(X_j\right)}\right] \\
&=D\left(\mu_{i j} \| \mu_i \mu_j\right) \\
&=H\left(\mu_i\right)+H\left(\mu_j\right)-H\left(\mu_{i j}\right)
\end{aligned}
$$
is the mutual information associated with the pairwise marginal $\mu_{i j}$. Since this is a Kullback-Leibler divergence, it is nonnegative. The mutual information is zero if $X_i$ and $X_j$ are independent random variables and is upper-bounded by both $H\left(\mu_i\right)$ and $H\left(\mu_j\right)$ (value achieved if $X_j$ is a function of $X_i$, or vice-versa).

The entropy of $p$ is easily computed but is not concave in $\mu$. Equivalently, the set of distributions $p$ on a tree graph is generally nonconvex. (Consider a length-3 chain for instance.)


\subsection{Naive Mean-Field Methods In Graph}
Approximate complicated p.d.f. $p_\theta(\vec{x})=e^{\theta^T I(\vec{x})-A(\theta)}$ in $G=(V,E)$ with tractable $q(\vec{x})\in G'(V,E')$, where $E'\subset E$. $q(\vec{x})=\prod_{i\in V}q_i(x_i)$.

Minimizing divergence:
\begin{equation}
    \begin{aligned}
        D(q\| p_\theta)&=\mathbb{E}_q \left[\ln\frac{q(\vec{x})}{p_\theta(\vec{x})}\right]\\
        &=-\theta^T \mathbb{E}_q\left[T(\vec{x})\right]+A(\theta)-H(q)
    \end{aligned}
    \nonumber
\end{equation}
$Q=\{q:\mathbb{E}_q[T(\vec{x})]=\mu, \mu\in M'\}$
\begin{equation}
    \begin{aligned}
        \min_{q\in Q} D(q\| p_\theta)= A(\theta)-\max_{\mu\in M'} [\theta^T \mu-A^*(\mu)]
    \end{aligned}
    \nonumber
\end{equation}
\begin{equation}
    \begin{aligned}
        \max_{\{\mu_i\}_{i\in V}} [\theta^T \mu-A^*(\mu)]&=\sum_{i\in V} \sum_{x\in X}\theta_{ix}\mu_i(x)+\sum_{(i,j)\in E}\sum_{x,x'}\theta_{ijxx'}\mu_{ij}(x,x')+\sum_{i\in V}H(\mu_i)
    \end{aligned}
    \nonumber
\end{equation}
Taking Lagrangian and taking derivative
\begin{equation}
    \begin{aligned}
        0=\frac{\partial L(\mu,\lambda)}{\partial \mu_i(x)} \Rightarrow \mu_{i(x)}=\frac{1}{Z} e^{\theta_{ix}+\sum_{i\in N(i)}\sum_{x'\in X}\theta_{ijxx'}\mu_j(x')}
    \end{aligned}
    \nonumber
\end{equation}

\subsection{Structural Mean Field Optimization}
$q(\vec{x})=q_1(x_1)q_2(x_2|x_1)q_3(x_3|x_2)$ (Markov Chain in a tree $G'=(V,E')$)
\begin{equation}
    \begin{aligned}
        \mu_{12}(x_1,x_3)&=\sum_{x_2}p(x_1,x_2,x_3)=\sum_{x_2}p(x_1,x_2)p(x_3|x_2)=\sum_{x_2}p(x_1,x_2)\frac{p_{23}(x_2,x_3)}{p(x_2)}
    \end{aligned}
    \nonumber
\end{equation}

\subsection{Bethe Entropy Approximation}
\begin{equation}
    \begin{aligned}
        A^*(\mu)=-H(p_{\theta^*}(\mu))
    \end{aligned}
    \nonumber
\end{equation}
For a distribution $p$ that is not defined on a tree graph, $H(p)$ does not admit a simple expression, and cannot be expressed simply in terms of 1-D marginals and pairwise marginals. (Verify on a 3-cycle). However if these marginals are known, one could use (31) as an approximation to $H(p)$. This approximation is known as the Bethe approximation, and the functional
$$
H_{\text {Bethe }}(\tau) \triangleq \sum_{i \in \mathcal{V}} H\left(\tau_i\right)-\sum_{(i, j) \in \mathcal{E}} I\left(\tau_{i j}\right), \quad \tau \in \mathcal{L}(\mathcal{G})
$$
is known as the Bethe entropy. This "entropy" is well defined for all pseudomarginals $\tau \in \mathcal{L}(\mathcal{G})$
The Bethe variational problem is defined as
$$
A_{\text {Bethe }}(\theta) \triangleq \max _{\tau \in \mathcal{L}(\mathcal{G})}\left[\theta^{\top} \tau+H_{\text {Bethe }}(\tau)\right]
$$
and is relatively tractable owing to the simple nature of $\mathcal{L}(\mathcal{G})$ and the availability of a closed-form expression for $H_{\text {Bethe }}(\tau)$. Compare with the expression
$$
A(\theta)=\sup _{\mu \in \mathcal{M}(\mathcal{G})}\left[\theta^{\top} \mu+H\left(p_{\theta(\mu)}\right)\right]
$$
that is unfortunately intractable because of the complex nature of $\mathcal{M}(\mathcal{G})$ and the lack of an explicit form for $H\left(p_\mu\right)$. For a general graph, $\mathcal{M}(\mathcal{G}) \subset \mathcal{L}(\mathcal{G})$ and Bethe entropy is an approximation to entropy; $A_{\text {Bethe }}(\theta)$ is not a bound on $A(\theta)$, only an approximation (see example below). For a tree graph however, $\mathcal{M}(\mathcal{G})=\mathcal{L}(\mathcal{G})$ and $A_{\text {Bethe }}(\theta)=A(\theta)$

\textbf{Example: Inexactness of Bethe approximation} Consider a fully connected graph with four nodes, $V = \{1, 2, 3, 4\}$, uniform 1-D marginals $\mu_i, i \in V$, and pairwise marginals. $\mu_{ij}=\begin{bmatrix}
    0.5&0\\
    0&0.5
\end{bmatrix}$, $\forall i,j\in V$ $\Rightarrow$ $X_i=X_j$ w.p.1 . $\vec{x}=[0,0,0,0]$ or $[1,1,1,1]$ with probability $0.5$ each.

We have $\mu \in \mathcal{M}(\mathcal{G})$; indeed the distribution $p$ that places probability $\frac{1}{2}$ on the sequences $(0,0,0,0)$ and $(1,1,1,1)$ satisfies the marginal constraints above. We have $H\left(\mu_i\right)=\ln 2$ for all $i \in \mathcal{V}$ and $I\left(\mu_{i j}\right)=\ln 2$ for all $i \neq j \in \mathcal{V}$. Since there are 6 edges, we obtain
$$
H_{\text {Bethe }}(\mu)=4 \ln 2-6 \ln 2=-2 \ln 2<0
$$
which shows that the Bethe entropy does not satisfy the same properties as an entropy (it can be negative). The actual entropy $H(p) = \ln 2 > 0$.


\section{$\ell_1$ Penalized Least Squares Minimization}
Given an observation vector $y \in \mathbb{R}^m$, a $m \times n$ matrix $\mathrm{A}$, a constant $\lambda>0$, find a vector $x \in \mathbb{R}^n$ that achieves the minimum of
$$
f(x) \triangleq \frac{1}{2}\|y-\mathrm{A} x\|^2+\lambda\|x\|_1 .
$$
The first component is half the squared $\ell_2$ norm of $r \triangleq y-A x$, which can be interpreted as an observation error. The second component, $\|x\|_1 \triangleq \sum_{i=1}^n\left|x_i\right|$, is the $\ell_1$ norm of $x$.

The problem admits a Bayesian interpretation, in which the observations $y$ are the sum of $A x$ and white Gaussian noise with mean zero and variance $\sigma^2$, $$y=Ax+z,\text{ where }z\sim N(0,\sigma^2)$$

and $x$ is a realization of a random vector with iid entries following a double-exponential (Laplace) distribution. In this case,
$$
\ln p(y \mid x)+\ln p(x)=-\frac{n}{2} \ln \left(2 \pi \sigma^2\right)-\frac{1}{2 \sigma^2}\|y-\mathrm{A} x\|^2-n \ln 2-\|x\|_1
$$
hence minimizing $f$ is equivalent to MAP estimation for the above Bayesian problem, with $\lambda=2 \sigma^2$.

The structure of $A$ depends on the application. In signal processing and computer vision, $A$ is usually related to a convolution operator, describing for instance motion blur in video. In compressive sensing, the entries of $A$ are typically iid random variables. The algorithms we will focus on do not require $A$ to have any special structure. We begin with two important simple cases (identity and orthonormal A), then move to the general problem.

\subsection{Special Cases}
\subsubsection{Identity $A$}
Let $m = n$ and $A$ be the identity matrix
$$
f(x) \triangleq \sum_{i=1}^n\left(\frac{1}{2}(y_i-x_i)^2+\lambda |x_i|\right)
$$
\begin{equation}
    \begin{aligned}
        f'(x)&=\left\{\begin{matrix}
            -y+x+\lambda \text{sign}(x),&x\neq 0\\
            \text{does not exist},&x=0
        \end{matrix}\right.\\
        0&=f'(x) \Rightarrow x=\left\{\begin{matrix}
            y-\lambda,&\text{for }y\geq \lambda\\
            y+\lambda&\text{for }y\leq -\lambda\\
            0&\text{for }|y|< \lambda\\
        \end{matrix}\right.\triangleq S_\lambda(y)
    \end{aligned}
    \nonumber
\end{equation}

\subsubsection{Orthonormal $A$}
If $m = n$ and $A$ is orthonormal, then $A^{-1} = A^T$ and
\begin{equation}
    \begin{aligned}
        \|y-Ax\|^2=\|A(A^Ty-x)\|^2=\|A^Ty-x\|^2\\
        \Rightarrow x=S_\lambda (A^Ty)
    \end{aligned}
    \nonumber
\end{equation}

\subsection{Quadratic Optimization}
We now consider general A. It is useful to first study the case $\lambda=0$, in which case $f$ is quadratic and the optimization problem is smooth. The solution $x$ satisfies the necessary first-order optimality condition
$$
0=\nabla f(x)=-\mathrm{A}^{\top}(y-\mathrm{A} x) \quad \in \mathbb{R}^n .
$$
If $\operatorname{rank}(\mathrm{A}) \geq n$ (which implies $m \geq n$ ), the unique solution is $x=\left(\mathrm{A}^{\top} \mathrm{A}\right)^{-1} \mathrm{~A}^{\top} y$.\\
Otherwise the solution is nonunique. Any $x=\mathrm{A}^{+} y+z$ where $z \in \operatorname{Null}(\mathrm{A})$ and $\mathrm{A}^{+} \in \mathbb{R}^{n \times m}$ is the Moore pseudo-inverse of $A$, is a solution. The minimum-norm solution is $x=\mathrm{A}^{+} y$.

Even though a closed-form solution exists, for large $n$ one would avoid the computationally expensive matrix inverse and use an iterative algorithm such as gradient descent or conjugate gradient to derive the solution. The gradient descent update takes the form
$$
x^{k+1}=x^k+\alpha \mathrm{A}^{\top}\left(y-\mathrm{A} x^k\right), \quad k=1,2,3, \cdots
$$
where $\alpha$ is the step size.



















\end{document}