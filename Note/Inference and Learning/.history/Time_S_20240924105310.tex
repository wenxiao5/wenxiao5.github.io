\documentclass[11pt]{elegantbook}
\usepackage{graphicx}
%\usepackage{float}
\definecolor{structurecolor}{RGB}{40,58,129}
\linespread{1.6}
\setlength{\footskip}{20pt}
\setlength{\parindent}{0pt}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\elegantnewtheorem{proof}{Proof}{}{Proof}
\elegantnewtheorem{claim}{Claim}{prostyle}{Claim}
\DeclareMathOperator{\col}{col}
\title{Time Series}
\author{Wenxiao Yang}
\institute{Haas School of Business, University of California Berkeley}
\date{2024}
\setcounter{tocdepth}{2}
\extrainfo{All models are wrong, but some are useful.}

\cover{cover.png}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}

\addbibresource[location=local]{reference.bib} % bib

\begin{document}
\maketitle

\frontmatter
\tableofcontents

\mainmatter

\chapter{Time Series Analysis}
\section{Goals and Challenge}
\textbf{Data} in time series is denoted by
\begin{equation}
    \begin{aligned}
        \{\underbrace{y_t}_{n\times 1}:1\leq t\leq T\}
    \end{aligned}
    \nonumber
\end{equation}
%Some fundamental assumptions are needed for statistics.
\begin{assumption}
    Each $y_t$ is the realization of some random vector $Y_t$.
\end{assumption}
The \textbf{objective} is to provide data-based answers to questions about the distribution of $\{Y_t:1\leq t\leq T\}$.

The \textbf{challenge} we face is $Y_1,Y_2,...,Y_T$ are \textit{not necessarily independent}. Time series analysis gives the models and methods that can accommodate dependence.

\section{Stochastic Processes}
Some terminologies we need to know:
\begin{definition}[Stochastic Process]
    A \textbf{stochastic process} is a collection $\{Y_t:t\in\mathcal{T}\}$ of random variables/vectors (defined on the same probability space).
    \begin{enumerate}
        \item $\{Y_t:t\in\mathcal{T}\}$ is \textbf{discrete time process} if $\mathcal{T}=\{1,...,T\}$ or $\mathcal{T}=\mathbb{N}=\{1,2,...\}$ or $\mathcal{T}=\mathbb{Z}=\{...,-1,0,1,...\}$.
        \item $\{Y_t:t\in\mathcal{T}\}$ is \textbf{continuous time process} if $\mathcal{T}=[0,1]$ or $\mathcal{T}=\mathbb{R}_+$ or $\mathcal{T}=\mathbb{R}$.
    \end{enumerate}
\end{definition}
Observed data $Y_t$ is a realization of a discrete time process with $\mathcal{T}=\{1,...,T\}$.

\subsection{Strictly Stationary}
\begin{definition}[Strictly Stationary (Discrete and Scalar Process)]
    A scalar\footnote{i.e., $Y_t$ is $1\times 1$} process $\{Y_t:t\in \mathbb{Z}\}$ is \textbf{strictly stationary} \textit{if and only if}
    \begin{equation}
        \begin{aligned}
            \left(Y_t,...,Y_{t+k}\right)\underbrace{\sim}_\textnormal{``is distributed as''} \left(Y_0,...,Y_{k}\right),\ \forall t\in \mathbb{Z},k\geq 0
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}
\begin{note}
    \begin{enumerate}
        \item If $Y_t\sim i.i.d.$, then $\{Y_t:t\in \mathbb{Z}\}$ is strictly stationary.
        \item If $\{Y_t:t\in \mathbb{Z}\}$ is strictly stationary, then $Y_t$ are identically distributed (i.e., ``marginal stationary'').
        \begin{example}[ Strictly Stationary and Dependent]
            A constant process that $...=Y_{-1}=Y_0=Y_1=...$ is strictly stationary.
        \end{example}
    \end{enumerate}
    All these above hold for strictly stationary vector process.
\end{note}

\begin{lemma}[Property of Strictly Stationary]
    If $\{Y_t:t\in \mathbb{Z}\}$ is strictly stationary with $\mathbb{E}[Y_t^2]<\infty$ ($\forall t$), then
    \begin{enumerate}
        \item Same Expectation:
        \begin{equation}
            \begin{aligned}
                \mathbb{E}[Y_t]=\mu,\ \forall t \textnormal{ (for some constant $\mu$)}
            \end{aligned}
            \label{s}
            \tag{*}
        \end{equation}
        \item Covariance only depends on time length:
        \begin{equation}
            \begin{aligned}
                \textnormal{Cov}(Y_t,Y_{t-j})=\gamma(j),\ \forall t,j \textnormal{ (for some function $\gamma(\cdot)$)}
            \end{aligned}
            \label{ss}
            \tag{**}
        \end{equation}
        Note $\gamma(0)= \textnormal{Var}(Y_t),\forall t$.
    \end{enumerate}
\end{lemma}

\subsection{Covariance Stationary}
A subset of strictly stationary processes that has second moment (i.e., $\mathbb{E}[Y_t^2]<\infty$) can be defined as \textbf{covariance stationary}.
\begin{definition}[Covariance Stationary]
    A process $\{Y_t:t\in \mathbb{Z}\}$ is \textbf{covariance stationary} \textit{iff} $\mathbb{E}[Y_t^2]<\infty$ ($\forall t$) and it satisfies \eqref{s} and \eqref{ss}.
\end{definition}
\begin{note}
    Not every strictly stationary process is covariance stationary. (e.g., if it does not have second moment).
\end{note}

\subsection{Autocovariance and Autocorrelation Functions}
\begin{definition}[Autocovariance and Autocorrelation Functions]
    $\gamma(\cdot)$ in \eqref{ss} is called \textbf{autocovariance function} of $\{Y_t:t\in \mathbb{Z}\}$.\\
    The \textbf{autocorrelation function} is $\rho(j)=\textnormal{Corr}(Y_t,Y_{t-j})=\frac{Cov(Y_t,Y_{t-j})}{\sqrt{\textnormal{Var}(Y_t)\textnormal{Var}(Y_{t-j})}}=\frac{\gamma(j)}{\gamma(0)}$.
\end{definition}
\begin{lemma}\label{lemma_ACF property}
    The autocovariance function satisfies the following properties:
    \begin{enumerate}
        \item $\gamma(\cdot)$ is \textbf{even} i.e., $\gamma(j)=\gamma(-j)$.
        \item $\gamma(\cdot)$ is \textbf{positive semi-definite} (psd) i.e., for any  $n\in \mathbb{N}$ and any $a_1,...,a_n$,
        \begin{equation}
            \begin{aligned}
                \sum_{i=1}^n\sum_{j=1}^na_ia_j\gamma(i-j)=\textnormal{Var}(\sum_{i=1}^na_iY_i)\geq 0
            \end{aligned}
            \nonumber
        \end{equation}
    \end{enumerate}
\end{lemma}


\section{Moving-Average Process}
\begin{definition}[White Noise]
    A process $\{\epsilon_t:t\in \mathbb{Z}\}$ is a \textbf{white noise} process iff it is covariance stationary with $\mathbb{E}[\epsilon_t]=0$ and
    \begin{equation}
        \begin{aligned}
            \textnormal{Cov}(\epsilon_t,\epsilon_{t-j})=\left\{\begin{matrix}
                \sigma^2,& \textnormal{ if }j=0\\
                0,& \textnormal{ otherwise}
            \end{matrix}\right.
        \end{aligned}
        \nonumber
    \end{equation}
    We use \textit{notation} $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$.
\end{definition}
\begin{note}
    \begin{enumerate}
        \item If $\epsilon_t\sim \textnormal{i.i.d.}(0,\sigma^2)$, then $\{\epsilon_t:t\in \mathbb{Z}\}$ is white noise, i.e., $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$.
        \item Gauss-Markov theorem assumes WN errors.
        \item WN terms are used as ``building blocks'': often a variable can be generated as
        \begin{equation}
            \begin{aligned}
                Y_t=h(\epsilon_t,\epsilon_{t-1},...) \textnormal{ for some function $h(\cdot)$ and some $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$.}
            \end{aligned}
            \nonumber
        \end{equation}
    \end{enumerate}
\end{note}

\subsection{Moving-Average Process}
\begin{definition}[Finite Moving-Average Process]
    \begin{enumerate}
        \item First-order moving average process: $Y_t\sim \textnormal{MA}(1)$ iff
        \begin{equation}
            \begin{aligned}
                Y_t=\mu+\epsilon_t+\theta\epsilon_{t-1}
            \end{aligned}
            \nonumber
        \end{equation}
        where $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$.
        \begin{claim}
            $\{Y_t\}$ is covariance stationary: $\mathbb{E}[Y_t]=\mu$ and its autocovariance function is
            \begin{equation}
                \begin{aligned}
                    \gamma(j):=\textnormal{Cov}(Y_t,Y_{t-j})=\left\{\begin{matrix}
                        (1+\theta^2)\sigma^2,&j=0\\
                        \theta\sigma^2,&j=1\\
                        0,&j\geq 2
                    \end{matrix}\right.
                \end{aligned}
                \nonumber
            \end{equation}
        \end{claim}
        \item $Y_t\sim \textnormal{MA}(q)$ (for some $q\in \mathbb{N}$) iff
        \begin{equation}
            \begin{aligned}
                Y_t=\mu+\epsilon_t+\sum_{i=1}^{q}\theta_i\epsilon_{t-i}
            \end{aligned}
            \nonumber
        \end{equation}
        where $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$.
        \begin{claim}
            $\{Y_t\}$ is covariance stationary: $\mathbb{E}[Y_t]=\mu$ and its autocovariance function is
            \begin{equation}
                \begin{aligned}
                    \gamma(j):=\textnormal{Cov}(Y_t,Y_{t-j})=\left\{\begin{matrix}
                        \left(\sum_{i=0}^{q-j}\theta_i\theta_{i+j}\right)\sigma^2,&j\leq q\\
                        0,&j\geq q+1
                    \end{matrix}\right.
                \end{aligned}
                \nonumber
            \end{equation}
            where $\theta_0=1$.
        \end{claim}
    \end{enumerate}
\end{definition}

\begin{definition}[Infinite Moving-Average Process]
    $Y_t\sim \textnormal{MA}(\infty)$ iff
        \begin{equation}
            \begin{aligned}
                Y_t=\mu+\sum_{i=0}^{\infty}\psi_i\epsilon_{t-i}
            \end{aligned}
            \nonumber
        \end{equation}
        where $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$ and \underline{$\sum_{i=0}^{\infty}\psi_i^2<\infty$}
\end{definition}

\subsection{Conditions for Infinite Moving-Average Process}
\begin{note}
    Conjecture:
    \begin{enumerate}
        \item $\{Y_t\}$ is covariance stationary;
        \item $\mathbb{E}[Y_t]=\mu$ and
        \item its autocovariance function is $$\gamma(j):=\textnormal{Cov}(Y_t,Y_{t-j})=(\sum_{i=0}^{\infty}\psi_i\psi_{i+j})\sigma^2,\forall j\geq 0.$$
    \end{enumerate}
\end{note}
The necessary condition to make these conjectures correct is
\begin{equation}
    \begin{aligned}
        \mathbb{E}[Y_t^2]&=(\mathbb{E}[Y_t])^2+\Gamma(0)\\
        &=\mu^2+(\sum_{i=0}^{\infty}\psi_i^2)\sigma^2<\infty\\
        \Leftrightarrow \sum_{i=0}^{\infty}\psi_i^2&<\infty
    \end{aligned}
    \nonumber
\end{equation}
\begin{claim}
    With the `right' definition of ``$\sum_{i=0}^{\infty}$'', the conjecture is correct.
\end{claim}
\begin{remark}
    \begin{enumerate}
        \item If $X_0,X_1,...$ are i.i.d. with $X_0=0$, then $\sum_{i=0}^\infty X_i$ denote $\lim_{n \rightarrow \infty}\sum_{i=0}^n X_i$ (assuming the limit exists).
        \item $\exists$ various models of stochastic convergence.
        \item There: convergence in mean square.
    \end{enumerate}
\end{remark}
\begin{definition}[Stochastic Convergence in Mean Square]
    If $X_0,X_1,...$ are random (with $\mathbb{E}[X_i^2]<\infty,\forall i$), then $\sum_{i=0}^\infty X_i$ denotes any $S$ such that $\lim_{n \rightarrow \infty}\mathbb{E}[(S-\sum_{i=0}^n X_i)^2]=0$.
\end{definition}
\begin{lemma}
    The properties of the $S$ are
    \begin{enumerate}
        \item $S$ is ``essentially unique.''
        \item $\mathbb{E}[S]=\sum_{i=0}^\infty \mathbb{E}[X_i]=\lim_{n \rightarrow \infty}\sum_{i=0}^n \mathbb{E}[X_i]$
        \item $\textnormal{Var}[S]=...=\lim_{n \rightarrow \infty}\textnormal{Var}[\sum_{i=0}^n X_i]$
        \item (Higher order moments of $S$ are similar) $\cdots$
    \end{enumerate}
\end{lemma}

\begin{theorem}[Cauchy Criterion]
    $\sum_{i=0}^\infty X_i$ exists iff
    \begin{equation}
        \begin{aligned}
            \lim_{n \rightarrow \infty}\sup_{m>n}\mathbb{E}[(S_m-S_n)^2]=0,
        \end{aligned}
        \nonumber
    \end{equation}
    where $S_n=\sum_{i=0}^n X_i$.
\end{theorem}
In the \underline{$MA(\infty)$ context}: The condition that can make
\begin{equation}
    \begin{aligned}
        \lim_{n \rightarrow \infty}\sup_{m>n}\mathbb{E}[(Y_{t,m}-Y_{t,n})^2]=0
    \end{aligned}
    \nonumber
\end{equation}
where $Y_{t,n}=\mu+\sum_{i=0}^n\psi_i\epsilon_{t-i}$.\\
This condition is given as: If $m>n$,
\begin{equation}
    \begin{aligned}
        &Y_{t,m}-Y_{t,n}=\sum_{i=n+1}^m\psi_i\epsilon_{t-i}\\
        \Rightarrow & \mathbb{E}[(Y_{t,m}-Y_{t,n})^2]=\mathbb{E}\left[\left(\sum_{i=n+1}^m\psi_i\epsilon_{t-i}\right)^2\right]=\left(\sum_{i=n+1}^m\psi_i^2\right)\sigma^2\\
        \Rightarrow & \sup_{m>n}\mathbb{E}[(Y_{t,m}-Y_{t,n})^2]=\left(\sum_{i=n+1}^\infty\psi_i^2\right)\sigma^2\\
        \Rightarrow & \lim_{n \rightarrow \infty}\sup_{m>n}\mathbb{E}[(Y_{t,m}-Y_{t,n})^2]=\left(\lim_{n \rightarrow \infty}\sum_{i=n+1}^\infty\psi_i^2\right)\sigma^2
    \end{aligned}
    \nonumber
\end{equation}
Thus,
\begin{equation}
    \begin{aligned}
        \lim_{n \rightarrow \infty}\sup_{m>n}\mathbb{E}[(Y_{t,m}-Y_{t,n})^2]=0 &\textnormal{ iff }\lim_{n \rightarrow \infty}\sum_{i=n+1}^\infty\psi_i^2=0\\
        &\textnormal{ iff }\sum_{i=0}^\infty\psi_i^2<\infty
    \end{aligned}
    \nonumber
\end{equation}


\subsection{Remarks about $MA(\infty)$ models}
\begin{enumerate}
    \item $MA(\infty)$ models are useful in theoretical work.
    \item The $MA(\infty)$ class is ``large'': Wold decomposition (theorem).
    \item Parametric $MA(\infty)$ models are useful in inference.
\end{enumerate}

\section{Autoregressive Model (Special Case of $MA(\infty)$)}
Autoregressive model is an example of well-defined $MA(\infty)$ model.
\begin{example}[ (Autoregressive model)]
    Suppose
    \begin{equation}
        \begin{aligned}
            Y_{t}=\sum_{i=0}^\infty \psi_i\epsilon_{t-i},\ \forall t
        \end{aligned}
        \nonumber
    \end{equation}
    where
    \begin{enumerate}[$\circ$]
        \item $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$;
        \item $\psi_i=\phi^i$ ($\forall i\geq 0$) for some $|\phi|<1$.
    \end{enumerate}
\end{example}
Checking the condition: $\lim_{n \rightarrow \infty}\sum_{i=0}^n\psi_i^2=\lim_{n \rightarrow \infty}\sum_{i=0}^n\phi^{2i}=\lim_{n \rightarrow \infty}\frac{1-\phi^{2(n+1)}}{1-\phi^2}=\frac{1}{1-\phi^2}<\infty$.

\begin{lemma}[Property of Autoregressive Model]
    For $j\geq 0$, the autocovariance function is
    \begin{equation}
        \begin{aligned}
            \gamma(j):&=\textnormal{Cov}(Y_t,Y_{t-j})=(\sum_{i=0}^{\infty}\psi_i\psi_{i+j})\sigma^2=\phi^{j}(\sum_{i=0}^{\infty}\phi^{2i})\sigma^2=\phi^j\frac{\sigma^2}{1-\phi^2}\\
            &=\phi^j \gamma(0)
        \end{aligned}
        \nonumber
    \end{equation}
    \begin{note}
        \begin{enumerate}
            \item $\gamma(j)\neq 0, \forall j$ if $\phi\neq 0$.
            \item $\gamma(j)\propto \phi^j$ decays exponentially.
        \end{enumerate}
    \end{note}
\end{lemma}

\begin{definition}[Alternative Representation of AR]
    Alternatively, the AR model ca be represented as
    \begin{equation}
        \begin{aligned}
            Y_t=\phi Y_{t-1}+\epsilon_t,\ \forall t
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}
\begin{proof}
    \begin{equation}
        \begin{aligned}
            Y_{t}=\sum_{i=0}^\infty \psi_i\epsilon_{t-i}=\epsilon_t+\sum_{i=1}^\infty \psi_i\epsilon_{t-i}=\epsilon_t+\sum_{i=0}^\infty \psi_{i+1}\epsilon_{t-i-1}=\epsilon_t+\phi\sum_{i=0}^\infty \psi_{i}\epsilon_{t-i-1}=\epsilon_t+\phi Y_{t-1}
        \end{aligned}
        \nonumber
    \end{equation}
\end{proof}

The natural estimator of $\phi$ (OLS) is
\begin{equation}
    \begin{aligned}
        \hat{\phi}=\frac{\sum_{t=2}^TY_{t-1}Y_t}{\sum_{t=2}^T Y_{t-1}^2}
    \end{aligned}
    \nonumber
\end{equation}



\begin{definition}[Model for Finite AR]
    \begin{equation}
        \begin{aligned}
            Y_t=\phi Y_{t-1}+\epsilon_t,\ 2\leq t\leq T
        \end{aligned}
        \nonumber
    \end{equation}
    where
    \begin{enumerate}[$\circ$]
        \item $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$;
        \item $|\phi|<1$;
        \item $Y_1=\sum_{i=0}^\infty \phi^i\epsilon_{1-i}$
    \end{enumerate}
\end{definition}


More generally, consider an AR with a drift,
\begin{equation}
    \begin{aligned}
        Y_t=c+\phi Y_{t-1}+\epsilon_t,\ \forall t
    \end{aligned}
    \nonumber
\end{equation}
where $c=\mu(1-\phi)$.

\begin{definition}[$AR(1)$]
    $\{Y_t:1\leq t\leq T\}$ is an \textbf{autoregreessive process} of order $1$, $Y_t\sim AR(1)$, if
    \begin{equation}
        \begin{aligned}
            Y_t=c+\phi Y_{t-1}+\epsilon_t,\ 2\leq t\leq T
        \end{aligned}
        \nonumber
    \end{equation}
    where $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$.
    \begin{note}
        $|\phi|<1$ is not assumed (yet) and $Y_1=\mu+\sum_{i=0}^\infty \phi^i\epsilon_{1-i}$ is not assumed.
    \end{note}
    We call the AR(1) model is \textbf{stable} iff $|\phi|<1$.
\end{definition}
\begin{enumerate}[$\circ$]
    \item If $|\phi|<1$ and $Y_1=\mu+\sum_{i=0}^\infty \phi^i\epsilon_{1-i}$, $$Y_t=\mu+\sum_{i=0}^\infty \phi^i\epsilon_{t-i},$$ where $\mu=\frac{c}{1-\phi}$.
    \item OLS ``works'' when $|\phi|<1$.
    \item The $AR(1)$ model admits and $MA(\infty)$ solution
    \begin{equation}
        \begin{aligned}
            Y_t=\mu+\sum_{i=0}^\infty \psi_i\epsilon_{t-i},\quad \sum_{i=0}^\infty \psi_i^2<\infty
        \end{aligned}
        \nonumber
    \end{equation}
    \underline{iff} $|\phi|<1$.
    \item The AR(1) model admits a covariance stationary solution \underline{iff} $|\phi|\neq 1$.
    \begin{note}
        Consider the case that $\phi>1$, the intuition is
        \begin{equation}
            \begin{aligned}
                Y_t=\phi Y_{t-1}+\epsilon_t \Leftrightarrow Y_{t-1}=\phi^{-1}(Y_t-\epsilon_t)
            \end{aligned}
            \nonumber
        \end{equation}
    \end{note}
\end{enumerate}

\section{$AR(p)$}
\begin{definition}
    $\{Y_t:t\in \mathbb{N}\}$ is a \textbf{$p^{th}$-order autoregressive process}, $Y_t\sim AR(p)$, iff
    \begin{equation}
        \begin{aligned}
            Y_t=c+\phi_1 Y_{t-1}+\phi_2 Y_{t-2}+\cdots+\phi_p Y_{t-p}+\epsilon_t,\ t\geq p+1
        \end{aligned}
        \nonumber
    \end{equation}
    where $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$.
\end{definition}
\begin{claim}
    OLS ``works'' when the $AR(p)$ model is \underline{stable}.
\end{claim}
\paragraph*{Vector Notation}
We write
\begin{equation}
    \begin{aligned}
        Y_t=\beta'X_t+\epsilon_t,\ t\geq p+1
    \end{aligned}
    \nonumber
\end{equation}
where $\beta=(c,\phi_1,\phi_2,\cdots,\phi_p)'$ and $X_t=(1,Y_{t-1},Y_{t-2},\cdots,Y_{t-p})'$.

Then the \textit{OLS estimator} is given by
\begin{equation}
    \begin{aligned}
        \hat{\beta}=(\sum_{t=p+1}^TX_t'X_t)^{-1}(\sum_{t=p+1}^TX_t'Y_t)
    \end{aligned}
    \nonumber
\end{equation}

\paragraph*{Lag Operator Notation}
For a time series $\{X_t\}$, let
\begin{equation}
    \begin{aligned}
        L X_t&=X_{t-1}\\
        &\vdots\\
        L^k X_t&=X_{t-k},\ \forall t\in \mathbb{Z}
    \end{aligned}
    \nonumber
\end{equation}
which is called \textit{lag operator notation}.

Then, in this notation, the $AR(p)$ model can be written as
\begin{equation}
    \begin{aligned}
        \phi(L)Y_t=c+q_t,\ t\geq p+1
    \end{aligned}
    \nonumber
\end{equation}
where $\phi(L)=1-\phi_1 L - \phi_2 L^2 - \cdots - \phi_p L^p$.

\begin{definition}[Stability of $AR(p)$]
    The $AR(p)$ model is \textbf{stable} if \begin{equation}
        \begin{aligned}
            \phi(z)=1-\phi_1 z - \phi_2 z^2 - \cdots - \phi_p z^p=0 \Rightarrow |z|>1
        \end{aligned}
        \nonumber
    \end{equation}
    (All solutions are greater than 1).
\end{definition}

\begin{enumerate}[$\circ$]
    \item The $AR(p)$ model admits an $MA(\infty)$ solution
    \begin{equation}
        \begin{aligned}
            Y_t=\mu+\sum_{i=0}^\infty \psi_i\epsilon_{t-i},\quad \sum_{i=0}^\infty \psi_i^2<\infty
        \end{aligned}
        \nonumber
    \end{equation}
    \underline{iff} it is \textit{stable}.
    The $MA(\infty)$ solution has
    \begin{equation}
        \begin{aligned}
            \mu=\frac{c}{1-\phi_1-\cdots-\phi_p}=\frac{c}{\phi(1)}
        \end{aligned}
        \nonumber
    \end{equation}
    and (computable) $\psi_i$'s satisfy
    \begin{equation}
        \begin{aligned}
            |\psi_i|\leq M \lambda^i,\ \forall i,
        \end{aligned}
        \nonumber
    \end{equation}
    where $M<\infty$ and $|\lambda|<1$.
\end{enumerate}


\section{}
\paragraph*{$MA(q)$ model in lag operator notation}:
\begin{equation}
    \begin{aligned}
        Y_t&=\mu+\underbrace{\epsilon_t+\sum_{i=1}^{q}\theta_i\epsilon_{t-i}}_{:=\theta(L)\epsilon_t}\\
        &=\mu + \theta(L)\epsilon_t,
    \end{aligned}
    \nonumber
\end{equation}
where $\theta(L)=1+\theta_1 L + \theta_2 L^2 + \cdots + \theta_q L^q$.
\begin{definition}[Invertibility of $MA(q)$]
    The $MA(q)$ model is \textbf{invertible} if \begin{equation}
        \begin{aligned}
            \theta(z)=1+\theta_1 z + \theta_2 z^2 + \cdots + \theta_q z^q=0 \Rightarrow |z|>1
        \end{aligned}
        \nonumber
    \end{equation}
    (All solutions are greater than 1).
\end{definition}
\begin{note}
    If the $MA(q)$ model is invertible, then
    \begin{equation}
        \begin{aligned}
            \epsilon_t=\pi(L)(Y_t-\mu),
        \end{aligned}
        \nonumber
    \end{equation}
    where $\Pi(L)=\sum_{i=0}^\infty \pi_i L^i$ with $\sum_{i=0}^\infty |\pi_i|<\infty$.
\end{note}

\paragraph*{Technicalities}
\begin{enumerate}[$\circ$]
    \item If $\sum_{i=0}^\infty |\pi_i|<\infty$, then $\sum_{i=0}^\infty\pi_i^2<\infty$.
    \item If
    \begin{equation}
        \begin{aligned}
            |\pi_i|\leq M \lambda^i,\ \forall i \textnormal{ (some $M<\infty$ and $|\lambda|<1$)},
        \end{aligned}
        \label{(*)}
        \tag{*}
    \end{equation}
    then
    \begin{equation}
        \begin{aligned}
            \sum_{i=0}^\infty i^r|\pi_i|^s<\infty,\ \forall r\geq 0,s>0
        \end{aligned}
        \nonumber
    \end{equation}
    \item Invertibility $\Rightarrow$ \eqref{(*)}.
    \item If $X_0,X_1,...$ are random variables with $\sup_i \mathbb{E}X_i^2<\infty$, then $\sum_{i=0}^\infty \pi_i X_i$ exists (as a limit in mean squared) if $\sum_{i=0}^\infty |\pi_i|<\infty$.
\end{enumerate}


\subsection{}
\begin{lemma}
    If $\{Y_t\}$ is covariance stationary, then $\gamma(j)=0$, $\forall j>q$ iff $Y_t\sim MA(q)$.
\end{lemma}
\textbf{Question}: Is there a ``$q=\infty$'' analog?
\begin{example}
    Suppose $Y_t=Z\sim \mathcal{N}(0,1), \forall t$. Then, $\textnormal{Cov}(Y_t,Y_{t-1})=1,\forall j$.
    \begin{enumerate}
        \item $Y_t$ is covariance stationary.
        \item It is not a $MA(\infty)$.
        \item $Y_t$ can be predicted without error using $\{Y_s:s\leq t-1\}$.
        \item $Y_t$ is ``deterministic''.
    \end{enumerate}
\end{example}
\begin{definition}[Deterministic]
    A mean zero covariance stationary process $\{v_t\}$ is \textbf{deterministic} iff $\exists p$ and $\{\phi_i:1\leq i\leq p\}$ such that
    \begin{equation}
        \begin{aligned}
            \mathbb{E}[(v_t-\phi v_{t-1}-\cdots -\phi_p v_{t-p})^2]\leq \epsilon^2,\ \forall t
        \end{aligned}
        \nonumber
    \end{equation}
\end{definition}
\begin{claim}
    If $v_t$ is deterministic, then $v_t$ is not a $MA(\infty)$.
\end{claim}


\begin{definition}[Wold Decomposition]
    If $\{Y_t\}$ is a mean zero covariance stationary process, then
    \begin{equation}
        \begin{aligned}
            Y_t=\sum_{i=0}^\infty \phi_i \epsilon_{t-i} + v_t, \forall t,
        \end{aligned}
        \nonumber
    \end{equation}
    where
    \begin{enumerate}
        \item $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$
        \item $\psi_0=1$ and $\sum_{i=0}^\infty \psi_i^2<\infty$
        \item $\mathbb{E}[\epsilon_t v_s]=0, \forall t,s$
        \item $\{v_t\}$ is deterministic
    \end{enumerate}
\end{definition}

\textit{Question}:
When is a function $\gamma(\cdot)$ the autocovariance function (ACF) of a covariance stationary process?

Recall that, if $\gamma(\cdot)$ is an ACF, it satisfies the following properties by Lemma \ref{lemma_ACF property}.
\begin{enumerate}
    \item \underline{Even}: $\gamma(j)=\gamma(-j),\forall j\in \mathbb{N}$.
    \item \underline{Positive semi-definite (PSD)} i.e., for any  $n\in \mathbb{N}$ and any $a_1,...,a_n$,
    \begin{equation}
        \begin{aligned}
            \sum_{i=1}^n\sum_{j=1}^na_ia_j\gamma(i-j)=\textnormal{Var}(\sum_{i=1}^na_iY_i)\geq 0
        \end{aligned}
        \nonumber
    \end{equation}
\end{enumerate}
\begin{lemma}[ACF $\Leftrightarrow$ Even and PSD]
    A function $\gamma(\cdot)$ is an ACF iff it is even and positive semi-definite.
\end{lemma}

\begin{theorem}[Herglotz's Theorem]
    A function $\gamma: \mathbb{Z}\rightarrow \mathbb{R}$ is even and positive semi-definite iff
    \begin{equation}
        \begin{aligned}
            \gamma(j)=\int_{-\pi}^\pi \exp\left(ij\lambda\right)d F(\lambda)
        \end{aligned}
        \nonumber
    \end{equation}
    for some $F:[-\pi, \pi] \rightarrow \mathbb{R}_+$ that is bounded, non-decreasing, and right-continuous (and has $F(-\pi)=0$).
\end{theorem}

\begin{remark}
    \begin{enumerate}
        \item $F(\cdot)$ is called the \underline{spectral distribution function} (of $\gamma(\cdot)$).
        \item If $\exists f:[-\pi, \pi] \rightarrow \mathbb{R}$ such that $$F(\lambda)=\int_{-\pi}^\lambda f(r)d r, \forall \lambda\in[-\pi, \pi],$$
        then $f(\cdot)$ is called a \underline{spectral density function} (of $\gamma(\cdot)$) and
        \begin{equation}
            \begin{aligned}
                \gamma(j)=\int_{-\pi}^\pi \exp\left(ij\lambda\right)f(\lambda)d \lambda = \int_{-\pi}^\pi \cos(j\lambda) f(\lambda) d \lambda
            \end{aligned}
            \nonumber
        \end{equation}
    \end{enumerate}
\end{remark}


\paragraph*{Symmetry} Suppose $\gamma(j)=\int_{-\pi}^\pi \exp\left(ij\lambda\right)d F(\lambda),j\in \mathbb{Z}$. If $\gamma(j\in \mathbb{R}),\forall j$, then
\begin{equation}
    \begin{aligned}
        \int_{-\pi}^\pi \exp\left(ij\lambda\right)d F(\lambda)&=\int_{-\pi}^\pi \left(\cos(j\lambda)+i\sin(j\lambda)\right)d F(\lambda)\\
        &=\int_{-\pi}^\pi \cos(j\lambda)d F(\lambda) + i\int_{-\pi}^\pi\sin(j\lambda)d F(\lambda)\\
    \end{aligned}
    \nonumber
\end{equation}
So $\int_{-\pi}^\pi\sin(j\lambda)d F(\lambda)=0.$

So, $\gamma(\cdot)$ is even by the property of $\cos(\cdot)$.

Then, $\frac{F(\cdot)}{F(\pi)}$ is the CDF of a symmetric distribution on $[-\pi, \pi]$.

\begin{example}
    Suppose $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$. Then,
    \begin{equation}
        \begin{aligned}
            \gamma(j)&=\left\{\begin{matrix}
                1,&\textnormal{ if }j=0\\
                0,&\textnormal{ otherwise}
            \end{matrix}\right.\\
            &=\int_{-\pi}^\pi \cos(j\lambda)f(\lambda) d \lambda\\
            &\Rightarrow f(\lambda)=\frac{1}{2\pi}
        \end{aligned}
        \nonumber
    \end{equation}
\end{example}

\begin{example}
    Suppose $Y_t=Z\sim \mathcal{N}(0,1)$ for all $t$. Then,
    \begin{equation}
        \begin{aligned}
            \gamma(j)&=1\\
            &=\int_{-\pi}^\pi \cos(j\lambda) d F(\lambda)\\
            & \Rightarrow F(\lambda)=
            \left\{\begin{matrix}
                1,&\lambda\geq 0\\
                0,&\lambda<0
            \end{matrix}\right.
        \end{aligned}
        \nonumber
    \end{equation}
\end{example}

\textit{Question}: When does an ACF $\gamma(\cdot)$ admits a spectral density function?

\textit{Partial Answer}: An even function $\gamma: \mathbb{Z}\rightarrow \mathbb{R}$ with ``$\sum_{j=-\infty}^\infty |\gamma(j)|<\infty$'' is psd iff
\begin{equation}
    \begin{aligned}
        f(\lambda)=\frac{1}{2\pi}\sum_{j=-\infty}^\infty \cos\left(j\lambda\right)\gamma(j)\geq 0,\ \forall \lambda\in[-\pi, \pi],
    \end{aligned}
    \label{eq:partial_answer}
\end{equation}
in which case $f(\cdot)$ is a spectral density function of $\gamma(\cdot)$.


\begin{remark}
    A covariance stationary process with an ACF $\gamma(\cdot)$ has \textbf{short memory} if ``$\sum_{j=-\infty}^\infty |\gamma(j)|<\infty$''.
\end{remark}

\begin{proposition}[Implication of Short Memory]
    Given the covariance stationary process has \textbf{short memory} ($\sum_{j=-\infty}^\infty |\gamma(j)|<\infty$), we have
    \begin{enumerate}
        \item $f(\cdot)$ exists (given as \eqref{eq:partial_answer}) and is bounded.
        \item $\gamma(0)=\int_{-\pi}^\pi f(\lambda) d \lambda$.
        \item $f(0)=\frac{1}{2\pi}\sum_{j=-\infty}^\infty \gamma(j)$.
    \end{enumerate}
\end{proposition}

\paragraph*{$MA(\infty)$ Case:}
Suppose
\begin{equation}
    \begin{aligned}
        Y_t=\mu+\sum_{i=0}^\infty \psi_i\epsilon_{t-i},\ \forall t,
    \end{aligned}
    \nonumber
\end{equation}
where
\begin{enumerate}[$\cdot$]
    \item $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$
    \item $\sum_{i=0}^\infty |\psi_i|<\infty$
\end{enumerate}
Then,
\begin{enumerate}[$\circ$]
    \item $\gamma(\cdot)$ has short memory
    \item $\gamma(\cdot)$ has spectral density function given by
    \begin{equation}
        \begin{aligned}
            f(\lambda)&=\frac{1}{2\pi}\sum_{j=-\infty}^\infty \cos\left(j\lambda\right)\gamma(j)\\
            &=\frac{\sigma^2}{2\pi}|\psi(e^{i\lambda})|^2
        \end{aligned}
        \nonumber
    \end{equation}
    where $\gamma(j)=(\sum_{i=0}^\infty \psi_i\psi_{i+j})\sigma^2$ and $\psi(z)=\sum_{i=0}^\infty \psi_iz^i$.
    \item $f(0)=\frac{\sigma^2}{2\pi}\psi(1)^2$
\end{enumerate}


\chapter{Estimation and Inference}

\section{OLS Estimation in $AR(1)$ Model}
Suppose
\begin{equation}
    \begin{aligned}
        Y_t=\phi Y_{t-1}+\epsilon_t,\ \forall t\geq 2,
    \end{aligned}
    \nonumber
\end{equation}
where $\epsilon_t\sim \textnormal{WN}(0,\sigma^2)$.

The \textbf{OLS Estimator of $\phi$} is
\begin{equation}
    \begin{aligned}
        \hat{\phi}_{OLS}=\frac{\sum_{t=2}^TY_{t-1}Y_t}{\sum_{t=2}^T Y_{t-1}^2}
    \end{aligned}
    \nonumber
\end{equation}

\begin{claim}[OLS Estimator is MLE]
    If $\epsilon_t\sim \textnormal{i.i.d.} \mathcal{N}(0,\sigma^2)$ and if $(\epsilon_2,\epsilon_3,...)\perp Y_1$, then $\hat{\phi}_{OLS}$ is the (conditional) MLE of $\phi$.
\end{claim}
The (conditional) MLE of $(\phi,\sigma^2)$ is
\begin{equation}
    \begin{aligned}
        (\hat{\phi}_{ML},\hat{\sigma}^2_{ML})=\argmax_{(\phi,\sigma^2)} f_{2:T}\left(Y_2,...Y_T\mid Y_1;\phi,\sigma^2\right),
    \end{aligned}
    \nonumber
\end{equation}
where $f_{2:T}(\cdot\mid Y_1; \phi,\sigma^2)$ is the (conditional) pdf of $(Y_2,...,Y_T)$ given $Y_1$.


\begin{definition}[Prediction-error Decomposition]
    The objective function (conditional likelihood function) can be written as
    \begin{equation}
        \begin{aligned}
            f_{2:T}\left(Y_2,...,Y_T\mid Y_1;\phi,\sigma^2\right)=\prod_{t=2}^Tf_t\left(Y_t\mid Y_1,...,Y_{t-1};\phi,\sigma^2\right),
        \end{aligned}
        \nonumber
    \end{equation}
    where $f_t\left(Y_t\mid Y_1,...,Y_{t-1};\phi,\sigma^2\right)$ is the conditional pdf of $Y_t$ given $Y_1,...,Y_{t-1}$.
\end{definition}
















































































\end{document}