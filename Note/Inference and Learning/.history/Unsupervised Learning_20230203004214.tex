\documentclass[11pt]{elegantbook}
\definecolor{structurecolor}{RGB}{40,58,129}
\linespread{1.6}
\setlength{\footskip}{20pt}
\setlength{\parindent}{0pt}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\elegantnewtheorem{proof}{Proof}{}{Proof}
\elegantnewtheorem{claim}{Claim}{prostyle}{Claim}
\DeclareMathOperator{\col}{col}
\title{\textbf{Unsupervised Learning}}
\author{Wenxiao Yang}
\institute{Department of Mathematics, University of Illinois at Urbana-Champaign}
\date{}
\setcounter{tocdepth}{2}
\cover{cover.jpg}
\extrainfo{All models are wrong, but some are useful.}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}

\addbibresource[location=local]{reference.bib} % bib

\begin{document}

\maketitle
\frontmatter
\tableofcontents
\mainmatter

\chapter{Clustering}
General Goal of \textbf{Clustering Algorithm}:
\begin{enumerate}[$\circ$]
    \item the "similarity" of the objects in the same cluster is \underline{maximized} while
    \item the "similarity" of objects in different clusters is \underline{minimized}.
\end{enumerate}

\begin{definition}
    For a given set of objects $V = \{x_1, x_2, ... , x_m\}$, we call a \textbf{cluster $\mathbf{S_k}$} a subset of these objects, and we call a \textbf{clustering} the set of all $K$ clusters $\mathbf{\{S_1 ,S_2 , ... , S_k\}}$.
\end{definition}
\begin{example}
    Clustering of $\{x_1,x_2,x_3,x_4\}$: (1). $\{\{x_1,x_3\},\{x_2,x_4\}\}$; (2). $\{\{x_1,x_3\},\{x_1,x_2,x_4\}\}$; (3). $\{\{x_3\},\{x_2,x_4\}\}$.
\end{example}

\section{K-Means}
\subsection{K-Means Clustering Optimization Problem}
\begin{enumerate}
    \item \textbf{Input:}
    \subitem Desired number of clusters (ex: $K=3$)
    \subitem Dataset of $m$ objects $X=\{\vec{x}_1,\vec{x}_2,...,\vec{x}_m\}$, where each object $\vec{x}_i=(x_{i1},x_{i2},...\vec{x}_{in})$ has $n$ numerical attributes. (We can also think of $X$ as being an $m\times n$ matrix $X_{m\times n}$.)
    \item \textbf{Goal of K-Means:}
    \subitem Out of all possible clusterings of $\{S_1 ,S_2 , ... , S_K\}$ with $K$ clusters that can be made from the $m$ objects in $X$, find the optimal clustering $\{S_1^*,S_2^*,...,S_K^*\}$ that \underline{minimizes} the sum of the "distance" of each object and the centroid (the mean of the cluster that object is assigned to).
    \subitem Technically, we can write this as an optimization problem
    \begin{equation}
        \begin{aligned}
            \{S_1^*,S_2^*,...,S_K^*\}=\argmin_{S_1,S_2,...,S_K}\sum_{k=1}^K\sum_{x\in S_k}\|x-\mu_k\|^2\\
            \textnormal{Optimal Inertia}=\min_{S_1,S_2,...,S_K}\sum_{k=1}^K\sum_{x\in S_k}\|x-\mu_k\|^2\\
        \end{aligned}
        \nonumber
    \end{equation}
    Inertia measures how well a dataset was clustered by $K$-Means. It is calculated by measuring the distance between each data point and its centroid, squaring this distance, and summing these squares across one cluster. A good model is one with low inertia \underline{and} a low number of clusters ($K$).
\end{enumerate}
Find the clustering $\{S_1^*,S_2^*,...,S_K^*\}$ that provides a \underline{global minimum} is \textbf{NP-hard}.

We use a \underline{heuristic} algorithm to find a \underline{local minimum} is good enough.

\subsection{Lloyd's Algorithm}
\begin{enumerate}
    \item \textbf{Input:}
    \subitem Desired number of clusters (ex: $K=3$)
    \subitem Dataset of $m$ objects $X=\{\vec{x}_1,\vec{x}_2,...,\vec{x}_m\}$, where each object $\vec{x}_i=(x_{i1},x_{i2},...x_{in})$ has $n$ numerical attributes. (We can also think of $X$ as being an $m\times n$ matrix $X_{m\times n}$.)
    \item \textbf{Algorithm:}
    \begin{enumerate}[$\bullet$]
        \item \textbf{\underline{Step 1:} Centroid Initialization Step}\\
        Randomly select $K$ centroids $\{\vec{\mu}_1,\vec{\mu}_2,...,\vec{\mu}_K\}$, where $\vec{\mu}_k=(\mu_{k1},\mu_{k2},...,\mu_{kn})$
        \item \textbf{\underline{Step 2:} Cluster Assignment Step}\\
        Assign each object $x_i$ in the dataset to it's \underline{closest} centroid (specifically the \textit{smallest squared euclidean distance})
        \item \textbf{\underline{Step 3:} Centroid Update Step}\\
        Find the \underline{mean} of each cluster created in step 2. These means are now the new \underline{centroids}.
        \item \textbf{\underline{Step 4:} Stopping Criterion}\\
        If the old centroids and the new centroids are the \underline{same}, stop the algorithm. Otherwise, go back to step 2.
    \end{enumerate}
    \item \textbf{Output:} Clustering with $K$ clusters $\{V_1,V_2,...,V_K\}$.
\end{enumerate}
Lloyd's algorithm is known as a \textbf{non-deterministic} algorithm because,
even with the same input, it can exhibit different behaviors on different runs.

\subsection{Benefits and Drawbacks}
\subsubsection*{Benefits}
\begin{enumerate}[$\bullet$]
    \item Fast algorithm.
    \item Computationally efficient.
    \item It scales well as the number of objects or attributes grows really large. (However, k-means is not great for "big data".)
    \item One of the easiest to understand.
\end{enumerate}
\subsubsection*{Drawbacks}
\begin{enumerate}[$\bullet$]
    \item Only works well with some types of data.\\
    The K-means algorithm works best for data when "the underlying clustering" of the data has the following properties:
    \begin{enumerate}[(1).]
        \item Each cluster has roughly the same number of objects;
        \item The clusters are spherical;
        \item The clusters have the same sparsity;
        \item There is good separation between the clusters;
        \item You know the right number of clusters to ask for;
        \item Attributes are numerical (non-categorical);
        \item Data does not have a lot of noise or outliers.
    \end{enumerate}
    (\textbf{Caveat:} Just because some of these assumptions are not met does not mean necessarily the algorithm will perform worse.)
    \item Need to know the "right" number of clusters to ask for in advance.
    (We use k-means elbow plot method)
    \item It is a non-deterministic algorithm.
\end{enumerate}

\subsection{Elbow Method}
\subsubsection*{Elbow Plot}
\begin{enumerate}
    \item For $k=1$ to $K$:
    \subitem[a] Cluster the data several times into $k$ clusters.
    \subitem[b] Calculate the average inertia of these resulting clusterings.
    \item Plot "k vs. average inertia".
\end{enumerate}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.5]{elbow plot.png}
    \caption{Elbow Plot Example}
    \label{}
\end{figure}\end{center}
\subsubsection*{Interpretation of Elbow Plot}
\begin{enumerate}
    \item If there is not a dramatic elbow, then this suggests that either:
    \subitem 1. The dataset is \underline{not clusterable} or
    \subitem 2. K-means is not a suitable algorithm for detecting the underlying clusters.
    \item If there is a dramatic elbow, then this suggests that:
    \subitem 1. There is a clustering structure and
    \subitem 2. The k-means clustering algorithm is suggesting that there are about $K$ clusters where the plot levels off.
\end{enumerate}
In the example of the figure\\
1. We see a somewhat dramatic elbow in the plot. This suggests that there is some clustering structure in the dataset and that k-means is capable of identifying some clustering structure.

2. We see that that plot levels off dramatically at k=2 clusters. So this suggests that asking the k-means algorithm to return k=2 clusters will be the most insightful.

\section{Types of Clusters Definitions}
As we know the K-means algorithm can only work well with data that fulfills specific properties, we define some common \textbf{types of clusters} that could be considered in a numerical dataset to help introduce our new algorithms.

\begin{definition}[Well-Separated Cluster]
    A \textbf{well-separated cluster} defines a cluster only when the data contains natural clusters that are \underline{far apart from each other}. (This definition is vague in how far apart do clusters have to be.)
\end{definition}
Why K-means may not work well?: Well-Separated Cluster can be non-spherical.
\begin{definition}[Density-Based Cluster]
    A \textbf{density-based cluster} defines a cluster as a \underline{dense} region of objects that is surrounded by a region of \underline{lower} density. (This definition is vague in how dense it needs to be considered a cluster.)
\end{definition}
Why K-means may not work well?: Density-Based Cluster can have noise.

\begin{definition}[Graph-Based Cluster]
    \textbf{Graph-based cluster} is a group of objects that are \underline{connected} to one another, but have no \underline{connection} to objects outside the group. (This definition is vague in how do we decide objects are connected.)
\end{definition}
Why K-means may not work well?: Graph-based cluster can be non-spherical and not well separated.

\begin{definition}[Contiguity-Based Cluster (a type of graph-based
    cluster definition)]
    In \textbf{contiguity-based cluster} (a type of graph-based
    cluster definition),  two objects are \underline{connected} only if they are within a \underline{specified distance} of one another.
\end{definition}
\underline{Types of contiguity-based clustering algorithms:} spectral clustering.
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.25]{well-separated.png}
    \includegraphics[scale=0.25]{density-based.png}
    \caption{(1). Well-Separated Cluster; (2). Density-Based Cluster}
    \includegraphics[scale=0.2]{Contiguity-based.png}
    \caption{Contiguity-Based Cluster}
\end{figure}\end{center}

\begin{definition}[Prototype-Based Cluster]
    A \textbf{prototype-based cluster} defines a cluster as a set of objects in which each object is closer (or more similar) to the \underline{prototype} (e.g. mean, median) that defines the cluster than to the \underline{prototype} of any other cluster.
\end{definition}
Why K-means may not work well?: Prototype-Based Cluster may be not well-separated and have outliers.
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.23]{prototype-based.png}
    \includegraphics[scale=0.18]{prototype-based2.png}
    \caption{Prototype-Based Cluster}
\end{figure}\end{center}
\underline{Types of prototype-based clustering algorithms:}
\begin{enumerate}[$\bullet$]
    \item \textbf{K-means}: Prototype is the mean of the cluster.
    \item \textbf{K-median}: Prototype is the median of the cluster.
\end{enumerate}


\section{K-Medians}
\subsection{K-Medians Clustering Optimization Problem}

\textbf{Goal of K-Means:}

Out of all possible clustering of $\{S_1 ,S_2 , ... , S_K\}$ with $K$ clusters that can be made from the $m$ objects in $X$, find the optimal clustering $\{S_1^*,S_2^*,...,S_K^*\}$ that \underline{minimizes} the sum of the \underline{Manhattan distances} (i.e., $L_1$ distances) of each object and the centroid (the \underline{median} of the cluster that object is assigned to).

Technically, we can write this as an optimization problem
\begin{equation}
    \begin{aligned}
        \{S_1^*,S_2^*,...,S_K^*\}=\argmin_{S_1,S_2,...,S_K}\sum_{k=1}^K\sum_{x\in S_k}\|x-c_k\|_1\\
        \textnormal{Optimal Inertia}=\min_{S_1,S_2,...,S_K}\sum_{k=1}^K\sum_{x\in S_k}\|x-c_k\|_1\\
    \end{aligned}
    \nonumber
\end{equation}

\subsection{K-Medians Heuristic Algorithm}
\begin{enumerate}
    \item \textbf{Input:}
    \subitem Desired number of clusters (ex: $K=3$)
    \subitem Dataset of $m$ objects $X=\{\vec{x}_1,\vec{x}_2,...,\vec{x}_m\}$, where each object $\vec{x}_i=(x_{i1},x_{i2},...x_{in})$ has $n$ numerical attributes. (We can also think of $X$ as being an $m\times n$ matrix $X_{m\times n}$.)
    \item \textbf{Algorithm:}
    \begin{enumerate}[$\bullet$]
        \item \textbf{\underline{Step 1:} Centroid Initialization Step}\\
        Randomly select $K$ centroids $\{\vec{c}_1,\vec{c}_2,...,\vec{c}_K\}$, where $\vec{c}_k=(c_{k1},c_{k2},...,c_{kn})$
        \item \textbf{\underline{Step 2:} Cluster Assignment Step}\\
        Assign each object $x_i$ in the dataset to its \underline{closest} centroid (specifically the \textit{smallest Manhattan distance})
        \item \textbf{\underline{Step 3:} Centroid Update Step}\\
        Find the \underline{median} of each cluster created in step 2. These medians are now the new \underline{centroids}.
        \item \textbf{\underline{Step 4:} Stopping Criterion}\\
        If the old centroids and the new centroids are the \underline{same}, stop the algorithm. Otherwise, go back to step 2.
    \end{enumerate}
    \item \textbf{Output:} Clustering with $K$ clusters $\{V_1,V_2,...,V_K\}$.
\end{enumerate}


\section{K-Medoids}
\begin{definition}[Medoid]
    In the context of clustering, we define a \textbf{medoid} as \underline{an actual object} in a cluster whose sum of distance to all the objects in the cluster is minimal.
\end{definition}
\subsection{K-Medoids Clustering Optimization Problem}

\textbf{Goal of K-Medoids:}

Out of all possible clustering of $\{S_1 ,S_2 , ... , S_K\}$ with $K$ clusters that can be made from the $m$ objects in $X$, find the optimal clustering $\{S_1^*,S_2^*,...,S_K^*\}$ that \underline{minimizes} the sum of the \underline{distances} (any distance metric) of each object and the centroid (the \underline{medoid} of the cluster that object is assigned to).

Technically, we can write this as an optimization problem
\begin{equation}
    \begin{aligned}
        \{S_1^*,S_2^*,...,S_K^*\}=\argmin_{S_1,S_2,...,S_K}\sum_{k=1}^K\sum_{x\in S_k}\textnormal{dist}(x,c_k)\\
        \textnormal{Optimal Inertia}=\min_{S_1,S_2,...,S_K}\sum_{k=1}^K\sum_{x\in S_k}\textnormal{dist}(x,c_k)\\
    \end{aligned}
    \nonumber
\end{equation}

\subsection{K-Medoids Clustering Algorithm}
\begin{enumerate}
    \item \textbf{Input:}
    \subitem Desired number of clusters (ex: $K=3$)
    \subitem Dataset of $m$ objects $X=\{\vec{x}_1,\vec{x}_2,...,\vec{x}_m\}$, where each object $\vec{x}_i=(x_{i1},x_{i2},...x_{in})$ has $n$ numerical attributes. (We can also think of $X$ as being an $m\times n$ matrix $X_{m\times n}$.)
    \item \textbf{Algorithm:}
    \begin{enumerate}[$\bullet$]
        \item \textbf{\underline{Step 1:} Centroid Initialization Step}\\
        Randomly select $K$ centroids $\{\vec{c}_1,\vec{c}_2,...,\vec{c}_K\}$, where $\vec{c}_k=(c_{k1},c_{k2},...,c_{kn})$
        \item \textbf{\underline{Step 2:} Cluster Assignment Step}\\
        Assign each object $x_i$ in the dataset to its \underline{closest} centroid (specifically the \textit{using distance metric you've chosen})
        \item \textbf{\underline{Step 3:} Centroid Update Step}\\
        Find the \underline{medoid} of each cluster created in step 2. These medians are now the new \underline{centroids}.
        \item \textbf{\underline{Step 4:} Stopping Criterion}\\
        If the old centroids and the new centroids are the \underline{same}, stop the algorithm. Otherwise, go back to step 2.
    \end{enumerate}
    \item \textbf{Output:} Clustering with $K$ clusters $\{V_1,V_2,...,V_K\}$.
\end{enumerate}

\subsection{K-Medoids vs. K-Means}
\subsubsection*{Benefit of K-Medoids over K-Means:}
\begin{enumerate}
    \item The medoid is more robust to outliers.
    \item Guaranteed to converge using any distance metric we want (K-means has to use squared euclidean distance).
\end{enumerate}
\subsubsection*{Benefit of K-Means over K-Medoids:}
K-Medoids is more computationally complex than k-means:
\begin{enumerate}
    \item K-means: $O\left(\textnormal{number of objects}\times \textnormal{number of attributes}\times \textnormal{number of clusters}\times \textnormal{number of iterations}\right)$
    \item K-medoids: $O\left((\textnormal{number of objects})^2\times \textnormal{number of attributes}\times \textnormal{number of clusters}\times \textnormal{number of iterations}\right)$
\end{enumerate}

\section{Types of Clustering Algorithms Results}
\subsection{Partitional vs. Hierarchical Clustering Results}
\begin{definition}[Partitional Clustering]
    We call a \textbf{partitional clustering} a division of the set of data objects into $k$ subsets (clusters) such that each object is in \underline{exactly one} subset.
\end{definition}
\begin{example}
    $\{1,2,8\},\{3,7\},\{4,5,6\}$
\end{example}

\begin{definition}[Hierarchical Clustering]
    In a \textbf{hierarchical clustering} we allow for clusters to have \underline{nested subclusters}.\\
    A hierarchical
    clustering is displayed as a set of nested clusters displayed as a \textbf{dendrogram} tree. The dendrogram can reflect which objects and clusters are closer to each other than others.
\end{definition}
\begin{example}
    $\{\{1,3\},\{\{\{6,9\},10\},\{11,15\}\}\},\{4,\{12,19\}\},\{\{\{2,14\},\{\{17,20\},18\}\},\{5,8\}\},\{7,\{13,16\}\}$.
\end{example}
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.25]{Hierarchical Clustering.png}
    \caption{Hierarchical Clustering Example}
    \label{}
\end{figure}\end{center}

\subsection{Exclusive vs. Overlapping vs. Fuzzy Clustering Results}
\begin{definition}
    \textbf{Exclusive Clustering} will assign an object to an \underline{exactly one cluster}.
\end{definition}
\begin{example}
    $\{1,3,5\},\{2,4\}$.
\end{example}
\begin{definition}
    \textbf{Overlapping Clustering} can allow for an object to be assigned to \underline{more than one cluster}.
\end{definition}
\begin{example}
    $\{1,3,5\},\{2,4,5\}$
\end{example}

\begin{definition}
    In a \textbf{Fuzzy Clustering} every object belongs to every cluster with a membership weight that is between $0$ (absolutely doesn't belong to the cluster) to $1$ (absolutely belongs).
    \begin{enumerate}
        \item Usually the sum of each object's weights must sum to $1$.
        \item $w_{ij}$ $=$ the probability that object $i$ belongs to cluster $j$.
    \end{enumerate}
\end{definition}








\chapter{Clustering Evaluation Metrics}
\section{Clusterability Evaluation Metric: Is the dataset clusterable?}
\begin{definition}
    A dataset is \textbf{clusterable} if there exist some distinct groupings of observations in a dataset.
\end{definition}
Then, how distinct do the observation need to be is a question.

\subsection{General Elbow Plot Method}
When determining whether a clustering structure can be detected by a particular clustering algorithm (K-means, K-medians, K-medoids), we can use an elbow plot that plots the value of the objective function that we are trying to minimize of the clusterings found by that particular clustering algorithm with $k=1,k=2,...$ clusters respectively.

The \textbf{drawback} of this method is each of these methods are dependent on the ability of a clustering algorithm to detect the clustering structure.

\subsection{Hopkin's Statistics}
\begin{enumerate}[$\bullet$]
    \item \textbf{\underline{Input:}} Dataset of $m$ objects $X=\{\vec{x}_1,\vec{x}_2,...,\vec{x}_m\}$, where each object $\vec{x}_i=(x_{i1},x_{i2},...x_{in})$ has $n$ numerical attributes.
    \item \textbf{\underline{How to calculate:}}
    \begin{enumerate}[$(1)$]
        \item Create a set of random \underline{artificial data point} closest distances $\{u_1,u_2,...,u_p\}$ as follows.
        \begin{enumerate}[a)]
            \item Generate $p$ \underline{random artificial data points} $\{\vec{y}_1, \vec{y}_2, ... , \vec{y}_p\}$ distributed across
            the range of the dataset.
            \item For each random artificial data points $i=1,...,p$ calculate $$u_i=\min_{\vec{x}\in X}\textnormal{dist}(\vec{y}_i,\vec{x})$$
        \end{enumerate}
        \item Create a set of random \underline{actual data point} closest distances $\{w_1,w_2,...,w_p\}$ as follows.
        \begin{enumerate}[a)]
            \item Random select $p$ \underline{actual points} $\{\vec{z}_1, \vec{z}_2, ... , \vec{z}_p\}$ from the dataset.
            \item For each randomly selected actual points $i=1,...,p$ calculate $$w_i=\min_{\vec{x}\in X}\textnormal{dist}(\vec{z}_i,\vec{x})$$
        \end{enumerate}
        \item $$\textbf{Hopkins Statistic}=\frac{\sum_{i=1}^p w_i}{\sum_{i=1}^p w_i+\sum_{i=1}^p u_i}$$
    \end{enumerate}
    \item \textbf{\underline{How to interpret:}} The dataset is \underline{clusterable} if the Hopkins Statistic \underline{close to $0$} and is \underline{not clusterable} if the Hopkins Statistic \underline{close to $0.5$}.
    \item \textbf{\underline{Intuition:}}
    \begin{center}\begin{figure}[htbp]
        \centering
        \includegraphics[scale=0.25]{HS.png}
        \caption{When Hopkins Statistic works well and not well}
        \label{}
    \end{figure}\end{center}
    \item \textbf{\underline{Additional Tips and Information:}} $p=10\%\times \textnormal{(the number of observations in the dataset)}$; Hopkins Statistic is \underline{non-deterministic} evaluation metric.
\end{enumerate}

\section{Unsupervised Clustering Evaluation Metrics: How cohesive and well separated are the clusters in the
clustering?}
\subsection{Definition}
\begin{definition}
    \textbf{Unsupervised clustering evaluation metrics} evaluate the goodness of the clustering without using pre-assigned class labels.
\end{definition}

\underline{Types of Unsupervised Clustering Evaluation Metrics:}
\begin{enumerate}
    \item \textbf{Cohesion} measures how closely related the objects in a cluster are.
    \item \textbf{Separation} measures how distinct or well-separated a cluster is from other clusters.
    \item \textbf{Validity of a clustering} can be expressed as some function of \textbf{cohesion} and \textbf{separation} of all the clusters in a clustering.
\end{enumerate}

\subsection{Graph-based view of cohesion and separation for a clustering}
A graph-based view of calculating cohesion and separation for a clustering involves first creating a \textbf{proximity matrix} (graph) of the objects in the dataset, that measures the “proximity” of each pair of objects in the dataset.
\begin{center}\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.18]{proximity matrix.png}
    \caption{Proximity Matrix}
    \label{}
\end{figure}\end{center}
\underline{Different ways to measure proximity:}
\begin{enumerate}
    \item \textbf{Similarity metric measure of proximity:} The more similar two objects are the lower the proximity measure is. e.g. Euclidean distance.
    \item \textbf{Dissimilarity metric measure of proximity:} The more similar two objects are the higher this proximity measure is. e.g. The number of attribute agreements between two categorical objects.
\end{enumerate}

Cohesion of a graph-based cluster is the sum of proximities between all pairs of points within the same cluster.
\begin{equation}
    \begin{aligned}
        \textnormal{cohesion}(C_i)=\sum_{\vec{x}\in C_i,\vec{y}\in C_i}\textnormal{proximity}(\vec{x},\vec{y})
    \end{aligned}
    \nonumber
\end{equation}



\section{Cluster Number Evaluation Metrics: What is the 'correct' number of clusters?}




\section{Supervised Clustering Evaluation Metrics: How similar is the clustering to a set of (external) pre-assigned class labels?}




\section{Clustering Comparison Metrics: Which clustering is better for a given dataset?}




















































\end{document}