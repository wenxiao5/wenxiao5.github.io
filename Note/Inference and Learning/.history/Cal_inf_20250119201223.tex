\documentclass[11pt]{elegantbook}
\usepackage{graphicx}
%\usepackage{float}
\definecolor{structurecolor}{RGB}{40,58,129}
\linespread{1.6}
\setlength{\footskip}{20pt}
\setlength{\parindent}{0pt}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\elegantnewtheorem{proof}{Proof}{}{Proof}
\elegantnewtheorem{claim}{Claim}{prostyle}{Claim}
\DeclareMathOperator{\col}{col}
\title{Causal Inference}
\author{Wenxiao Yang}
\institute{Haas School of Business, University of California Berkeley}
\date{2025}
\setcounter{tocdepth}{2}
\extrainfo{All models are wrong, but some are useful.}

\cover{cover.png}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}


\bibliographystyle{apalike_three}

\begin{document}
\maketitle

\frontmatter
\tableofcontents

\mainmatter



\chapter{Causal Inference}
The fundamental problem of causal inference:
\begin{enumerate}[(a).]
    \item Never see the same person treated and untreated
    \item Missing data problem
    \item "Solve" by finding a comparison group
\end{enumerate}

\begin{definition}[Notations and Estimands]
    \normalfont
    \begin{enumerate}[$\circ$]
        \item Treatment: $T\in\{0,1\}$
        \item Potential Outcome with treatment $Y(1), Y(0)$
        \item Other Variable $X$
        \item Individual Treatment Effect (ITE) $= Y_i(1)-Y_i(0)$
        \item Conditional Average Treatment Effect (CATE) $=\mathbb{E}[Y(1)-Y(0)|X=x]:=\tau(x)$
        \item Average Treatment Effect (ATE) $=\mathbb{E}[Y(1)-Y(0)]:=\tau$
        \item Average Treatment Effects on Treated (ATT) $=\mathbb{E}[Y(1)-Y(0)\mid T=1]$
    \end{enumerate}
\end{definition}


\subsection*{Difference in Means}
\begin{equation}
    \begin{aligned}
        \hat{\tau}=\bar{Y}_1-\bar{Y}_0=\frac{1}{n_1}\sum_{i=1}^n Y_i T_i - \frac{1}{n_0}\sum_{i=1}^n Y_i(1-T_i)
    \end{aligned}
    \nonumber
\end{equation}

By the Law of Large Numbers,
\begin{equation}
    \begin{aligned}
        \lim_{n \rightarrow \infty}\frac{1}{n_1}\sum_{i=1}^n Y_i T_i&=\lim_{n \rightarrow \infty}\frac{n}{n_i}\frac{1}{n}\sum_{i=1}^n Y_i T_i\\
        &=\left(P[T=1]\right)^{-1} \mathbb{E}[YT]\\
        &= \left(P[T=1]\right)^{-1}\mathbb{E}[YT\mid T=1]P[T=1]\\
        &=\mathbb{E}[YT\mid T=1]\\
        \bar{Y}_1 &\stackrel{P}{\longrightarrow} \mathbb{E}[YT\mid T=1]
    \end{aligned}
    \nonumber
\end{equation}

\subsubsection*{Causal Effect}
\begin{assumption}\quad
    \begin{enumerate}[(1).]
        \item SUTVA: Only your treatment matters;
        \item Consistency: Observed outcome matches treatment "assignment": $Y=TY(1)+(1-T)Y(0)$.
    \end{enumerate}
\end{assumption}

Only yields $\hat{\tau}=\bar{Y}_1-\bar{Y}_0\stackrel{P}{\longrightarrow} \mathbb{E}[Y(1)\mid T=1]-\mathbb{E}[Y(0)\mid T=0]$

\begin{equation}
    \begin{aligned}
        &\mathbb{E}[Y(1)\mid T=1]-\mathbb{E}[Y(0)\mid T=0]\\
        =&\underbrace{\mathbb{E}[Y(1)\mid T=1]-\mathbb{E}[Y(0)\mid T=1]}_{\textnormal{ATT}}+\underbrace{\mathbb{E}[Y(0)\mid T=1]-\mathbb{E}[Y(0)\mid T=0]}_{\textnormal{selection bias}}\\
    \end{aligned}
    \nonumber
\end{equation}
To get the ATT (eliminate the selection bias), we need exclusion/independence: Randomization.


Assume $Y(t)=\mu(t)+\epsilon_t$ (SUTVA). Consider the consistency assumption:
\begin{equation}
    \begin{aligned}
        Y&=TY(1)+(1-T)Y(0)\\
        &=Y(0)+T(Y(1)-Y(0))\\
        &=\underbrace{\mu_0}_\alpha+T\underbrace{(\mu_1-\mu_0)}_{\beta^T}+\underbrace{\epsilon_0+T(\epsilon_1-\epsilon_0)}_{\epsilon}
    \end{aligned}
    \nonumber
\end{equation}

Consider the covariate between $T$ and $X$. Why important?

\begin{equation}
    \begin{aligned}
        &\mathbb{E}[Y|X=1,T=1]-\mathbb{E}[Y|X=1,T=0]\\
        =& \underbrace{\mathbb{E}[Y(1)|X(1)=1]-\mathbb{E}[Y(0)|X(1)=1]}_{\textnormal{ATE}|X(1)=1} + \underbrace{\mathbb{E}[Y(0)|X(1)=1]-\mathbb{E}[Y(0)|X(0)=1]}_{\textnormal{selection bias}}\\
    \end{aligned}
    \nonumber
\end{equation}


$Y(t)=\mu(t,X)+\epsilon_t$. Then,
\begin{equation}
    \begin{aligned}
        Y&=Y(1)T+Y(0)(1-T)\\
        &=\underbrace{\mu(0,X)}_{\alpha(X)} + T\underbrace{(\mu_1(X)-\mu_0(X))}_{\beta(X)}+\epsilon
    \end{aligned}
    \nonumber
\end{equation}



\chapter{STAT 256}
\section{Lecture Zero: Causal Inference vs Association}{Amanda Coston}

\subsection{Causal Inference on the Effect of Red Wine on Health}\label{sec:intro}

\subsection{Is red wine good for heart health?}
For many years consensus held that red wine improved cardiovascular health. The evidence behind this was largely from studies on people of drinking age that compared the health outcomes of those who self-reported that they drank wine to those who self-reported that they did not. Findings under such a design showed that people who drink  red wine have better cardiovascular outcomes than those who don't drink alcohol.

What are potential problems with this study design? You may be thinking that people who drink wine systematically differ from those who don't in ways that matter for health outcomes. In fact, sociologist Kaye Middleton Fillmore showed that the statistical significance of these findings hinged on the inclusion of \emph{previous} drinkers in the ``non-drinkers" category. That is, some people who said they did not drink red wine previously drank alcohol. Problematically, a common reason these people gave up alcohol was poor health. Therefore the definition of  ``non-drinkers" selected for people who had poorer health outcomes. Fillmore showed that redefining ``non-drinkers" to be ``never-drinkers" eliminated any supposed advantage of drinking wine.

The debate here was one of causal inference -- looking at the cause and effect of red wine on health.

For more details, see \cite{fillmore2007moderate}.

\subsection{Syllabus}

See course syllabi here: \href{https://stat156.berkeley.edu/fall-2024/syllabus.html}{https://stat156.berkeley.edu/fall-2024/syllabus.html}.

\subsection{Association vs Causation}

Association is the focus of much of statistics but in causal inference our focus is, of course, on causation! As a starting point, we will today consider common measures of association and discuss why they may not capture causation. In the next lecture we will see causal analogues of these measures.
aaa
We first consider the setting where the outcome $Y$ and treatment $Z$ are both binary. 

\begin{definition}[Risk Difference]
    The associative risk difference (RD) is $E[Y \mid Z = 1] - E[Y \mid Z = 0]$.
\end{definition}


\begin{definition}[Risk Ratio]
    The associative risk ratio (RR) is $P[Y =1 \mid Z = 1]/P[Y =1 \mid Z = 0]$.
\end{definition}

\begin{definition}[Odds Ratio]
    The associative odds ratio (OR) is $\frac{P[Y =1 \mid Z = 1]}{P[Y =0 \mid Z = 1]}/\frac{P[Y=1 \mid Z = 0]}{P[Y=0 \mid Z = 0]}$.
\end{definition}

Which measure one chooses depends on their particular setting -- what question they are interested in and what data they have available (odds ratios can be estimated in with outcome-dependent sampling whereas the risk difference and risk ratio generally cannot). The measures are related to each other as follows:

\begin{enumerate}
    \item $Z \perp Y  \Longleftrightarrow \mathrm{RD} = 0 \Longleftrightarrow \mathrm{RR} = 1 \Longleftrightarrow \mathrm{OR} = 1$
    \item $\mathrm{RD} > 0 \Longleftrightarrow \mathrm{RR} > 1 \Longleftrightarrow \mathrm{OR} > 1$ assuming all conditional probabilities are non-zero.
    \item $\mathrm{RR} \approx \mathrm{OR}$ when $P(Y=1)$ is small.
\end{enumerate}

Next we consider measures of association that can accommodate non-binary outcomes. 

\subsection{Correlation and linear regression}
Suppose we are now interested in the outcome blood pressure as a measure of cardiovascular health. A natural starting point is to model the relationship between blood pressure ($Y$) and whether one drinks red wine ($Z$) as 

\begin{align*}
    Y = \beta Z + \alpha + \epsilon
\end{align*}

where $E[\epsilon] = 0 $ and $E[\epsilon Z ] = 0$.

Recall that we can relate $\beta$ to the Pearson correlation coefficient $\rho$ as follows

\begin{align*}
    \beta = \rho \frac{\mathrm{var}(Z)}{\mathrm{var}(Y)}.
\end{align*}

The coefficient $\beta$ describes the change in $Y$ associated with whether one drinks red wine. More generally, the coefficient $\beta$ describes the change in $Y$ associated with one unit increase in $Z$.
Sometimes people refer to $\beta$ as the ``effect" of $Z$ on $Y$ but this is generally misleading (without further assumptions). We have simply modeled an associative relationship; we can't claim anything causal yet! Next time we will introduce a new language, potential outcomes, so that we can make causal claims.


\section{Lecture One: Potential Outcomes Framework}{Aryan Shafat, Frederik Stihler, Mika Lee (Revisions)}

\subsection{Review of last lecture}\label{sec:review}
Review of Risk Difference, Risk Ratio and Odds Ratio for setting where the outcome $Y$ and treatment $Z$ are both binary. (See lecture notes of Lecture 0 for details)

Explanation of the last equivalence in the following statement:
\begin{center}
$\mathrm{RD} > 0 \Longleftrightarrow \mathrm{RR} > 1 \Longleftrightarrow \mathrm{OR} > 1$ assuming all conditional probabilities are non-zero
\end{center}

We know that:

\begin{center}
    $\mathbb{P}[Y = 1 \mid Z = 1] + \mathbb{P}[Y = 0 \mid Z = 1] = 1$ and
    $\mathbb{P}[Y = 1 \mid Z = 0] +$ $\mathbb{P}[Y = 0 \mid Z = 0] = 1$. 
\end{center}    
    $\mathrm{RR} > 1$ implies that $\mathbb{P}[Y = 1 \mid Z = 1] > \mathbb{P}[Y = 1 \mid Z = 0]$, and hence $\mathbb{P}[Y = 0 \mid Z = 0] > \mathbb{P}[Y = 0 \mid Z = 1]$. This leads us to $\mathrm{OR} > 1$.

\subsection{Potential Outcome Framework}\label{sec:pof}

\subsubsection{Types of questions we are interested in}

\begin{enumerate}
    \item Does journaling reduce the risk of depression?
    \item Do personalized AI tutors improve a student's grade?
    \item Does smoking cigarettes cause cancer?
\end{enumerate}

We can summarize these questions in the following table: \par
\begin{center}
\begin{tabular}{ccc}
  \hline
  & \textbf{$Z$} & \textbf{$Y$} \\
  \hline
  1 & Journal & Depression \\
  \hline
  2 & AI tutor & Grade  \\
  \hline
  3 & Cigarettes & Cancer  \\
  \hline
\end{tabular}
\end{center}

For all the examples, we want to form a causal estimand using potential outcomes. The potential outcome framework was developed by \cite{neyman1923} in 1923 over one hundred years ago and later revitalized and repopularized by \cite{rubin1980} in 1980. 

\subsubsection{Potential Outcomes}

We are interested in potential (hypothetical) outcomes when we are thinking about causal questions.

\begin{definition}[Potential Outcome]
    The potential outcome Y is a function of a particular treatment value $Y(Z=z)$.
\end{definition}

E.g. $Y(Z=1) = Y(1)$ (the potential outcome under the intervention that assigns treatment) vs. $Y(Z=0) = Y(0)$ (the potential outcome under the intervention that assigns no treatment).

In our particular examples, these quantities are represented by the following hypothetical questions: \par

(1) Would someone have depression if they journalled $\approx Y(1)$ \\
(2) Would a  student get a particular grade if they had an AI tutor $\approx Y(1)$ \\
(3) Would someone get cancer if they smoked cigarettes $\approx Y(1)$

\subsubsection{Causal Estimands}

Next we define the causal versions of our measures of association (called  causal estimands).

\begin{definition}[Causal Risk Difference]
    The causal risk difference (on a population level) is $E[Y(1) - Y(0)]$.
\end{definition}

\begin{definition}[Causal Risk Ratio]
    The causal risk ratio is $\frac{E[Y(1)]}{E[Y(0)]} = \frac{P(Y(1) = 1)}{P(Y(0) = 1)}$.
\end{definition}

\begin{definition}[Causal Odds Ratio]
    The causal odds ratio is $\frac{P(Y(1) = 1)}{P(Y(1) = 0)}/\frac{P(Y(0) = 1)}{P(Y(0) = 0)} = \\ 
    \frac{E[Y(1)]}{E[1-Y(1)]}/\frac{E[Y(0)]}{E[1-Y(0)]}$.
\end{definition}

\subsubsection{Hidden Assumptions}

As mentioned, \cite{rubin1980} repopularized this framework by clarifying some important hidden assumptions:

\begin{assumption}[Consistency]
The treatment levels are well-defined (there are no other versions of the treatment).
\end{assumption}

\begin{assumption}[No Interference]
The treatment assigned to other units does not affect the potential outcomes for unit $i$ (no spillover).
\end{assumption}

These 2 assumptions are called \textbf{Stable Unit Treatment Value Assumption (SUTVA).}

For example, our 3rd question/example about cigarette smoking violates both assumptions.

The question isn't well-defined (does smoking entail  smoking 1 cigarette a day or smoking a whole pack a day) and thus violates Assumption 1.

It also violates Assumption 2, through non-smokers who might end up passively smoking by being around smokers.

\subsection{Causal Estimands}
Causal Effects are functions of potential outcomes. For example, the causal risk difference is essentially the 'average treatment effect'.

\begin{itemize}
    \item Unit 'i' has 2 potential outcomes: $Y_i$(1) and $Y_i$(0)
\end{itemize}

\begin{itemize}
    \item \textbf{Individualized treatment effect:} $Y_i$(1) - $Y_i$(0)
\end{itemize}

\textbf{Fundamental Problem of Causal Inference (1986 Holland):}
Never observe both potential outcomes.

\subsubsection{Add a time-element}
We could include a time element - write into our personal journal/smoke a cigarette/receive treatment one day and then go without treatment the next day, to see the 'causal effect' of the treatment.

However, the effect of the treatment might be long-lasting. Thus, one could 'experience'/observe their causal effects even on the days without treatment. Additionally, say for a time-period of 2 days, we actually end up having 4 potential outcomes:

\begin{itemize}
    \item $Y_{i, day 1}$(1) vs $Y_{i, day 1}$(0)
\end{itemize}

\begin{itemize}
    \item $Y_{i, day 2}$(1) vs $Y_{i, day 2}$(0)
\end{itemize}


Going back to the potential outcomes framework, we basically end up 'observing' one of the potential outcomes framework.

\begin{itemize}
    \item The factual/observed outcome is, $Y_i = 
\begin{cases}
Y_i(1) & \text{if } Z_i = 1 \\
Y_i(0) & \text{if } Z_i = 0
\end{cases}
$ 
\end{itemize}

Equivalently, 

$Y_i = Z_i*Y_i(1) + (1-Z_i)*Y_i(0)$

\begin{itemize}

\item The (unobserved) counterfactual, or missing potential outcome is given by:

$Y_i^{\text{mis}} = Z_i*Y_i(0) + (1-Z_i)*Y_i(1)$

\end{itemize}

\subsection{Simpson's Paradox}
Based on the class poll, we saw that the effect of the hint wasn't that strong. There was a confounding variable (row number) that was dampening the effect of the hint (since most of the hints were given to people in the rows at the back). This was attributed to \textbf{Simpson's Paradox}, which might have made it seem like the hints had an \textbf{effect reversal} (as if those who got the hints actually ended up doing worse on the poll).

This is mathematically shown as:
$\mathbb{P}(Y=1|Z=0) > \mathbb{P}(Y=1|Z=1)$. However, we can easily counter this by conditioning on the confounding variable (X):

\begin{center}
    $\mathbb{P}(Y=1|Z=0, X=x) < \mathbb{P}(Y=1|Z=1, X=x) \quad  \forall x \in X $
\end{center}

\subsubsection{Sources of the paradox:}
\begin{itemize}
    \item Confounding variables/factors
    \item Non-collapsibility
\end{itemize}

\section{Lecture Three: Randomized Experiments}{Daisy Wang \& Mika Lee (Revisions)}

\subsection{Last Lecture: Simpson's Paradox}
    Simpson's paradox is when the data may originally appear to have one trend, but not when grouped. In stats terms:
    
\begin{center}
    $\mathbb{E}[Y \mid Z=1] - \mathbb{E}[Y \mid Z=0] > 0$ 
\end{center}
but is actually 
\begin{center}
    $\mathbb{E}[Y \mid Z=1, X=x] - \mathbb{E}[Y \mid Z=0, X=x] <0$ 
\end{center} 
when we condition on the confounding variable X. This is caused by confounding and non-collapsibility.

    \textbf{Note on notation:} Potential outcomes $Y(Z=1) = Y(1)$ and in Hernan, $Y^{Z=1} = Y^1$

\subsection{Randomized Experiments}
    Why is randomization so powerful? 
    \begin{itemize}
        \item\textit{\textbf{Ignorable}} treatment assignment
        \item Groups are \textit{\textbf{exchangeable}}, in that groups could swap assignments and still have the same result
    \end{itemize}

\subsubsection{Exchangeability}
$\boldsymbol{\Pr{Y(1) = 1 \mid Z = 1}}= \Pr{Y(1) = 1 \mid Z = 0}$

$\Pr{Y(0) = 1 \mid Z = 1} = \Pr{Y(0) = 1 \mid Z = 0} =$
$ 
\boldsymbol{\Pr(Y = 1 \mid Z = 0)}$

where the two bolded equations are identifiable from the data. 

    Additionally $Y(Z) \perp Z$  $\forall  z = 0, 1$ which means treatment assignment is independent of potential outcome. In general, treatment assignment is exchangeable which implies ignorable which implied exogenous.

    In an \textbf{ideal random experiment}, association would be equal to causation:
\begin{center}$\mathbb{E}[Y \mid Z=1] = \mathbb{E}[Y(1)\mid Z=1]$ \end{center}
    Additionally:
\begin{center} Risk Difference $\mathbb{E}[Y \mid Z=1] - \mathbb{E}[Y\mid Z=0]$ = Causal Risk Difference $\mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]$\end{center}
In tandem with the previous part about the independence of random treatment assignment, it is important to note that $Z \perp 
 Y(Z) \neq Z\perp Y$,  in which the relationship on the left side of the inequality contains random treatment assignment and the one on the right side references no treatment assignments.

    Finally a \textbf{randomized experiment} is where treatments are assigned in a known and probabilistic ("random") manner i.e. Bernoulli random experiments.

 \subsubsection{Completely Randomized Experiment}
    We denote those who get the treatment as $n_1$ , and those who get the control as $n_0 = n - n_1$. The treatment assignment is denoted as $\mathbb{Z}= (z_1, ... z_n)$.  Then the probability that $\Pr(\mathbb{Z}=z ) = \frac{1}{\binom{n}{n_1}}$ where $\mathbb{Z}$ is such that $\sum_{i=1}^{n} z_i = n_1$. Also keep in mind that $\mathbb{Y}(1)$ and $\mathbb{Y}(0)$ are fixed in this situation.

\subsubsection{Fisher (1935) Fisher's Sharp Null}
    $H_0: Y_i(1) = Y_i(0)$ for all $i = 1,...n$ where $\mathbb{Y} = \mathbb{Y}(1) = \mathbb{Y}(0)$ and the test statistic is $T(Z, \mathbb{Y})$. Here the $Z$ is random and $\mathbb{Y}$ is fixed. This is also known as Neyman's Null Hypothesis (Weak Null).
    
    Under the null, $\{T(z^1, \mathbb{Y})...T(z^{\binom{n}{n_1}}, \mathbb{Y}\}$ is uniform, representing the randomization distribution. Thus we can find the p-value:
    \begin{center} 
    $p = \frac{1}{\binom{n}{n_1}} \sum_{m=1}^{\binom{n}{n_1}} \mathbf{1}\{T(z^m,y) \geq T(Z, Y)\}$ 
    \end{center}
    We can also use the Monte Carlo method to approximate the p-value:
     \begin{center} 
     $\frac{1}{R} \sum_{r=1}^{R} \mathbb{1}\{T(z^r,y) \geq T(Z, Y)\}$ 
     \end{center}
     where $T(z^r,y) $ is a particular fixed value and $T(Z, Y)$ is an observed random variable.
     All of this is the \textbf{Fisher's randomization test}, aka \textbf{permutation test}.
     
 \subsubsection{Choices for the Test-Statistic}
 \paragraph{Difference-in-means statistic} 
$\hat{\tau} = \hat{\bar{Y}}(1) - \hat{\bar{Y}}(0)$ where $\hat{\bar{Y}}(1) = \frac{1}{n_1}\sum_{z_i=1}Y_i = \frac{1}{n_1}\sum_{i=1}z_iY_i $ 

\textbf{Note:} this statistic is easily ruined by outliers

\paragraph{Wilcoxon Rank Sum}
    Unlike the difference-in-means statistic, the Wilcoxon rank sum test statistic is robust to outliers. This is because it is defined as follows: 
    \begin{center} 
    $R_i:$ the rank of $Y_i = \#\{j: Y_j \leq Y_i\}$, and so the test statistic itself is  $W=\sum_{i=1}^{n}z_iR_i$ 
    \end{center}
    The test statistic is more broad and may miss distributional differences. It can also be viewed as the difference-in-means of the rank under treatment vs control. 

\paragraph{Kolmogorov-Smirnov Statistic}
The empirical CDF of treated units is $\hat{F_1}(y) = \frac{1}{n_1}\sum_{i=1}^n z_1 \mathbf{1}\{Y_i \leq y\}$  and the control is $\hat{F_0}(y) = \frac{1}{n_0}\sum_{i=1}^n (1-z_1) \mathbf{1}\{Y_i \leq y\}$. The test statistic is then
\begin{center} $D=\underset{y}{\max}|\hat{F_1}(y) - \hat{F_0}(y)|$ \end{center}






\section{Lecture Four: Continue on Randomized Experiments}{Yulin Zhang \& Aadya Agarwal}
\subsection{Last Lecture: Complete Randomized Experiments}
    \begin{itemize}
        \item Complete randomized experiment
        \begin{itemize}
            \item \textit{terms}: let $n$ denotes the total number of test units, $n_1$ denotes the number of units get treated, $n_0 = 1 - n_1$ denotes the number of units get control.
        \end{itemize}
        \item exchangability: $y(z) \perp z$, where $z$ denotes treatment, $y(z)$ denotes the outcome of the treatment.
        \item Fisher's sharp null
    \end{itemize}
    
\subsection{Neymanian estimation \& inference}

\subsubsection{Finite population counterfactual (potential) quantities}
Let $y_{i}(1)$ and $y_{i}(0)$ denote the potential outcomes of the $i$th unit under treatment and control. Define the \textit{individual casual effect} of the $i$th unit as $$\tau_{i} = y_{i}(1) - y_{i}(0)$$
\textbf{Theoretical population level statistics}:\\
$$\bar{y}(1)= \frac{1}{n}\sum_{i = 1}^{n}y_i(1)$$
$$\bar{y}(0)= \frac{1}{n}\sum_{i = 1}^{n}y_i(0)$$
$$S^{2}(1) = \frac{1}{n - 1}\sum_{i = 1}^{n}(y_i(1) - \bar{y}(1))^{2}$$
$$S^{2}(0) = \frac{1}{n - 1}\sum_{i = 1}^{n}(y_i(0) - \bar{y}(0))^{2}$$
$$S(1, 0) = \frac{1}{n - 1}\sum_{i = 1}^{n}(y_i(1) - \bar{y}(1))(y_i(0) - \bar{y}(0))$$
$$\tau = \frac{1}{n}\sum_{i = 1}^{n} \tau_{i} = \bar{y}(1) - \bar{y}(0)$$
$$S^{2}(\tau) = \frac{1}{n - 1}\sum_{i = 1}^{n}(\tau_{i} - \tau)^2$$

\textbf{Lemma}: $2S(1, 0) = S^{2}(1) + S^{2}(0) - S^{2}(\tau)$ \\

\textbf{In the data}:
$$\hat{\bar{y}}(1) = \frac{1}{n_1}\sum_{i = 1}^{n}z_{i}y_{i}$$
$$\hat{\bar{y}}(0) = \frac{1}{n_0}\sum_{i = 1}^{n}(1 - z_{i})y_{i}$$
$$\hat{S^{2}}(1) = \frac{1}{n_1 - 1}\sum_{i = 1}^{n}z_i(y_i - \hat{\bar{y}}(1))^{2}$$
$$\hat{S^{2}}(0) = \frac{1}{n_0 - 1}\sum_{i = 1}^{n}(1 - z_i)(y_i - \hat{\bar{y}}(0))^{2}$$
Note, since we could not observe $y_i(1)$ and $y_i(0)$ at the same time, no other theoretical statistics could be calculated at this point.

\subsubsection{Neyman (1993) theorm}
\begin{itemize}
    \item $\hat{\tau} = \hat{\bar{y}}(1) - \hat{\bar{y}}(0)$ satisfies:
    \begin{itemize}
        \item unbiasedness: $\mathbb{E}(\hat{\tau}) = \tau$
        \item has variance: $$\mathbb{V}(\hat{\tau}) = \frac{S^{2}(1)}{n_1} + \frac{S^{2}(0)}{n_0} - \frac{S^{2}(\tau)}{n} = \frac{n_0}{n_{1}n}S^{2}(1) + \frac{n_1}{n_{0}n}S^{2}(0) + \frac{2}{n}S(1, 0)$$
        \item Note in variance terms, $S^{2}(\tau)$ and $S(1, 0)$ are not identifiable from the data, this variance could not be calculated from the data directly.
    \end{itemize}
    \item Neyman's variance estimator: $$\hat{\mathbb{V}}(\hat{\tau}) = \frac{\hat{S^{2}}(1)}{n_1} + \frac{\hat{S^{2}}(0)}{n_0}$$
    \begin{itemize}
        \item Note: this is the usual variance estimator under the assumption that the population is infinite and random samples. However, this is not the case we have here: the population is finite and we do not have random samples (treatment and controls are usually mutually exclusive).
        \item In the paper, Neyman proved that $$\mathbb{E}(\hat{V}) - \mathbb{V}(\hat{\tau}) = \frac{S^{2}(\tau)}{n} \ge 0$$ where equality holds when $\tau_{i} = \tau $ $\forall i$. This shows that the estimator above is conservative for estimating the variance of $\hat{\tau}$. People call this a "minor miracle that the two mistakes cancel" (Freeman, Pisani \& Purues, 2006)
    \end{itemize}
    \item Regression analysis of the complete randomized experiment \\
    In OLS settings, we could get $(\hat{\alpha}, \hat{\beta}) = argmin_{a, b}\sum_{i = 1}^{n} (y_i - a - bz_i)^2$, where $\hat{\beta} = \hat{\tau}$
    \begin{itemize}
        \item Usual OLS variance: $\hat{\mathbb{V}}_{OLS}(\hat{\beta}) = \hat{\mathbb{V}}_{OLS}(\hat{\tau}) \approx \frac{\hat{S^{2}}(1)}{n_0} + \frac{\hat{S^{2}}(0)}{n_1}$, note this is not equal to the variance we want. 
        \item Eicker-Huber-White (EHW) Robust Variance Estimator: $\hat{\mathbb{V}}_{EWH}(\hat{\tau}) \approx \frac{\hat{S^{2}}(1)}{n_1} + \frac{\hat{S^{2}}(0)}{n_0}$
        \item HC2 variant of EHW $= \frac{\hat{S^{2}}(1)}{n_1} + \frac{\hat{S^{2}}(0)}{n_0}$
    \end{itemize}
\end{itemize}

\subsection{Stratified (Conditional) Random Experiments}
\subsubsection{A potential problem with completely randomized experiment}
Bias could be introduced if the different covariate strata have non-random treatments assigned. 
\begin{itemize}
    \item Let $x_i \in \{1..k\}$ denote a covariate that we are interested in.
    \item Covariate imbalance: bias would be introduced if the proportion of units in stratum $k$ are different across treatment and control groups. 
\end{itemize}
\subsubsection{Definition}
Let $n_{k, 1} = \# \{i: x_i = k, z_i = 1\}$, $n_{k, 0} = \# \{i: x_i = k, z_i = 0\}$. $n_{k} = \# \{i: x_i = k\}$ \\
Stratified random experiment (SRE) is to run $k$ independent complete random experiments (CRE) within k strata of discrete covariate $x$ for fixed $n_{k, 1}$, $n_{k, 0.}$ \\
We could view stratum as block, stratified randomization could also be called block randomization. \\
$$\tau_{k} = \frac{1}{n_k}\sum_{x_i = k} \tau_{i}$$
$$\tau = \frac{1}{n}\sum_{i = 1}^{n}\tau_i = \frac{1}{n}\sum_{k = 1}^{k}\sum_{x_i = k}\tau_i = \sum_{k = 1}^{k}\pi_{k}\tau_{k}, \pi_k = \frac{n_k}{n}$$
\textbf{Comparing SRE and CRE}:
\begin{itemize}
    \item Number of potential assignments: $SRE: \prod_{k = 1}^{k} \binom{n_k}{n_{k, 1}} < CRE: \binom{n}{n_1}$
    \item Propensity score (propensity of getting treatment): $e_k = \frac{n_{k, 1}}{n_k}$. For SRE, $e_k$ is fixed; for CRE, $e_k$ is random.
\end{itemize}

\subsubsection{Fisher Randomization Test}
$H_0: Y_i(0) = Y_i(1)$ for all i \\
Run Fisher Randomization test by permuting the treatment indicators within each strata X according to the fixed values of treatment assigned for each of those strata \\
This process is called the \textbf{Conditional Randomization Test or Conditional Permutation test}\\
\\
Test Statistic Choice: \\
\\
\textbf{1. Stratified Estimator\\}
$\hat{\tau}_s = \sum_{k=1}^{K} \pi_k \hat{\tau}_k$\\
\\
\textbf{2. Combined Wilcoxon-Rank Sum Statistic\\}
\\
$W_k$ : Wilcoxon rank-sum statistic in stratum k
\\$W_s = \sum_{k=1}^{k}C_k W_k$
\\$C_k = \frac{1}{n_k,_1 n_k,_0}$ OR $C_k = \frac{1}{n_k + 1}$\\
\\Note: Works well only if k is small
\\
\\\textbf{3. Aligned Rank Statistic (Hodges and Lehmann)} \\
\\$\tilde{Y}_i = Y_i - \bar{Y}_k$ where $Y_k = \frac{1}{n-k} \sum_{X_i = k} Y_k$ (stratum specific mean)
\\Let $\tilde{Y}_i$ be the rank of $\tilde{Y}_i$
\\ $\tilde{W} = \sum_{i=1} ^ {n} Z_i \tilde{R}_i$

\subsubsection{Neymanian Inference for SRE}

$\mathbb{E}[\hat{\tau}_k] - {\tau}_k = 0$
\\$\hat{\tau}_k$ has variance:
Var($\hat{\tau}_k$) = $\frac{S_k^2 (1)}{n_k,1} + \frac{S_k^2 (0)}{n_k,0} - \frac{S_k^2 (\tau)}{n_k} $
\\where $S_k^2(1), S_k^2(0), S_k^2(\tau)$ are the stratum specific analogs (stratum specific variance of potential outcomes and causal effects)
\\We can combine these to get a stratified estimator: $\hat{\tau}_s = \sum_{k=1}^{k} \pi_k \hat{\tau}_k$ where $\pi_k = \frac{n_k}{n}$ and 
\begin{itemize}
    \item $\mathbb{E}[\hat{\tau}_s] - \tau = 0$ (unbiased)
    \item has variance var($\hat{\tau}_s$) = $\sum_{k=1}^{k} \pi_k^2 var(\hat{\tau}_k)$
\end{itemize}
If $n_k,1 >= 2$ and $n_k,0 >= 2$, we can compute $\hat{S_k^2}(1)$, $\hat{S_k^2}(0)$
\\$\hat{V_s} = \sum_{k=1}^{k} \pi_k^2 (\frac{\hat{S_k^2}(1)}{n_k,1} + \frac{\hat{S_k^2}(0)}{n_k,0})$
\\$\hat{\tau_s} +- Z_{1-\frac{\alpha}{2}} \sqrt{\hat{V_s}}$
\\$H_o$ : $\tau = 0 $ \quad $\epsilon_s = \frac{\hat{\tau_s}}{\sqrt{\hat{V_s}}}$

\subsubsection{SRE vs CRE}
$e_k = e$ for all k, $\hat{\tau} = \hat{\tau_s}$
\\$var_{CRE} (\hat{\tau}) - var_{SRE} (\hat{\tau_s}) = \sum_{k=1}^{k}\frac{\pi_k}{n} (\sqrt{\frac{n_0}{n_1}} (\bar{Y_k}(1) - \bar{Y}(1)) + \sqrt{\frac{n_1}{n_0}} (\bar{Y_k}(0) - \bar{Y}(0)))^2 >= 0 $
\\$\hat{\tau_s}$ is generally more efficient (has lower variance) than $\hat{\tau}_{CRE}$

\subsubsection{Post-Stratification}

We've already run a CRE
\\$\textbf{n} = \{n_{k,1}, n_{k,0}\}_{k=1}^{K}$
\\$P_{CRE} (\textbf{Z}=z | \textbf{n}) = \frac{P(\textbf{Z}=z, \textbf{n})}{P(\textbf{n})}$ where $\textbf{Z} \epsilon R^n = \{Z_1, ...., Z_n\}$
\\= $\frac{1}{\pi_{k=1}^k \binom{n_k}{n_k,1}}$
\\How to choose k?
\\$k=5$ empirically is a good choice OR $k = \frac{n}{2}$ which is the 'matched pairs experiment'


\section{Lecture Five: Rerandomization and Regression Adjustment}{Inigo Artiagoitia \& Sai Kolasani}
\subsection{Last Lecture: Discrete Covariate}
    \begin{itemize}
        \item Design: Stratification
        \end{itemize}
    \begin{itemize}
        \item Analysis: Post-stratification
    \end{itemize}
\subsection{Completely randomized experiment}
$n$ total experimental units

$n_1$ units randomly assigned to the treatment group

$n_0$ units randomly assigned to the control group

number of ways to assign $n_1$ units to treatment out of $n$ total units in a CRE:
$$\mathbb{Z}\binom{n}{n_1}$$
\subsubsection{Rerandomization}

In rerandomization, we draw $Z$ from the CRE and accept it if and only if the covariates are balanced across treatment and control groups.

Also, we have covariates. For example:
\begin{itemize}
    \item $x_i \in \mathbb{R}^k$ (e.g., age)
    \item Outcome $y$ might be fitness, while treatment $z$ could be a health intervention
\end{itemize}

We want to ensure these covariates are balanced between our treatment and control groups. To check this, we calculate the difference in means of covariates:

\[
\hat{\tau}_x = \frac{1}{n_1} \sum_{i=1}^n z_i x_i - \frac{1}{n_0} \sum_{i=1}^n (1-z_i) x_i
\]

Under CRE, $E[\hat{\tau}_x] = 0$, meaning the covariates are perfectly balanced on average. However, in practice, we often see $\hat{\tau}_x \neq 0$.

The covariance of this difference is given by:

\[
\text{cov}(\hat{\tau}_x) = \frac{1}{n_1} S_x^2 + \frac{1}{n_0} S_x^2 
\]

\[
= \frac{n}{n_1 n_0} S_x^2
\]

\[
S_x^2 = \frac{1}{n-1} \sum_{i=1}^{n} x_i x_i^T
\]

\subsubsection{Mahalanobis Distance}

The following Mahalanobis distance measures the difference between the treatment and control groups:

\[
M = \hat{\tau}_x^T \, \text{cov}(\hat{\tau}_x)^{-1} \hat{\tau}_x 
\]

\[
= \hat{\tau}_x^T \left( \frac{n}{n_i n_0} S_x^2 \right)^{-1} \hat{\tau}_x 
\]

\subsubsection{Rerandomization Procedure}

For this, we:
\begin{enumerate}
    \item Draw $Z$ from CRE
    \item Accept if and only if $M \leq a$
\end{enumerate}

The value of 'a' determines how strict we are about balance:
\begin{itemize}
    \item $a = \infty \rightarrow$ We accept any randomization (equivalent to CRE)
    \item $a = 0 \rightarrow$ We only accept perfect balance (usually impossible)
    \item $a = 0.001 \rightarrow$ A good threshold used.
\end{itemize}

This procedure is invariant to linear transformations of covariates.

\subsubsection{Inference}

\begin{itemize}
    \item We use FRT that simulates $Z$ under the constraint $M \leq a$
    \item For $a = \infty$: $\varepsilon$ be a univariate standard normal RV, and $\hat{\tau} - \tau \stackrel{D}{\sim} \sqrt{\text{var}(\hat{\tau})} \varepsilon$
    \item For $a$ close to zero: $\hat{\tau} - \tau \stackrel{D}{\sim} \sqrt{\text{var}(\hat{\tau})(1-R^2)} \varepsilon$ where $R^2$ is the squared correlation between $\hat{\tau}$ and $\hat{\tau}_x$
\end{itemize}

\subsection{Regression Adjustment (analysis)}

FRT:
\begin{enumerate}
    \item Regress $Y_i \sim X_i \rightarrow \hat{Y}_i$. 
    $\hat{\varepsilon}_i = \hat{Y}_i - Y_i$: pseudo outcome for test statistic.
    \item Regress $Y_i \sim (Z_i, X_i) \rightarrow \hat{\beta}$ (coefficient of $Z_i$): Test statistic.
    \item Estimation $\tau$:
\end{enumerate}

\subsubsection{Fisher's ANCOVA (1925)}

\begin{enumerate}
    \item Run an OLS Regression: $Y_i \sim (1, z_i, x_i)$
    \item Use $\beta$ (coefficient of $Z$) as our estimator for $\tau = \hat{\tau}_F$
\end{enumerate}

However, this method was criticized because:
\begin{enumerate}
    \item Bias: $E[\hat{\tau}_F] - \tau \neq 0$
    \item $\text{Var}(\hat{\tau}_F) > \text{var}(\hat{\tau})$ for $n_1 \neq n_0$
    \item Standard errors from OLS are incorrect
\end{enumerate}

\subsubsection{Lin (2013) Improvements}

Lin proposed some improvements to address these issues:
\begin{enumerate}
    \item The bias $E[\hat{\tau}_F] - \tau$ is small for large $n$ and approaches 0 as $n \rightarrow \infty$
    \item Run an OLS Regression: $y_i \sim (1, z_i, x_i, x_i * z_i) \rightarrow \hat{\tau}_L$ (coefficient of $z$)
    \begin{itemize}
        \item $\hat{\tau}_L$ is more efficient than $\hat{\tau}_F$
    \end{itemize}
    \item EHW standard error gives conservative estimate for $\hat{\tau}_L$.
\end{enumerate}


\subsubsection{Difference-in-Differences}

\begin{itemize}
    \item A special case of covariate adjustment, where $X$ is the \textbf{lagged outcome before the treatment}.
    
    \item The estimator for the average treatment effect is given by:
    \[
    \hat{\tau}_{\text{DiD}} = \frac{1}{n_{1}} \sum_{i=1}^{n} z_i (Y_{i} - x_i) - \frac{1}{n_{0}} \sum_{i=1}^{n} (1 - z_i)(Y_{i} - x_i)
    \]
    where $n_{1}$ is the number of treated units and $n_{0}$ is the number of control units. 
    
    This expression represents the \textbf{difference in treated and control outcomes after adjusting for pre-treatment covariates (the lagged outcomes $x_i$).}
    
    \item This can be rewritten as:
    \[
    \hat{\tau}_{\text{DiD}} = \left( \hat{Y}(1) - \hat{Y}(0) \right) - \left( \hat{X}(1) - \hat{X}(0) \right)
    \]
    where:
    \begin{itemize}
        \item $\hat{Y}(1)$ and $\hat{Y}(0)$ are the mean outcomes for the treated and control groups, respectively.
        \item $\hat{X}(1)$ and $\hat{X}(0)$ are the mean pre-treatment covariates for the treated and control groups, respectively.
    \end{itemize}
    
    \item This estimator is \textbf{unbiased}, and the mean of the treatment and control groups are:
    \[
    \hat{g}(C1) = \frac{1}{n_{1}} \sum_{i=1}^{n} z_i g_i
    \]
    \[
    \hat{g}(C0) = \frac{1}{n_{0}} \sum_{i=1}^{n} (1 - z_i) g_i
    \]
    where $g_i = Y_i - X_i$ represents the difference between the outcome and the lagged pre-treatment outcome for unit $i$.
    
    \item The variance of the estimator is:
    \[
    \hat{V} = \frac{1}{n_{1}(n_{1} - 1)} \sum_{i=1}^{n} z_i \left( g_i - \hat{g}(1) \right)^2 + \frac{1}{n_{0}(n_{0} - 1)} \sum_{i=1}^{n} (1 - z_i) \left( g_i - \hat{g}(0) \right)^2
    \]
    
\end{itemize}

\subsection{Matched Pairs Experiment}
\subsubsection{Notation and Setup}

\begin{itemize}
    
    \item Each pair contains one treated unit and one control unit. This ensures that comparisons between treatment and control are made within pairs, reducing the influence of extraneous variables.
    
    \item There are $2n$ experimental units, where:
    \[
    Z_i = \begin{cases}
    1 & \text{if the 1st unit receives treatment} \\
    0 & \text{if the 2nd unit receives treatment}
    \end{cases}
    \]
    
    \item $(i, j)$ indexes the $j$-th unit of pair $i$, with:
    \[
    i = 1, 2, \dots, n \quad j = 1, 2
    \]
    
    \item \textbf{Potential Outcomes Framework}: Each unit $(i, j)$ has potential outcomes $Y_{ij}(1)$ and $Y_{ij}(0)$, where:
    \begin{itemize}
        \item $Y_{ij}(1)$ is the outcome if unit $(i,j)$ receives treatment.
        \item $Y_{ij}(0)$ is the outcome if unit $(i,j)$ does not receive treatment.
    \end{itemize}
    The goal is to estimate the treatment effect by comparing the potential outcomes between treated and control units.
    
    \item The treatment assignment $Z_i$ is drawn \textit{i.i.d.} (independently and identically distributed) from a Bernoulli distribution with probability $\frac{1}{2}$:
    \[
    Z_i \overset{iid}{\sim} \text{Bernoulli}\left( \frac{1}{2} \right)
    \]
    This means that for each pair, there is an equal chance that either the first unit or the second unit will be assigned to treatment. This randomization ensures that treatment assignment is unbiased.
    
    \item The outcomes for an individual pair $(i)$ are defined as:
    \[
    y_{i1} = z_i y_{i1}(1) + (1 - z_i) y_{i1}(0)
    \]
    \[
    y_{i2} = z_i y_{i2}(0) + (1 - z_i) y_{i2}(1)
    \]
    These equations capture the realized outcomes for the units in the experiment. Depending on the treatment assignment $z_i$, we observe either the treated or control potential outcome.
    
\end{itemize}

\subsubsection{Fisher's Randomization Test (FRT)}

\begin{itemize}
    
    \item \textbf{Null Hypothesis ($H_0$)}: The null hypothesis in FRT states that there is no treatment effect, meaning the potential outcomes are the same regardless of whether the unit receives treatment or not. This implies:
    \[
    Y_{ij}(1) = Y_{ij}(0) \quad \text{for all} \quad i = 1, \dots, n \quad j = 1, 2
    \]
    Under $H_0$, the assignment of treatment is purely random, and any observed differences between the treated and control units are due to chance.
    
    \item \textbf{Pairwise Treatment Effect ($\hat{\tau}_i$)}: For each pair $i$, the treatment effect is estimated as the difference between the outcomes of the two units. The treatment effect for pair $i$ is given by:
    \[
    \hat{\tau}_i = y_{i1} - y_{i2} \quad \text{if} \quad z_i = 1
    \]
    \[
    \hat{\tau}_i = y_{i2} - y_{i1} \quad \text{if} \quad z_i = 0
    \]
    Here, $\hat{\tau}_i$ represents the observed treatment effect within pair $i$.
    
    \item \textbf{Average Treatment Effect ($\hat{\tau}$)}: The overall treatment effect, $\hat{\tau}$, is the average treatment effect across all pairs. This is calculated as:
    \[
    \hat{\tau} = \frac{1}{n} \sum_{i=1}^{n} \hat{\tau}_i
    \]
    The goal is to determine whether this observed treatment effect is significantly different from what would be expected under the null hypothesis.
    
    \item \textbf{Distribution of $\hat{\tau}$ under $H_0$}: Under the null hypothesis, the expected value of $\hat{\tau}$ is zero:
    \[
    \mathbb{E}[\hat{\tau}] = 0
    \]
    The variance of $\hat{\tau}$ is calculated as:
    \[
    \text{Var}(\hat{\tau}) = \frac{1}{n^2} \sum_{i=1}^{n} \text{Var}(\hat{\tau}_i)
    \]
    Given that $\hat{\tau}_i = Y_{i1} - Y_{i2}$, the variance can be expanded as:
    \[
    \text{Var}(\hat{\tau}) = \frac{1}{n^2} \sum_{i=1}^{n} \text{Var}(S_i) \left( Y_{i1} - Y_{i2} \right)^2
    \]
    Finally, the variance is simplified to:
    \[
    \text{Var}(\hat{\tau}) = \frac{1}{n^2} \sum_{i=1}^{n} \hat{\tau}_i^2
    \]
    
    \item \textbf{Test Statistic}: The test statistic for FRT is constructed by standardizing the observed treatment effect. Under the null hypothesis, the standardized test statistic follows a standard normal distribution:
    \[
    \frac{\hat{\tau}}{\sqrt{\frac{1}{n^2} \sum_{i=1}^{n} \hat{\tau}_i^2}} \sim N(0, 1)
    \]
    This allows us to evaluate whether the observed treatment effect $\hat{\tau}$ is significantly different from zero, which would indicate a treatment effect.
    
\end{itemize}

\subsubsection{Kolmogorov-Smirnov Type Statistic}

\begin{itemize}
    
    \item The observed treatment effects $\hat{\tau}_1, \dots, \hat{\tau}_n$ are fixed, and the test statistic $\Delta$ is defined as:
    \[
    \Delta = \sum_{i=1}^{n} \mathbb{I} \left( \hat{\tau}_i > 0 \right)
    \]
    where $\mathbb{I} \left( \hat{\tau}_i > 0 \right)$ is an indicator function that takes the value 1 if the treatment effect $\hat{\tau}_i$ is non-negative and 0 otherwise. This statistic counts the number of pairs where the treatment effect is positive.
    
    \item \textbf{Distribution of $\Delta$ under the Null Hypothesis ($H_0$)}: 
    Under the null hypothesis that there is no treatment effect, the indicator function $\mathbb{I} \left( \hat{\tau}_i > 0 \right)$ follows a Bernoulli distribution with probability $\frac{1}{2}$:
    \[
    \mathbb{I} \left( \hat{\tau}_i > 0 \right) \overset{iid}{\sim} \text{Bernoulli}\left( \frac{1}{2} \right)
    \]
    This means that under $H_0$, there is a 50\% chance that the treatment effect is positive in any given pair.

    \item \textbf{Binomial Distribution of $\Delta$}: Since the indicator function follows a Bernoulli distribution, the sum $\Delta$ follows a binomial distribution with $n$ trials and probability of success $\frac{1}{2}$:
    \[
    \Delta \sim \text{Binomial}(n, \frac{1}{2})
    \]
    
    \item \textbf{Binomial Test and Normal Approximation}: To test whether the observed number of positive treatment effects is significantly different from what we expect under $H_0$, we can either use the Binomial test with $p = \frac{1}{2}$ or, for large sample sizes, apply the normal approximation:
    \[
    \frac{\Delta - \frac{n}{2}}{\sqrt{\frac{n}{4}}} \sim N(0, 1)
    \]
    
\end{itemize}

\subsubsection{McNemar's Statistic}

\textbf{Binary Outcome}

\[
\begin{array}{|c|c|c|c|c|c|}
\hline
\text{Pair} \, i & Y_{i1} & Y_{i2} & z_i & \text{Treated Outcome} & \text{Control Outcome} \\
\hline
1 & 1 & 1 & 1 & 1 & 1 \\
2 & 0 & 0 & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
3 & 1 & 0 & 1 & 1 & 0 \\
4 & 1 & 0 & 0 & 0 & 1 \\
\hline
\end{array}
\]

This will be continued in the following lecture...











\bibliography{ref}

\end{document}