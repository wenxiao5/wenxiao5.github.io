\contentsline {chapter}{\numberline {1}Statistics Basics}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Random Sampling}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Sample Mean and Sample Variance}{2}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Distributional Properties}{2}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Order Statistics}{2}{subsection.1.1.3}%
\contentsline {section}{\numberline {1.2}Statistics Model (ECON 240B)}{4}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Model}{4}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Parametric Model}{4}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Parameter}{4}{subsection.1.2.3}%
\contentsline {section}{\numberline {1.3}Model Estimation (ECON 240B)}{5}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Plug-In Estimation}{5}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Bootstrap}{7}{subsection.1.3.2}%
\contentsline {section}{\numberline {1.4}Point Estimation}{12}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Method of Moments (MM)}{12}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Maximum Likelihood (ML)}{14}{subsection.1.4.2}%
\contentsline {section}{\numberline {1.5}Comparing Estimators: Mean Squared Error}{15}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}Mean Squared Error = $\textnormal {Bias}^2$ + Variance}{15}{subsection.1.5.1}%
\contentsline {subsection}{\numberline {1.5.2}Uniform Minimum Variance Unbiased (UMVU)}{16}{subsection.1.5.2}%
\contentsline {section}{\numberline {1.6}Sufficient Statistics}{16}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1}Sufficient Statistic: contains all information of $\theta $}{16}{subsection.1.6.1}%
\contentsline {subsection}{\numberline {1.6.2}Rao-Blackwell Theorem}{17}{subsection.1.6.2}%
\contentsline {subsection}{\numberline {1.6.3}Fisher-Neyman Factorization Theorem}{17}{subsection.1.6.3}%
\contentsline {subsection}{\numberline {1.6.4}Minimal Sufficient Statistic}{18}{subsection.1.6.4}%
\contentsline {section}{\numberline {1.7}Complete Statistic}{19}{section.1.7}%
\contentsline {subsection}{\numberline {1.7.1}Complete Statistic}{19}{subsection.1.7.1}%
\contentsline {subsection}{\numberline {1.7.2}Unbiased $\hat {\theta }(T)$ with sufficient and complete $T$ is UMVU}{20}{subsection.1.7.2}%
\contentsline {section}{\numberline {1.8}Fisher Information}{21}{section.1.8}%
\contentsline {subsection}{\numberline {1.8.1}Score Function}{21}{subsection.1.8.1}%
\contentsline {subsection}{\numberline {1.8.2}Fisher Information}{22}{subsection.1.8.2}%
\contentsline {subsection}{\numberline {1.8.3}Cram√©r-Rao Lower Bound}{23}{subsection.1.8.3}%
\contentsline {section}{\numberline {1.9}Hypothesis Testing}{24}{section.1.9}%
\contentsline {subsection}{\numberline {1.9.1}Formulation of Testing Problem}{24}{subsection.1.9.1}%
\contentsline {subsection}{\numberline {1.9.2}Errors, Power Function, and Agenda}{25}{subsection.1.9.2}%
\contentsline {subsection}{\numberline {1.9.3}Choice of Critical Value}{26}{subsection.1.9.3}%
\contentsline {subsection}{\numberline {1.9.4}Choice of Test Statistic: Uniformly Most Powerful (UMP) Level $\alpha $ Test}{26}{subsection.1.9.4}%
\contentsline {subsection}{\numberline {1.9.5}Generalized Neyman-Pearson Lemma}{29}{subsection.1.9.5}%
\contentsline {section}{\numberline {1.10}Trinity of Classical Tests}{29}{section.1.10}%
\contentsline {subsection}{\numberline {1.10.1}Test Statistics}{29}{subsection.1.10.1}%
\contentsline {subsection}{\numberline {1.10.2}Approximation to $T_{LR}$}{30}{subsection.1.10.2}%
\contentsline {section}{\numberline {1.11}Interval Estimation}{31}{section.1.11}%
\contentsline {chapter}{\numberline {2}M-Estimation}{32}{chapter.2}%
\contentsline {section}{\numberline {2.1}M-Estimation}{32}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Extremum Estimator and M-Estimator}{32}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}Consistency of M-estimators}{34}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}Asymptotic Normality of M-estimators}{34}{subsection.2.1.3}%
\contentsline {subsection}{\numberline {2.1.4}Efficiency of Asymptotically Linear Estimator}{35}{subsection.2.1.4}%
\contentsline {subsection}{\numberline {2.1.5}Misspecification and Pseudo-true Parameter}{36}{subsection.2.1.5}%
\contentsline {section}{\numberline {2.2}Binary Choice}{38}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Latent Utility Models (structural motivation for probit model)}{38}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Estimation: Binary Regression}{39}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Consistency and Asymptotic Normality}{40}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}Example: Logistic Regression $F(t)=\frac {e^t}{1+e^t}$}{41}{subsection.2.2.4}%
\contentsline {section}{\numberline {2.3}Large Sample Testing}{41}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Wald Test: Distance on ``$x$ axis''}{42}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Lagrange Multiplier Test: Distance using ``gradient''}{42}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Likelihood Ratio Test}{42}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}Wald is not invariant to parametrization}{43}{subsection.2.3.4}%
\contentsline {section}{\numberline {2.4}Nonlinear Least Square}{43}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Efficient NLS: Weighted NLS}{45}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Example 1: $y_i=x_i^{\beta _0}+\epsilon _i$}{46}{subsection.2.4.2}%
\contentsline {section}{\numberline {2.5}(Linear) Quantile Regression}{46}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}Linear Quantile Regression Model}{47}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}Quantile Causal Effects}{48}{subsection.2.5.2}%
\contentsline {subsection}{\numberline {2.5.3}M-estimator of Quantile}{49}{subsection.2.5.3}%
\contentsline {section}{\numberline {2.6}Lasso}{50}{section.2.6}%
\contentsline {chapter}{\numberline {3}Bootstrap}{51}{chapter.3}%
\contentsline {section}{\numberline {3.1}Traditional Monte-Carlo Approach}{51}{section.3.1}%
\contentsline {section}{\numberline {3.2}Bootstrap (When data is not enough)}{52}{section.3.2}%
\contentsline {section}{\numberline {3.3}Residual Bootstrap (for problem with not i.i.d. data)}{52}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Example: Linear}{53}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Example: Nonlinear Markov Process}{53}{subsection.3.3.2}%
\contentsline {section}{\numberline {3.4}Posterior Simulation / Bayesian (Weighted) Bootstrap}{54}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Dirichlet Distribution Prior}{54}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Haldane Prior}{55}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Linear Model Case}{55}{subsection.3.4.3}%
\contentsline {subsection}{\numberline {3.4.4}Bernoulli Case}{56}{subsection.3.4.4}%
\contentsline {chapter}{\numberline {4}Linear Predictors / Regression}{57}{chapter.4}%
\contentsline {section}{\numberline {4.1}Best Linear Predictor}{57}{section.4.1}%
\contentsline {section}{\numberline {4.2}Convergence of OLS}{58}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Approximation}{58}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Testing and Confidence Interval}{60}{subsection.4.2.2}%
\contentsline {section}{\numberline {4.3}Long, Short, Auxilary Regression}{60}{section.4.3}%
\contentsline {section}{\numberline {4.4}Residual Regression}{62}{section.4.4}%
\contentsline {section}{\numberline {4.5}Card-Krueger Model}{63}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}Proxy Variable Regression}{64}{subsection.4.5.1}%
\contentsline {section}{\numberline {4.6}Instrumental Variables}{65}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Motivation}{65}{subsection.4.6.1}%
\contentsline {subsection}{\numberline {4.6.2}I.V. Model}{65}{subsection.4.6.2}%
\contentsline {subsection}{\numberline {4.6.3}Weak I.V.}{67}{subsection.4.6.3}%
\contentsline {section}{\numberline {4.7}Linear Generalized Method of Moments (Linear GMM)}{68}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}Generalized Method of Moments (GMM)}{68}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}Linear GMM}{69}{subsection.4.7.2}%
\contentsline {subsection}{\numberline {4.7.3}Properties of Linear GMM Estimator}{70}{subsection.4.7.3}%
\contentsline {subsection}{\numberline {4.7.4}Alternative: Continuous Updating Estimator}{71}{subsection.4.7.4}%
\contentsline {subsection}{\numberline {4.7.5}Inference}{71}{subsection.4.7.5}%
\contentsline {subsection}{\numberline {4.7.6}OVER-ID Test}{72}{subsection.4.7.6}%
\contentsline {subsection}{\numberline {4.7.7}Bootstrap GMM}{74}{subsection.4.7.7}%
\contentsline {section}{\numberline {4.8}Panel Data Models}{74}{section.4.8}%
\contentsline {subsection}{\numberline {4.8.1}Pooled OLS}{75}{subsection.4.8.1}%
\contentsline {subsection}{\numberline {4.8.2}Fixed Effect Model}{76}{subsection.4.8.2}%
\contentsline {subsection}{\numberline {4.8.3}Random Effect Model}{77}{subsection.4.8.3}%
\contentsline {subsection}{\numberline {4.8.4}Two-Way Fixed Effect Model}{78}{subsection.4.8.4}%
\contentsline {subsection}{\numberline {4.8.5}Arellano Bond Approach}{79}{subsection.4.8.5}%
\contentsline {section}{\numberline {4.9}Control Function Approach (another approach to handle endogenieity)}{80}{section.4.9}%
\contentsline {section}{\numberline {4.10}LATE (Local ATE): Application of I.V. on Potential Outcomes}{80}{section.4.10}%
\contentsline {section}{\numberline {4.11}Difference in Difference (DiD)}{82}{section.4.11}%
\contentsline {subsection}{\numberline {4.11.1}After OLS Regression}{83}{subsection.4.11.1}%
\contentsline {subsection}{\numberline {4.11.2}Difference in Difference}{83}{subsection.4.11.2}%
