\contentsline {chapter}{\numberline {1}Math Foundations}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Strongly Convexity}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}$\mu $-Strongly Convex: $ \langle \nabla f(w)-\nabla f(v), w-v\rangle \geq \mu \|w-v\|^{2}$}{1}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}$\mu $-strongly convex $\Leftrightarrow \nabla ^{2} f(x) \succeq \mu I\Leftrightarrow $"$f(x)-\frac {m}{2}\|x\|^2$ is convex"}{1}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Lemma: Strongly convexity $\Rightarrow $ Strictly convexity}{1}{subsection.1.1.3}%
\contentsline {subsection}{\numberline {1.1.4}Lemma: $\nabla ^2 f(x)\succeq mI$ $\Rightarrow $ $f(y)\geq f(x)+\nabla f(x)^T(y-x)+\frac {m}{2}\|y-x\|^2$}{2}{subsection.1.1.4}%
\contentsline {section}{\numberline {1.2}Lipschitz Gradient ($L$-Smooth)}{2}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Theorem: $-MI\preceq \nabla ^2 f(x)\preceq MI$ $\Rightarrow $ $f$ is $M$-smooth}{3}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Descent Lemma: $f$ is $L$-smooth $\Rightarrow $ $f(y)\leq f(x)+\nabla f(x)^T(y-x)+\frac {L}{2}\|y-x\|^2$}{3}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Co-coercivity Condition: $(\nabla f(x)-\nabla f(y))^T(x-y)\geq \frac {1}{L}\|\nabla f(x)-\nabla f(y)\|^2$}{4}{subsection.1.2.3}%
\contentsline {chapter}{\numberline {2}(Unconstrained Optimization) Gradient Methods}{5}{chapter.2}%
\contentsline {section}{\numberline {2.1}Steepest Descent}{5}{section.2.1}%
\contentsline {section}{\numberline {2.2}Methods for Choosing Step Size $\alpha _k$}{6}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Optimal (Exact) Line Search}{6}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Armijo's Rule}{6}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Armijo's Rule for Steepest Descent}{7}{subsection.2.2.3}%
\contentsline {section}{\numberline {2.3}Algorithm Convergence}{8}{section.2.3}%
\contentsline {section}{\numberline {2.4}Convergence of The Steepest Descent with Fixed Step Size}{8}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Theorem: $f$ is $L$-smooth $\Rightarrow $ $\{x_k\}$ converges to stationary point}{8}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Theorem: $f$ is convex and $L$-smooth $\Rightarrow $ $f(x_k)$ converges to global-min value with rate $\frac {1}{k}$}{10}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Theorem: $f$ is strongly convex and $L-$smooth $\Rightarrow $ $\{x_k\}$ converges to global-min geometrically}{11}{subsection.2.4.3}%
\contentsline {section}{\numberline {2.5}Convergence of Gradient Descent on Smooth Strongly-Convex Functions}{13}{section.2.5}%
\contentsline {section}{\numberline {2.6}From convergence rate to iteration complexity}{17}{section.2.6}%
\contentsline {chapter}{\numberline {3}(Unconstrained Optimization) Gradient Projection Methods}{18}{chapter.3}%
\contentsline {section}{\numberline {3.1}Projection onto Closed Convex Set}{18}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Def: Projection $[z]^\&$}{18}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Prop: \underline {unique} projection $[z]^\&$ on \underline {closed convex} subset of $\mathbb {R}^n$}{19}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}Obtuse Angle Criterion: $x=[z]^\&$ is projection on \underline {closed convex}subset of $\mathbb {R}^n$$\Leftrightarrow $ $(z-x)^T(y-x)\leq 0, \forall y\in \&$}{19}{subsection.3.1.3}%
\contentsline {subsection}{\numberline {3.1.4}Prop: Projection is non-expansive $\|[x]^\&-[z]^\&\|\leq \|x-z\|,\forall x,z\in \mathbb {R}^n$}{20}{subsection.3.1.4}%
\contentsline {section}{\numberline {3.2}Projection on (Linear) Subspaces of $\mathbb {R}^n$}{21}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Orthogonality Principle in subspaces of $\mathbb {R}^n$: $(z-y^*)^Tx= 0,\forall x\in \&$}{21}{subsection.3.2.1}%
\contentsline {section}{\numberline {3.3}Gradient Projection Method}{21}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Def: \underline {fixed point} in fixed step-size steepest descent method, $\tilde {x}=[\tilde {x}-\alpha \nabla f(\tilde {x})]^\&$}{22}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Prop: $L-$smooth, $0<\alpha <\frac {2}{L}$ $\Rightarrow $ limit point is a fixed point (in fixed step-size steepest descent method)}{22}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Prop: $x$ is minimizer in convex func $\Leftrightarrow $ fixed point (in fixed step-size steepest descent method)}{23}{subsection.3.3.3}%
\contentsline {subsection}{\numberline {3.3.4}Thm: Convergence of Gradient Projection: Convex, $L-$smooth, $0<\alpha <\frac {2}{L}$ $\Rightarrow $ $f(x_k)\rightarrow f(x^*)$ at rate $\frac {1}{k}$}{23}{subsection.3.3.4}%
\contentsline {subsection}{\numberline {3.3.5}Thm: Strongly convex, Lipschitz gradient $\Rightarrow $ $\{x_k\}$ converges to $x^*$ geometrically}{23}{subsection.3.3.5}%
\contentsline {chapter}{\numberline {4}(Unconstrained Optimization) Sub-gradient Methods}{25}{chapter.4}%
\contentsline {section}{\numberline {4.1}Sub-gradient}{25}{section.4.1}%
\contentsline {section}{\numberline {4.2}Sub-differential}{25}{section.4.2}%
\contentsline {section}{\numberline {4.3}More examples}{26}{section.4.3}%
\contentsline {section}{\numberline {4.4}First-order necessary conditions for optimality in terms of subgradient}{27}{section.4.4}%
\contentsline {section}{\numberline {4.5}Properties of Subgradients}{28}{section.4.5}%
\contentsline {section}{\numberline {4.6}Sub-gradient Descent for Unconstrained Optimization}{28}{section.4.6}%
\contentsline {section}{\numberline {4.7}(Revised) Sub-gradient "descent" with diminishing stepsize}{29}{section.4.7}%
\contentsline {chapter}{\numberline {5}(Unconstrained Optimization) Newton's Method}{32}{chapter.5}%
\contentsline {section}{\numberline {5.1}Classical Newton's Method}{32}{section.5.1}%
\contentsline {section}{\numberline {5.2}Variants of Newton's Method}{32}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Generalization to Optimization}{32}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}The Secant Method}{33}{subsection.5.2.2}%
\contentsline {section}{\numberline {5.3}A New Interpretation of Newton's Method}{33}{section.5.3}%
\contentsline {section}{\numberline {5.4}Convergence of Newton's Method}{33}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}Guarantees of Convergence}{33}{subsection.5.4.1}%
\contentsline {subsection}{\numberline {5.4.2}Convergence Rate}{33}{subsection.5.4.2}%
\contentsline {section}{\numberline {5.5}Note: Cons and Pros}{35}{section.5.5}%
\contentsline {section}{\numberline {5.6}Modifications to ensure global convergence}{36}{section.5.6}%
\contentsline {section}{\numberline {5.7}Quasi-Newton Methods}{36}{section.5.7}%
\contentsline {subsection}{\numberline {5.7.1}BFGS Method}{37}{subsection.5.7.1}%
\contentsline {section}{\numberline {5.8}Trust-Region Method}{38}{section.5.8}%
\contentsline {section}{\numberline {5.9}Cubic Regularization}{38}{section.5.9}%
\contentsline {chapter}{\numberline {6}(Constrained Optimization) Barrier Method}{39}{chapter.6}%
\contentsline {section}{\numberline {6.1}Barrier Method}{39}{section.6.1}%
\contentsline {section}{\numberline {6.2}An Exmaple Using KKT or Barrier}{40}{section.6.2}%
\contentsline {subsection}{\numberline {6.2.1}Solution using KKT conditions}{41}{subsection.6.2.1}%
\contentsline {subsection}{\numberline {6.2.2}Solution using logarithmic barrier}{41}{subsection.6.2.2}%
\contentsline {section}{\numberline {6.3}Penalty Method (For ECP)}{42}{section.6.3}%
\contentsline {chapter}{\numberline {7}Descent Method}{44}{chapter.7}%
\contentsline {section}{\numberline {7.1}Method of Steepest Descent}{44}{section.7.1}%
\contentsline {subsection}{\numberline {7.1.1}The Method of Steepest Descent}{44}{subsection.7.1.1}%
\contentsline {subsection}{\numberline {7.1.2}Properties of steepest descent}{45}{subsection.7.1.2}%
\contentsline {section}{\numberline {7.2}General Descent Method}{45}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}Criteria for a descent method}{45}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}Wolfe's theorem}{46}{subsection.7.2.2}%
\contentsline {subsection}{\numberline {7.2.3}Picking step sizes}{46}{subsection.7.2.3}%
\contentsline {subsection}{\numberline {7.2.4}Picking descent directions}{47}{subsection.7.2.4}%
\contentsline {subsection}{\numberline {7.2.5}Methods Summary}{48}{subsection.7.2.5}%
