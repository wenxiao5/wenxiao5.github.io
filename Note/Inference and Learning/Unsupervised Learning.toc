\contentsline {chapter}{\numberline {1}Clustering}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}K-Means}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}K-Means Clustering Optimization Problem}{1}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Lloyd's Algorithm}{2}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Benefits and Drawbacks}{2}{subsection.1.1.3}%
\contentsline {subsection}{\numberline {1.1.4}Elbow Method}{3}{subsection.1.1.4}%
\contentsline {section}{\numberline {1.2}Types of Clusters Definitions}{4}{section.1.2}%
\contentsline {section}{\numberline {1.3}K-Medians}{6}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}K-Medians Clustering Optimization Problem}{6}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}K-Medians Heuristic Algorithm}{7}{subsection.1.3.2}%
\contentsline {section}{\numberline {1.4}K-Medoids}{7}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}K-Medoids Clustering Optimization Problem}{7}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}K-Medoids Clustering Algorithm}{8}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}K-Medoids vs. K-Means}{8}{subsection.1.4.3}%
\contentsline {section}{\numberline {1.5}Types of Clustering Algorithms Results}{9}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}Partitional vs. Hierarchical Clustering Results}{9}{subsection.1.5.1}%
\contentsline {subsection}{\numberline {1.5.2}Exclusive vs. Overlapping vs. Fuzzy Clustering Results}{10}{subsection.1.5.2}%
\contentsline {chapter}{\numberline {2}Clustering Evaluation Metrics}{11}{chapter.2}%
\contentsline {section}{\numberline {2.1}Clusterability Evaluation Metric: Is the dataset clusterable?}{12}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Hopkin's Statistics}{12}{subsection.2.1.1}%
\contentsline {section}{\numberline {2.2}Unsupervised Clustering Evaluation Metrics: How cohesive and well separated are the clusters in the clustering?}{13}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Definition}{13}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Graph-based view of cohesion and separation for a clustering}{14}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Silhouette Coefficients (Scores)}{14}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}Prototype-Based View of Cohesion and Separation for a Clustering}{15}{subsection.2.2.4}%
\contentsline {subsection}{\numberline {2.2.5}Cluster-Sorted Similarity Matrix}{16}{subsection.2.2.5}%
\contentsline {section}{\numberline {2.3}Cluster Number Evaluation Metrics: What is the 'correct' number of clusters?}{16}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}General Elbow Plot Method}{16}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Average Silhouette Score Plot Method}{17}{subsection.2.3.2}%
\contentsline {section}{\numberline {2.4}Supervised Clustering Evaluation Metrics: How similar is the clustering to a set of (external) pre-assigned class labels?}{18}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Rand Index and Jaccard Coefficient of Two Partitions}{18}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Adjusted Rand Index}{18}{subsection.2.4.2}%
\contentsline {section}{\numberline {2.5}Clustering Comparison Metrics: Which clustering is better for a given dataset?}{19}{section.2.5}%
\contentsline {section}{\numberline {2.6}'Clusterable'? and 'Correct' Number of Clusters: $t$-SNE}{19}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Goal}{19}{subsection.2.6.1}%
\contentsline {subsection}{\numberline {2.6.2}Input/Output for the Algorithm}{20}{subsection.2.6.2}%
\contentsline {subsection}{\numberline {2.6.3}Main Idea of The Algorithm}{21}{subsection.2.6.3}%
\contentsline {chapter}{\numberline {3}Hierarchical Clustering}{23}{chapter.3}%
\contentsline {section}{\numberline {3.1}Agglomerative and Divisive Hierarchical Clustering Algorithms}{23}{section.3.1}%
\contentsline {section}{\numberline {3.2}Agglomerative Hierarchical Clustering}{23}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}General Algorithm for Agglomerative Hierarchical Clustering}{23}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}with Single Linkage}{24}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}with Complete Linkage}{24}{subsection.3.2.3}%
\contentsline {subsection}{\numberline {3.2.4}with Average Linkage}{25}{subsection.3.2.4}%
\contentsline {subsection}{\numberline {3.2.5}with Ward's Linkage}{25}{subsection.3.2.5}%
\contentsline {section}{\numberline {3.3}Divisive Hierarchical Clustering}{26}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}General Algorithm for Divisive Hierarchical Clustering}{26}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}with the Bisecting $k$-Means Algorithm}{26}{subsection.3.3.2}%
\contentsline {chapter}{\numberline {4}Categorical Data Clustering}{28}{chapter.4}%
\contentsline {section}{\numberline {4.1}Dataset Clustering with Categorical Variables}{28}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1} t-SNE Algorithm - Using a Distance Matrix Input}{28}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Creating a Distance Matrix for Datasets with Categorical Variables}{28}{subsection.4.1.2}%
\contentsline {section}{\numberline {4.2}Partitional Clustering Algorithms for Datasets with Categorical Variables}{29}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Just Categorical Variables: $k$-Modes Clustering Algorithm}{29}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Numerical and Categorical Variables: $k$-Prototypes Clustering Algorithm}{30}{subsection.4.2.2}%
\contentsline {section}{\numberline {4.3}Hierarchical Clustering Algorithms for Datasets with Categorical Variables}{30}{section.4.3}%
\contentsline {chapter}{\numberline {5}Principal Component Analysis (PCA)}{32}{chapter.5}%
\contentsline {section}{\numberline {5.1}Assumption}{32}{section.5.1}%
\contentsline {section}{\numberline {5.2}Principal component analysis- general goals}{32}{section.5.2}%
\contentsline {section}{\numberline {5.3}Process of PCA}{33}{section.5.3}%
\contentsline {section}{\numberline {5.4}How Do We Choose $p<n$?}{33}{section.5.4}%
\contentsline {chapter}{\numberline {6}Gaussian Mixture Models}{35}{chapter.6}%
\contentsline {section}{\numberline {6.1}Why use model-based clustering?}{35}{section.6.1}%
\contentsline {section}{\numberline {6.2}Overview of Mixture Models}{35}{section.6.2}%
\contentsline {section}{\numberline {6.3}Gaussian Mixture Models}{36}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}Step 1: Finding each $\vec {\mu }_k$ and $\vec {\Sigma }_k$ with maximum likelihood estimation}{36}{subsection.6.3.1}%
\contentsline {subsection}{\numberline {6.3.2}Step 2: Estimate the probability by $\vec {\mu }_k$ and $\vec {\Sigma }_k$}{37}{subsection.6.3.2}%
\contentsline {subsection}{\numberline {6.3.3}Expectation Maximization (EM) Algorithm for GMM}{37}{subsection.6.3.3}%
\contentsline {section}{\numberline {6.4}Benefits/Drawbacks of Gaussian Mixture Model Clustering (with EM Algorithm)}{38}{section.6.4}%
\contentsline {section}{\numberline {6.5}How to choose the number of clusters in a Gaussian Mixture Model}{39}{section.6.5}%
\contentsline {subsection}{\numberline {6.5.1}Evaluation Metric 1: Akaike Information Criterion (AIC)}{39}{subsection.6.5.1}%
\contentsline {subsection}{\numberline {6.5.2}Evaluation Metric 2: Bayes Information Criterion (BIC)}{39}{subsection.6.5.2}%
\contentsline {chapter}{\numberline {7}Nonnegative Matrix Factorization (NMF)}{40}{chapter.7}%
\contentsline {section}{\numberline {7.1}Goals of NMF}{40}{section.7.1}%
\contentsline {section}{\numberline {7.2}NMF Optimization - Unsupervised Case}{40}{section.7.2}%
