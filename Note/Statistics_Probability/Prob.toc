\contentsline {chapter}{\numberline {1}Metric Spaces Foundations \small {(Lec 01 @ ECON 240A)}}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Probability Space}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Sample Space}{1}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}$\sigma -$Algebra}{1}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Probability Function}{2}{subsection.1.1.3}%
\contentsline {section}{\numberline {1.2}Random Variables}{2}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Random Variable}{2}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Represeting / Specifying $P_X$: Cumulative Distribution Function}{3}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Correspondence Theorem; CDF $\Leftrightarrow $ Probability Function}{3}{subsection.1.2.3}%
\contentsline {subsection}{\numberline {1.2.4}Discrete / Continuous Random Variable}{3}{subsection.1.2.4}%
\contentsline {section}{\numberline {1.3}Expectation, Moment, Mean, Central Moment, Variance}{4}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Expectation}{4}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Moment, Mean, Central Moment, Variance}{5}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}$\textnormal {Var}(X)=\mathbb {E}(X^2)-\mathbb {E}(X)^2$, $\mathbb {E}(aX+b)=a \mathbb {E}(X)+b$, $\textnormal {Var}(aX+b)=a^2 \textnormal {Var}(X)$}{5}{subsection.1.3.3}%
\contentsline {section}{\numberline {1.4}Univariate Distribution}{5}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Bernoulli Distribution}{5}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Normal Distribution}{6}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}Uniform Distribution}{6}{subsection.1.4.3}%
\contentsline {section}{\numberline {1.5}Exponential Families}{6}{section.1.5}%
\contentsline {section}{\numberline {1.6}Multiple Random Variables}{8}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1}Bivariate Random Vector}{8}{subsection.1.6.1}%
\contentsline {subsection}{\numberline {1.6.2}Marginal CDF, PMF, PDF}{9}{subsection.1.6.2}%
\contentsline {subsection}{\numberline {1.6.3}Independent Random Variables}{9}{subsection.1.6.3}%
\contentsline {subsection}{\numberline {1.6.4}Conditional PMF, PDF, Expected Value, Mean, Variance}{10}{subsection.1.6.4}%
\contentsline {subsection}{\numberline {1.6.5}Law of Iterated Expectation: $\mathbb {E}_X(X)=\mathbb {E}_Y[\mathbb {E}_{X|Y}(X\mid Y)]$}{10}{subsection.1.6.5}%
\contentsline {subsection}{\numberline {1.6.6}Law of Total Variance: $\textnormal {Var}_X(X)=\mathbb {E}_Y[\textnormal {Var}_{X|Y}(X\mid Y)]+\textnormal {Var}_Y[\mathbb {E}_{X|Y}(X\mid Y)]$}{11}{subsection.1.6.6}%
\contentsline {subsection}{\numberline {1.6.7}Covariance}{11}{subsection.1.6.7}%
\contentsline {chapter}{\numberline {2}Basis}{13}{chapter.2}%
\contentsline {section}{\numberline {2.1}Covariance and Variance}{13}{section.2.1}%
\contentsline {section}{\numberline {2.2}Conditional Expectation and Variance}{13}{section.2.2}%
\contentsline {section}{\numberline {2.3}Gambler's Ruin}{13}{section.2.3}%
\contentsline {section}{\numberline {2.4}Moment Generating Function (MGF)}{14}{section.2.4}%
\contentsline {section}{\numberline {2.5}Inequality}{15}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}Cauchy-Schwarz inequality: $|\mathbb {E}XY|\leq \sqrt {\mathbb {E}X^2\cdot \mathbb {E}Y^2}$}{15}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}Jensen's Inequality: convex $g$ $\Rightarrow $ $\mathbb {E}(g(X))\geq g(\mathbb {E}(X))$}{16}{subsection.2.5.2}%
\contentsline {subsection}{\numberline {2.5.3}Markov's Inequality: $P(|X|\geq a)\leq \frac {\mathbb {E}|X|}{a}$}{16}{subsection.2.5.3}%
\contentsline {subsection}{\numberline {2.5.4}Chebychev's inequality: $P(|X-\mu |\geq a)\leq \frac {\sigma ^2}{a^2}$}{16}{subsection.2.5.4}%
\contentsline {subsection}{\numberline {2.5.5}Chernoff Inequality: $P(X\geq a)\leq \frac {\mathbb {E}e^{tX}}{e^{ta}}$}{16}{subsection.2.5.5}%
\contentsline {section}{\numberline {2.6}Law of Large Numbers (LLN)}{16}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Weak Law of Large Numbers (wLLN)}{17}{subsection.2.6.1}%
\contentsline {subsection}{\numberline {2.6.2}Strong Law of Large Numbers (sLLN)}{17}{subsection.2.6.2}%
\contentsline {subsection}{\numberline {2.6.3}Differences between \underline {convergence in probability} (wLLN) and \underline {wp1(a.s.)} (sLLN)}{17}{subsection.2.6.3}%
\contentsline {section}{\numberline {2.7}Central Limit Theorem (CLT)}{18}{section.2.7}%
\contentsline {section}{\numberline {2.8}Glivenko-Cantelli Theorem (Fundamental Theorem of Statistics)}{19}{section.2.8}%
\contentsline {chapter}{\numberline {3}Distribution}{20}{chapter.3}%
\contentsline {section}{\numberline {3.1}Discrete}{20}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Bernoulli Distribution -- $\operatorname {Bernoulli}(\pi )$: an event happens with probability $\pi $}{20}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Binomial distribution -- $bin(n,\pi )$: $n$ independent Bernoulli distributions}{20}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}Multinomial Distribution}{20}{subsection.3.1.3}%
\contentsline {subsection}{\numberline {3.1.4}Poisson Distribution -- $Pois(\lambda )$: an event happens $k$ times within unit time}{21}{subsection.3.1.4}%
\contentsline {subsection}{\numberline {3.1.5}Connection between Poisson and multinomial distribution}{22}{subsection.3.1.5}%
\contentsline {subsection}{\numberline {3.1.6}Geometric distribution: $P(X=k)=(1-p)^{k-1}p$}{23}{subsection.3.1.6}%
\contentsline {section}{\numberline {3.2}Continuous}{23}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Exponential distribution $Exp(\lambda )$: interval between to independent identical event / the first time a event happened}{23}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Gaussian/Normal Distribution}{24}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}Multivariate/Joint Gaussian/Normal Distribution (MVN)}{25}{subsection.3.2.3}%
\contentsline {section}{\numberline {3.3}Poisson process: A sequence of arrivals in continuous time with rate $\lambda $}{26}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Definition}{26}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}$T_j$: time of $j^{th}$ arrival}{26}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Theorem (Conditional counts): $N(t_1)|N(t_2)=n\sim Bin(n,\frac {t_1}{t_2})$}{26}{subsection.3.3.3}%
\contentsline {chapter}{\numberline {4}Markov Chain}{27}{chapter.4}%
\contentsline {section}{\numberline {4.1}Definition}{27}{section.4.1}%
\contentsline {section}{\numberline {4.2}Matrix Computations}{27}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Chapman Kolmogorov Equations (C-K Equations) $P(X_{n+m}=j|X_0=i)=(P^{m+n})_{ij}=\DOTSB \sum@ \slimits@ _{k\in S}(P^{m})_{ik}(P^{n})_{kj}$}{28}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Marginal Distribution $P(X_n=j)=(\alpha P^n)_{j}$}{28}{subsection.4.2.2}%
\contentsline {section}{\numberline {4.3}States, Class}{28}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Irreducible, Reducible}{28}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Recurrent, Transient}{29}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}Periodicity}{30}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Lemma: all states in an irreducible MC have the same period}{30}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Periodic, Aperiodic}{30}{subsection.4.4.2}%
\contentsline {section}{\numberline {4.5}Regular Matrix}{30}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}Regular matrix: $\exists n\geq 1$ s.t. $P^n>0$}{30}{subsection.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2}Lemma: Finite MC is Irreducible, Aperiodic $\Leftrightarrow $ has Regular transition matrix}{31}{subsection.4.5.2}%
\contentsline {section}{\numberline {4.6}Eigenvalues of a Stochastic Matrix: $\lambda =1$ must exist and other $|\lambda |\leq 1$ (not equal when if regular matrix)}{31}{section.4.6}%
\contentsline {section}{\numberline {4.7}Long Run Behavior of Finite Markov Chains}{31}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}Limiting Distribution}{31}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}Stationary Distribution}{32}{subsection.4.7.2}%
\contentsline {subsection}{\numberline {4.7.3}Limiting Distribution = Expected Proportion of time in each state}{32}{subsection.4.7.3}%
\contentsline {subsection}{\numberline {4.7.4}Fundamental Theorem for \underline {Irreducible, Aperiodic, Finite MC} (Regular transition matrix) $\Rightarrow $ $\exists $ unique limiting distribution $\pi $ and $\pi _j>0,\forall j$}{33}{subsection.4.7.4}%
\contentsline {subsection}{\numberline {4.7.5}Long run behavior for reducible and/or periodic chains}{33}{subsection.4.7.5}%
\contentsline {subsection}{\numberline {4.7.6}Fundamental Theorem for \underline {Irreducible, Finite MC}: expected first return time $\mathbb {E}(T_j|X_0 = j)=\frac {1}{\pi _j}$}{34}{subsection.4.7.6}%
\contentsline {section}{\numberline {4.8}Return Times and Absorption Probabilities}{35}{section.4.8}%
\contentsline {subsection}{\numberline {4.8.1}Expected Number of Visits to a Transient State: $E(Y_i|X_0 = j) = M_{ji} = (I-Q)^{-1}_{ji}$}{35}{subsection.4.8.1}%
\contentsline {subsection}{\numberline {4.8.2}Expected Time till Absorption to a Recurrent Class: $\mathbb {E}(T_{abs}|X_0=j)=\DOTSB \sum@ \slimits@ _{i\in T_1\cup T_2\cup \cdots \cup T_s} M_{ji}$}{36}{subsection.4.8.2}%
\contentsline {subsection}{\numberline {4.8.3}Expected first return time (different initial state) = Time till Absorption}{37}{subsection.4.8.3}%
\contentsline {subsection}{\numberline {4.8.4}Probability of Eventually Entering a Given Recurrent Class: $A=(I-Q)^{-1}S=MS$}{38}{subsection.4.8.4}%
\contentsline {section}{\numberline {4.9}Examples of Finite MC}{39}{section.4.9}%
\contentsline {subsection}{\numberline {4.9.1}Gambler's Ruin}{39}{subsection.4.9.1}%
\contentsline {subsection}{\numberline {4.9.2}Simple Random Walk (SRW) on Undirected Graph}{39}{subsection.4.9.2}%
\contentsline {chapter}{\numberline {5} Countably infinite MC}{41}{chapter.5}%
\contentsline {section}{\numberline {5.1}Recurrence and Transience}{41}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Recurrent or Transient State}{41}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Recurrent or Transient Class}{42}{subsection.5.1.2}%
\contentsline {subsection}{\numberline {5.1.3}Lemma: Transient Class $\Leftrightarrow $ $\DOTSB \sum@ \slimits@ _{n=0}^\infty P_{i,i}^n<\infty $}{42}{subsection.5.1.3}%
\contentsline {subsection}{\numberline {5.1.4}Recurrence/Transience of Simple Random Walk on Lattice}{42}{subsection.5.1.4}%
\contentsline {subsection}{\numberline {5.1.5}Null and Positive Recurrence}{43}{subsection.5.1.5}%
\contentsline {subsection}{\numberline {5.1.6}Stationary Distribution and Limiting Distribution}{43}{subsection.5.1.6}%
\contentsline {section}{\numberline {5.2}Differences between Finite and (Countably) Infinite Markov Chains}{44}{section.5.2}%
\contentsline {chapter}{\numberline {6}Branching Process}{45}{chapter.6}%
\contentsline {section}{\numberline {6.1}Extinction Probability in a Branching Process}{45}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}Expectation $\mathbb {E}X_n=\mu ^n \mathbb {E}X_0$}{45}{subsection.6.1.1}%
\contentsline {subsection}{\numberline {6.1.2}Lemma: $\mu <1$ $\Rightarrow $ $P(extinction)=1$}{46}{subsection.6.1.2}%
\contentsline {subsection}{\numberline {6.1.3}Variance: $Var X_n=n\sigma ^2$ if $\mu =1$; $Var X_n=\sigma ^2\mu ^{n-1}\frac {\mu ^n-1}{\mu -1}$ if $\mu \neq 1$}{46}{subsection.6.1.3}%
\contentsline {subsection}{\numberline {6.1.4}Extinction probability $\rho =1$ if $\mu \leq 1$; $\rho <1$ if $\mu >1$}{46}{subsection.6.1.4}%
\contentsline {subsection}{\numberline {6.1.5}$G_n(s)=G_{n-1}(\psi (s))=\psi (\psi (\psi (\cdots \psi (s)\cdots )))=\psi (G_{n-1}(s))$}{48}{subsection.6.1.5}%
\contentsline {chapter}{\numberline {7}Time Reversible Markov Chains}{50}{chapter.7}%
\contentsline {section}{\numberline {7.1}Definition: Local Balance $\pi (i)P(i,j)=\pi (j)P(j,i),\forall i,j\in S$}{50}{section.7.1}%
\contentsline {section}{\numberline {7.2}Discussion about Local Balance}{50}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}Flow: $Flow(A,B)=\DOTSB \sum@ \slimits@ _{i\in A}\DOTSB \sum@ \slimits@ _{j\in B}\pi (i)P_{ij}$}{50}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}Lemma: $Flow(A,A^c)=Flow(A^c,A)$ for any subset $A\subset S$}{50}{subsection.7.2.2}%
\contentsline {subsection}{\numberline {7.2.3}Lemma: Local balance $\Rightarrow $ $\pi $ is stationary}{51}{subsection.7.2.3}%
\contentsline {subsection}{\numberline {7.2.4}Lemma: All stationary birth and death chains are reversible}{51}{subsection.7.2.4}%
\contentsline {section}{\numberline {7.3}Example: Random Walk on an Undirected Graph}{51}{section.7.3}%
\contentsline {chapter}{\numberline {8}Markov Chain Monte Carlo (MCMC)}{53}{chapter.8}%
\contentsline {section}{\numberline {8.1}Strong Law of Large Numbers for Markov Chains}{53}{section.8.1}%
\contentsline {section}{\numberline {8.2}Example of Designing MC}{53}{section.8.2}%
\contentsline {section}{\numberline {8.3}Metropolis Hastings Algorithm}{54}{section.8.3}%
\contentsline {subsection}{\numberline {8.3.1}Example of generate standard normal distribution with uniform}{54}{subsection.8.3.1}%
\contentsline {subsection}{\numberline {8.3.2}Without MCMC: Box Muller Transform}{55}{subsection.8.3.2}%
\contentsline {section}{\numberline {8.4}Gibbs Sampling}{55}{section.8.4}%
\contentsline {subsection}{\numberline {8.4.1} Systematic scan Gibbs sampler}{55}{subsection.8.4.1}%
\contentsline {subsection}{\numberline {8.4.2} Random Scan Gibbs sampler}{56}{subsection.8.4.2}%
\contentsline {subsection}{\numberline {8.4.3}Example: Bivariate Normal Distribution}{57}{subsection.8.4.3}%
\contentsline {subsection}{\numberline {8.4.4}Example: Potts model (Ising model)}{57}{subsection.8.4.4}%
\contentsline {section}{\numberline {8.5}A Linear Algebraic Condition for Convergence}{58}{section.8.5}%
\contentsline {chapter}{\numberline {9}Poisson Process}{60}{chapter.9}%
\contentsline {section}{\numberline {9.1}Basics of Poisson Process}{60}{section.9.1}%
\contentsline {subsection}{\numberline {9.1.1}Counting Process}{60}{subsection.9.1.1}%
\contentsline {subsection}{\numberline {9.1.2}Poisson Distribution}{60}{subsection.9.1.2}%
\contentsline {subsection}{\numberline {9.1.3}Definition of Poisson Process}{61}{subsection.9.1.3}%
\contentsline {section}{\numberline {9.2}Inter-Arrival Times}{61}{section.9.2}%
\contentsline {subsection}{\numberline {9.2.1}First arrival time: Exponential distribution $Exp(\lambda )$}{61}{subsection.9.2.1}%
\contentsline {subsection}{\numberline {9.2.2}$k^{th}$ arrival time: Gamma distribution $Gamma(n,\lambda )$}{62}{subsection.9.2.2}%
\contentsline {subsection}{\numberline {9.2.3}Memorylessness of the Exponential Random Variable}{63}{subsection.9.2.3}%
\contentsline {section}{\numberline {9.3}Conditioning on the number of arrivals in a Poisson Process: Uniform}{63}{section.9.3}%
\contentsline {section}{\numberline {9.4}Superposition}{66}{section.9.4}%
\contentsline {subsection}{\numberline {9.4.1}Independent Poisson variables: $\DOTSB \sum@ \slimits@ _{i=1}^n Y_i\sim \textnormal {Poi}\left (\DOTSB \sum@ \slimits@ _{i=1}^n\lambda _i\right )$}{66}{subsection.9.4.1}%
\contentsline {subsection}{\numberline {9.4.2}Superposition Theorem: PP with $\lambda _1$ + PP with $\lambda _2$ = PP with $\lambda _1+\lambda _2$}{67}{subsection.9.4.2}%
\contentsline {subsection}{\numberline {9.4.3}Probability of type 1 event before type 2 event: $\frac {\lambda _1}{\lambda _1+\lambda _2}$}{67}{subsection.9.4.3}%
\contentsline {section}{\numberline {9.5}Thinning: PP can be divided into two independent PP}{68}{section.9.5}%
\contentsline {section}{\numberline {9.6}Variants of Poisson process}{70}{section.9.6}%
\contentsline {subsection}{\numberline {9.6.1}Spatial Poisson Process (dimension $\geq 2$)}{70}{subsection.9.6.1}%
\contentsline {subsection}{\numberline {9.6.2}Non Homogeneous Poisson Process}{71}{subsection.9.6.2}%
\contentsline {chapter}{\numberline {10}Brownian Motion}{72}{chapter.10}%
\contentsline {section}{\numberline {10.1}Brownian Motion}{72}{section.10.1}%
\contentsline {subsection}{\numberline {10.1.1}Definition}{72}{subsection.10.1.1}%
\contentsline {subsection}{\numberline {10.1.2}Sufficient Condition for BM}{72}{subsection.10.1.2}%
\contentsline {subsection}{\numberline {10.1.3}Standard Brownian Motion and Transformations}{73}{subsection.10.1.3}%
\contentsline {subsection}{\numberline {10.1.4}Brownian Motion as a limit of Random Walk}{73}{subsection.10.1.4}%
\contentsline {section}{\numberline {10.2} Gaussian Process}{73}{section.10.2}%
\contentsline {section}{\numberline {10.3}Transformations and Properties}{74}{section.10.3}%
