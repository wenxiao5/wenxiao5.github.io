\contentsline {chapter}{\numberline {1}Metric Spaces Foundations \small {(Lec 01 @ ECON 240A)}}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Probability Space}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Sample Space}{1}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}$\sigma -$Algebra}{1}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Probability Function}{2}{subsection.1.1.3}%
\contentsline {section}{\numberline {1.2}Random Variables}{2}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Random Variable}{2}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Represeting / Specifying $P_X$: Cumulative Distribution Function}{3}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Correspondence Theorem; CDF $\Leftrightarrow $ Probability Function}{3}{subsection.1.2.3}%
\contentsline {subsection}{\numberline {1.2.4}Discrete / Continuous Random Variable}{3}{subsection.1.2.4}%
\contentsline {chapter}{\numberline {2}Basis}{5}{chapter.2}%
\contentsline {section}{\numberline {2.1}Covariance and Variance}{5}{section.2.1}%
\contentsline {section}{\numberline {2.2}Conditional Expectation and Variance}{5}{section.2.2}%
\contentsline {section}{\numberline {2.3}Gambler's Ruin}{5}{section.2.3}%
\contentsline {section}{\numberline {2.4}Moment Generating Function (MGF)}{6}{section.2.4}%
\contentsline {section}{\numberline {2.5}Inequality}{6}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}Cauchy-Schwarz inequality: $|\mathbb {E}XY|\leq \sqrt {\mathbb {E}X^2\cdot \mathbb {E}Y^2}$}{6}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}Jensen's Inequality: convex $g$ $\Rightarrow $ $\mathbb {E}(g(X))\geq g(\mathbb {E}(X))$}{7}{subsection.2.5.2}%
\contentsline {subsection}{\numberline {2.5.3}Markov's Inequality: $P(|X|\geq a)\leq \frac {\mathbb {E}|X|}{a}$}{7}{subsection.2.5.3}%
\contentsline {subsection}{\numberline {2.5.4}Chebychev's inequality: $P(|X-\mu |\geq a)\leq \frac {\sigma ^2}{a^2}$}{7}{subsection.2.5.4}%
\contentsline {subsection}{\numberline {2.5.5}Chernoff Inequality: $P(X\geq a)\leq \frac {\mathbb {E}e^{tX}}{e^{ta}}$}{7}{subsection.2.5.5}%
\contentsline {section}{\numberline {2.6}Law of Large Numbers (LLN)}{7}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Weak Law of Large Numbers (wLLN)}{8}{subsection.2.6.1}%
\contentsline {subsection}{\numberline {2.6.2}Strong Law of Large Numbers (sLLN)}{8}{subsection.2.6.2}%
\contentsline {subsection}{\numberline {2.6.3}Differences between \underline {convergence in probability} (wLLN) and \underline {wp1(a.s.)} (sLLN)}{8}{subsection.2.6.3}%
\contentsline {section}{\numberline {2.7}Central Limit Theorem (CLT)}{9}{section.2.7}%
\contentsline {chapter}{\numberline {3}Distribution}{11}{chapter.3}%
\contentsline {section}{\numberline {3.1}Discrete}{11}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Bernoulli Distribution -- $\operatorname {Bernoulli}(\pi )$: an event happens with probability $\pi $}{11}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Binomial distribution -- $bin(n,\pi )$: $n$ independent Bernoulli distributions}{11}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}Multinomial Distribution}{11}{subsection.3.1.3}%
\contentsline {subsection}{\numberline {3.1.4}Poisson Distribution -- $Pois(\lambda )$: an event happens $k$ times within unit time}{12}{subsection.3.1.4}%
\contentsline {subsection}{\numberline {3.1.5}Connection between Poisson and multinomial distribution}{13}{subsection.3.1.5}%
\contentsline {subsection}{\numberline {3.1.6}Geometric distribution: $P(X=k)=(1-p)^{k-1}p$}{14}{subsection.3.1.6}%
\contentsline {section}{\numberline {3.2}Continuous}{14}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Exponential distribution $Exp(\lambda )$: interval between to independent identical event / the first time a event happened}{14}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Gaussian/Normal Distribution}{15}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}Multivariate/Joint Gaussian/Normal Distribution (MVN)}{16}{subsection.3.2.3}%
\contentsline {section}{\numberline {3.3}Poisson process: A sequence of arrivals in continuous time with rate $\lambda $}{17}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Definition}{17}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}$T_j$: time of $j^{th}$ arrival}{17}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Theorem (Conditional counts): $N(t_1)|N(t_2)=n\sim Bin(n,\frac {t_1}{t_2})$}{17}{subsection.3.3.3}%
\contentsline {chapter}{\numberline {4}Markov Chain}{18}{chapter.4}%
\contentsline {section}{\numberline {4.1}Definition}{18}{section.4.1}%
\contentsline {section}{\numberline {4.2}Matrix Computations}{18}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Chapman Kolmogorov Equations (C-K Equations) $P(X_{n+m}=j|X_0=i)=(P^{m+n})_{ij}=\DOTSB \sum@ \slimits@ _{k\in S}(P^{m})_{ik}(P^{n})_{kj}$}{19}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Marginal Distribution $P(X_n=j)=(\alpha P^n)_{j}$}{19}{subsection.4.2.2}%
\contentsline {section}{\numberline {4.3}States, Class}{19}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Irreducible, Reducible}{19}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Recurrent, Transient}{20}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}Periodicity}{21}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Lemma: all states in an irreducible MC have the same period}{21}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Periodic, Aperiodic}{21}{subsection.4.4.2}%
\contentsline {section}{\numberline {4.5}Regular Matrix}{21}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}Regular matrix: $\exists n\geq 1$ s.t. $P^n>0$}{21}{subsection.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2}Lemma: Finite MC is Irreducible, Aperiodic $\Leftrightarrow $ has Regular transition matrix}{22}{subsection.4.5.2}%
\contentsline {section}{\numberline {4.6}Eigenvalues of a Stochastic Matrix: $\lambda =1$ must exist and other $|\lambda |\leq 1$ (not equal when if regular matrix)}{22}{section.4.6}%
\contentsline {section}{\numberline {4.7}Long Run Behavior of Finite Markov Chains}{22}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}Limiting Distribution}{22}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}Stationary Distribution}{23}{subsection.4.7.2}%
\contentsline {subsection}{\numberline {4.7.3}Limiting Distribution = Expected Proportion of time in each state}{23}{subsection.4.7.3}%
\contentsline {subsection}{\numberline {4.7.4}Fundamental Theorem for \underline {Irreducible, Aperiodic, Finite MC} (Regular transition matrix) $\Rightarrow $ $\exists $ unique limiting distribution $\pi $ and $\pi _j>0,\forall j$}{24}{subsection.4.7.4}%
\contentsline {subsection}{\numberline {4.7.5}Long run behavior for reducible and/or periodic chains}{24}{subsection.4.7.5}%
\contentsline {subsection}{\numberline {4.7.6}Fundamental Theorem for \underline {Irreducible, Finite MC}: expected first return time $\mathbb {E}(T_j|X_0 = j)=\frac {1}{\pi _j}$}{25}{subsection.4.7.6}%
\contentsline {section}{\numberline {4.8}Return Times and Absorption Probabilities}{26}{section.4.8}%
\contentsline {subsection}{\numberline {4.8.1}Expected Number of Visits to a Transient State: $E(Y_i|X_0 = j) = M_{ji} = (I-Q)^{-1}_{ji}$}{26}{subsection.4.8.1}%
\contentsline {subsection}{\numberline {4.8.2}Expected Time till Absorption to a Recurrent Class: $\mathbb {E}(T_{abs}|X_0=j)=\DOTSB \sum@ \slimits@ _{i\in T_1\cup T_2\cup \cdots \cup T_s} M_{ji}$}{27}{subsection.4.8.2}%
\contentsline {subsection}{\numberline {4.8.3}Expected first return time (different initial state) = Time till Absorption}{28}{subsection.4.8.3}%
\contentsline {subsection}{\numberline {4.8.4}Probability of Eventually Entering a Given Recurrent Class: $A=(I-Q)^{-1}S=MS$}{29}{subsection.4.8.4}%
\contentsline {section}{\numberline {4.9}Examples of Finite MC}{30}{section.4.9}%
\contentsline {subsection}{\numberline {4.9.1}Gambler's Ruin}{30}{subsection.4.9.1}%
\contentsline {subsection}{\numberline {4.9.2}Simple Random Walk (SRW) on Undirected Graph}{30}{subsection.4.9.2}%
\contentsline {chapter}{\numberline {5} Countably infinite MC}{32}{chapter.5}%
\contentsline {section}{\numberline {5.1}Recurrence and Transience}{32}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Recurrent or Transient State}{32}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Recurrent or Transient Class}{33}{subsection.5.1.2}%
\contentsline {subsection}{\numberline {5.1.3}Lemma: Transient Class $\Leftrightarrow $ $\DOTSB \sum@ \slimits@ _{n=0}^\infty P_{i,i}^n<\infty $}{33}{subsection.5.1.3}%
\contentsline {subsection}{\numberline {5.1.4}Recurrence/Transience of Simple Random Walk on Lattice}{33}{subsection.5.1.4}%
\contentsline {subsection}{\numberline {5.1.5}Null and Positive Recurrence}{34}{subsection.5.1.5}%
\contentsline {subsection}{\numberline {5.1.6}Stationary Distribution and Limiting Distribution}{34}{subsection.5.1.6}%
\contentsline {section}{\numberline {5.2}Differences between Finite and (Countably) Infinite Markov Chains}{35}{section.5.2}%
\contentsline {chapter}{\numberline {6}Branching Process}{36}{chapter.6}%
\contentsline {section}{\numberline {6.1}Extinction Probability in a Branching Process}{36}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}Expectation $\mathbb {E}X_n=\mu ^n \mathbb {E}X_0$}{36}{subsection.6.1.1}%
\contentsline {subsection}{\numberline {6.1.2}Lemma: $\mu <1$ $\Rightarrow $ $P(extinction)=1$}{37}{subsection.6.1.2}%
\contentsline {subsection}{\numberline {6.1.3}Variance: $Var X_n=n\sigma ^2$ if $\mu =1$; $Var X_n=\sigma ^2\mu ^{n-1}\frac {\mu ^n-1}{\mu -1}$ if $\mu \neq 1$}{37}{subsection.6.1.3}%
\contentsline {subsection}{\numberline {6.1.4}Extinction probability $\rho =1$ if $\mu \leq 1$; $\rho <1$ if $\mu >1$}{37}{subsection.6.1.4}%
\contentsline {subsection}{\numberline {6.1.5}$G_n(s)=G_{n-1}(\psi (s))=\psi (\psi (\psi (\cdots \psi (s)\cdots )))=\psi (G_{n-1}(s))$}{39}{subsection.6.1.5}%
\contentsline {chapter}{\numberline {7}Time Reversible Markov Chains}{41}{chapter.7}%
\contentsline {section}{\numberline {7.1}Definition: Local Balance $\pi (i)P(i,j)=\pi (j)P(j,i),\forall i,j\in S$}{41}{section.7.1}%
\contentsline {section}{\numberline {7.2}Discussion about Local Balance}{41}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}Flow: $Flow(A,B)=\DOTSB \sum@ \slimits@ _{i\in A}\DOTSB \sum@ \slimits@ _{j\in B}\pi (i)P_{ij}$}{41}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}Lemma: $Flow(A,A^c)=Flow(A^c,A)$ for any subset $A\subset S$}{41}{subsection.7.2.2}%
\contentsline {subsection}{\numberline {7.2.3}Lemma: Local balance $\Rightarrow $ $\pi $ is stationary}{42}{subsection.7.2.3}%
\contentsline {subsection}{\numberline {7.2.4}Lemma: All stationary birth and death chains are reversible}{42}{subsection.7.2.4}%
\contentsline {section}{\numberline {7.3}Example: Random Walk on an Undirected Graph}{42}{section.7.3}%
\contentsline {chapter}{\numberline {8}Markov Chain Monte Carlo (MCMC)}{44}{chapter.8}%
\contentsline {section}{\numberline {8.1}Strong Law of Large Numbers for Markov Chains}{44}{section.8.1}%
\contentsline {section}{\numberline {8.2}Example of Designing MC}{44}{section.8.2}%
\contentsline {section}{\numberline {8.3}Metropolis Hastings Algorithm}{45}{section.8.3}%
\contentsline {subsection}{\numberline {8.3.1}Example of generate standard normal distribution with uniform}{45}{subsection.8.3.1}%
\contentsline {subsection}{\numberline {8.3.2}Without MCMC: Box Muller Transform}{46}{subsection.8.3.2}%
\contentsline {section}{\numberline {8.4}Gibbs Sampling}{46}{section.8.4}%
\contentsline {subsection}{\numberline {8.4.1} Systematic scan Gibbs sampler}{46}{subsection.8.4.1}%
\contentsline {subsection}{\numberline {8.4.2} Random Scan Gibbs sampler}{47}{subsection.8.4.2}%
\contentsline {subsection}{\numberline {8.4.3}Example: Bivariate Normal Distribution}{48}{subsection.8.4.3}%
\contentsline {subsection}{\numberline {8.4.4}Example: Potts model (Ising model)}{48}{subsection.8.4.4}%
\contentsline {section}{\numberline {8.5}A Linear Algebraic Condition for Convergence}{49}{section.8.5}%
\contentsline {chapter}{\numberline {9}Poisson Process}{51}{chapter.9}%
\contentsline {section}{\numberline {9.1}Basics of Poisson Process}{51}{section.9.1}%
\contentsline {subsection}{\numberline {9.1.1}Counting Process}{51}{subsection.9.1.1}%
\contentsline {subsection}{\numberline {9.1.2}Poisson Distribution}{51}{subsection.9.1.2}%
\contentsline {subsection}{\numberline {9.1.3}Definition of Poisson Process}{52}{subsection.9.1.3}%
\contentsline {section}{\numberline {9.2}Inter-Arrival Times}{52}{section.9.2}%
\contentsline {subsection}{\numberline {9.2.1}First arrival time: Exponential distribution $Exp(\lambda )$}{52}{subsection.9.2.1}%
\contentsline {subsection}{\numberline {9.2.2}$k^{th}$ arrival time: Gamma distribution $Gamma(n,\lambda )$}{53}{subsection.9.2.2}%
\contentsline {subsection}{\numberline {9.2.3}Memorylessness of the Exponential Random Variable}{54}{subsection.9.2.3}%
\contentsline {section}{\numberline {9.3}Conditioning on the number of arrivals in a Poisson Process: Uniform}{54}{section.9.3}%
\contentsline {section}{\numberline {9.4}Superposition}{57}{section.9.4}%
\contentsline {subsection}{\numberline {9.4.1}Independent Poisson variables: $\DOTSB \sum@ \slimits@ _{i=1}^n Y_i\sim \textnormal {Poi}\left (\DOTSB \sum@ \slimits@ _{i=1}^n\lambda _i\right )$}{57}{subsection.9.4.1}%
\contentsline {subsection}{\numberline {9.4.2}Superposition Theorem: PP with $\lambda _1$ + PP with $\lambda _2$ = PP with $\lambda _1+\lambda _2$}{58}{subsection.9.4.2}%
\contentsline {subsection}{\numberline {9.4.3}Probability of type 1 event before type 2 event: $\frac {\lambda _1}{\lambda _1+\lambda _2}$}{58}{subsection.9.4.3}%
\contentsline {section}{\numberline {9.5}Thinning: PP can be divided into two independent PP}{59}{section.9.5}%
\contentsline {section}{\numberline {9.6}Variants of Poisson process}{61}{section.9.6}%
\contentsline {subsection}{\numberline {9.6.1}Spatial Poisson Process (dimension $\geq 2$)}{61}{subsection.9.6.1}%
\contentsline {subsection}{\numberline {9.6.2}Non Homogeneous Poisson Process}{62}{subsection.9.6.2}%
\contentsline {chapter}{\numberline {10}Brownian Motion}{63}{chapter.10}%
\contentsline {section}{\numberline {10.1}Brownian Motion}{63}{section.10.1}%
\contentsline {subsection}{\numberline {10.1.1}Definition}{63}{subsection.10.1.1}%
\contentsline {subsection}{\numberline {10.1.2}Sufficient Condition for BM}{63}{subsection.10.1.2}%
\contentsline {subsection}{\numberline {10.1.3}Standard Brownian Motion and Transformations}{64}{subsection.10.1.3}%
\contentsline {subsection}{\numberline {10.1.4}Brownian Motion as a limit of Random Walk}{64}{subsection.10.1.4}%
\contentsline {section}{\numberline {10.2} Gaussian Process}{64}{section.10.2}%
\contentsline {section}{\numberline {10.3}Transformations and Properties}{65}{section.10.3}%
