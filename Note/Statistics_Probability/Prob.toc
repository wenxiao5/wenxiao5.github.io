\contentsline {chapter}{\numberline {1}Metric Spaces Foundations \small {(Lec 01 @ ECON 240A)}}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Probability Space}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Sample Space}{1}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}$\sigma -$Algebra}{1}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Probability Function}{2}{subsection.1.1.3}%
\contentsline {section}{\numberline {1.2}Random Variables}{2}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Random Variable}{2}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Represeting / Specifying $P_X$: Cumulative Distribution Function}{3}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Correspondence Theorem; CDF $\Leftrightarrow $ Probability Function}{3}{subsection.1.2.3}%
\contentsline {subsection}{\numberline {1.2.4}Discrete / Continuous Random Variable}{3}{subsection.1.2.4}%
\contentsline {section}{\numberline {1.3}Expectation, Moment, Mean, Central Moment, Variance}{4}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Expectation}{4}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Moment, Mean, Central Moment, Variance}{5}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}$\textnormal {Var}(X)=\mathbb {E}(X^2)-\mathbb {E}(X)^2$, $\mathbb {E}(aX+b)=a \mathbb {E}(X)+b$, $\textnormal {Var}(aX+b)=a^2 \textnormal {Var}(X)$}{5}{subsection.1.3.3}%
\contentsline {section}{\numberline {1.4}Univariate Distribution}{5}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Bernoulli Distribution}{5}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Normal Distribution}{6}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}Uniform Distribution}{6}{subsection.1.4.3}%
\contentsline {section}{\numberline {1.5}Exponential Families}{6}{section.1.5}%
\contentsline {section}{\numberline {1.6}Multiple Random Variables}{8}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1}Bivariate Random Vector}{8}{subsection.1.6.1}%
\contentsline {subsection}{\numberline {1.6.2}Marginal CDF, PMF, PDF}{9}{subsection.1.6.2}%
\contentsline {subsection}{\numberline {1.6.3}Independent Random Variables}{9}{subsection.1.6.3}%
\contentsline {subsection}{\numberline {1.6.4}Conditional PMF, PDF, Expected Value, Mean, Variance}{10}{subsection.1.6.4}%
\contentsline {subsection}{\numberline {1.6.5}Law of Iterated Expectation: $\mathbb {E}_X(X)=\mathbb {E}_Y[\mathbb {E}_{X|Y}(X\mid Y)]$}{10}{subsection.1.6.5}%
\contentsline {subsection}{\numberline {1.6.6}Law of Total Variance: $\textnormal {Var}_X(X)=\mathbb {E}_Y[\textnormal {Var}_{X|Y}(X\mid Y)]+\textnormal {Var}_Y[\mathbb {E}_{X|Y}(X\mid Y)]$}{11}{subsection.1.6.6}%
\contentsline {subsection}{\numberline {1.6.7}Covariance}{11}{subsection.1.6.7}%
\contentsline {section}{\numberline {1.7}Random Sampling}{12}{section.1.7}%
\contentsline {subsection}{\numberline {1.7.1}Sample Mean and Sample Variance}{12}{subsection.1.7.1}%
\contentsline {subsection}{\numberline {1.7.2}Distributional Properties}{13}{subsection.1.7.2}%
\contentsline {subsection}{\numberline {1.7.3}Order Statistics}{13}{subsection.1.7.3}%
\contentsline {section}{\numberline {1.8}Basic Statistics}{13}{section.1.8}%
\contentsline {chapter}{\numberline {2}Basis}{15}{chapter.2}%
\contentsline {section}{\numberline {2.1}Covariance and Variance}{15}{section.2.1}%
\contentsline {section}{\numberline {2.2}Conditional Expectation and Variance}{15}{section.2.2}%
\contentsline {section}{\numberline {2.3}Gambler's Ruin}{15}{section.2.3}%
\contentsline {section}{\numberline {2.4}Moment Generating Function (MGF)}{16}{section.2.4}%
\contentsline {section}{\numberline {2.5}Inequality}{16}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}Cauchy-Schwarz inequality: $|\mathbb {E}XY|\leq \sqrt {\mathbb {E}X^2\cdot \mathbb {E}Y^2}$}{16}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}Jensen's Inequality: convex $g$ $\Rightarrow $ $\mathbb {E}(g(X))\geq g(\mathbb {E}(X))$}{17}{subsection.2.5.2}%
\contentsline {subsection}{\numberline {2.5.3}Markov's Inequality: $P(|X|\geq a)\leq \frac {\mathbb {E}|X|}{a}$}{17}{subsection.2.5.3}%
\contentsline {subsection}{\numberline {2.5.4}Chebychev's inequality: $P(|X-\mu |\geq a)\leq \frac {\sigma ^2}{a^2}$}{17}{subsection.2.5.4}%
\contentsline {subsection}{\numberline {2.5.5}Chernoff Inequality: $P(X\geq a)\leq \frac {\mathbb {E}e^{tX}}{e^{ta}}$}{17}{subsection.2.5.5}%
\contentsline {section}{\numberline {2.6}Law of Large Numbers (LLN)}{17}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Weak Law of Large Numbers (wLLN)}{18}{subsection.2.6.1}%
\contentsline {subsection}{\numberline {2.6.2}Strong Law of Large Numbers (sLLN)}{18}{subsection.2.6.2}%
\contentsline {subsection}{\numberline {2.6.3}Differences between \underline {convergence in probability} (wLLN) and \underline {wp1(a.s.)} (sLLN)}{18}{subsection.2.6.3}%
\contentsline {section}{\numberline {2.7}Central Limit Theorem (CLT)}{19}{section.2.7}%
\contentsline {chapter}{\numberline {3}Distribution}{21}{chapter.3}%
\contentsline {section}{\numberline {3.1}Discrete}{21}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Bernoulli Distribution -- $\operatorname {Bernoulli}(\pi )$: an event happens with probability $\pi $}{21}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Binomial distribution -- $bin(n,\pi )$: $n$ independent Bernoulli distributions}{21}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}Multinomial Distribution}{21}{subsection.3.1.3}%
\contentsline {subsection}{\numberline {3.1.4}Poisson Distribution -- $Pois(\lambda )$: an event happens $k$ times within unit time}{22}{subsection.3.1.4}%
\contentsline {subsection}{\numberline {3.1.5}Connection between Poisson and multinomial distribution}{23}{subsection.3.1.5}%
\contentsline {subsection}{\numberline {3.1.6}Geometric distribution: $P(X=k)=(1-p)^{k-1}p$}{24}{subsection.3.1.6}%
\contentsline {section}{\numberline {3.2}Continuous}{24}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Exponential distribution $Exp(\lambda )$: interval between to independent identical event / the first time a event happened}{24}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Gaussian/Normal Distribution}{25}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}Multivariate/Joint Gaussian/Normal Distribution (MVN)}{26}{subsection.3.2.3}%
\contentsline {section}{\numberline {3.3}Poisson process: A sequence of arrivals in continuous time with rate $\lambda $}{27}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Definition}{27}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}$T_j$: time of $j^{th}$ arrival}{27}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Theorem (Conditional counts): $N(t_1)|N(t_2)=n\sim Bin(n,\frac {t_1}{t_2})$}{27}{subsection.3.3.3}%
\contentsline {chapter}{\numberline {4}Markov Chain}{28}{chapter.4}%
\contentsline {section}{\numberline {4.1}Definition}{28}{section.4.1}%
\contentsline {section}{\numberline {4.2}Matrix Computations}{28}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Chapman Kolmogorov Equations (C-K Equations) $P(X_{n+m}=j|X_0=i)=(P^{m+n})_{ij}=\DOTSB \sum@ \slimits@ _{k\in S}(P^{m})_{ik}(P^{n})_{kj}$}{29}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Marginal Distribution $P(X_n=j)=(\alpha P^n)_{j}$}{29}{subsection.4.2.2}%
\contentsline {section}{\numberline {4.3}States, Class}{29}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Irreducible, Reducible}{29}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Recurrent, Transient}{30}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}Periodicity}{31}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Lemma: all states in an irreducible MC have the same period}{31}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Periodic, Aperiodic}{31}{subsection.4.4.2}%
\contentsline {section}{\numberline {4.5}Regular Matrix}{31}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}Regular matrix: $\exists n\geq 1$ s.t. $P^n>0$}{31}{subsection.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2}Lemma: Finite MC is Irreducible, Aperiodic $\Leftrightarrow $ has Regular transition matrix}{32}{subsection.4.5.2}%
\contentsline {section}{\numberline {4.6}Eigenvalues of a Stochastic Matrix: $\lambda =1$ must exist and other $|\lambda |\leq 1$ (not equal when if regular matrix)}{32}{section.4.6}%
\contentsline {section}{\numberline {4.7}Long Run Behavior of Finite Markov Chains}{32}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}Limiting Distribution}{32}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}Stationary Distribution}{33}{subsection.4.7.2}%
\contentsline {subsection}{\numberline {4.7.3}Limiting Distribution = Expected Proportion of time in each state}{33}{subsection.4.7.3}%
\contentsline {subsection}{\numberline {4.7.4}Fundamental Theorem for \underline {Irreducible, Aperiodic, Finite MC} (Regular transition matrix) $\Rightarrow $ $\exists $ unique limiting distribution $\pi $ and $\pi _j>0,\forall j$}{34}{subsection.4.7.4}%
\contentsline {subsection}{\numberline {4.7.5}Long run behavior for reducible and/or periodic chains}{34}{subsection.4.7.5}%
\contentsline {subsection}{\numberline {4.7.6}Fundamental Theorem for \underline {Irreducible, Finite MC}: expected first return time $\mathbb {E}(T_j|X_0 = j)=\frac {1}{\pi _j}$}{35}{subsection.4.7.6}%
\contentsline {section}{\numberline {4.8}Return Times and Absorption Probabilities}{36}{section.4.8}%
\contentsline {subsection}{\numberline {4.8.1}Expected Number of Visits to a Transient State: $E(Y_i|X_0 = j) = M_{ji} = (I-Q)^{-1}_{ji}$}{36}{subsection.4.8.1}%
\contentsline {subsection}{\numberline {4.8.2}Expected Time till Absorption to a Recurrent Class: $\mathbb {E}(T_{abs}|X_0=j)=\DOTSB \sum@ \slimits@ _{i\in T_1\cup T_2\cup \cdots \cup T_s} M_{ji}$}{37}{subsection.4.8.2}%
\contentsline {subsection}{\numberline {4.8.3}Expected first return time (different initial state) = Time till Absorption}{38}{subsection.4.8.3}%
\contentsline {subsection}{\numberline {4.8.4}Probability of Eventually Entering a Given Recurrent Class: $A=(I-Q)^{-1}S=MS$}{39}{subsection.4.8.4}%
\contentsline {section}{\numberline {4.9}Examples of Finite MC}{40}{section.4.9}%
\contentsline {subsection}{\numberline {4.9.1}Gambler's Ruin}{40}{subsection.4.9.1}%
\contentsline {subsection}{\numberline {4.9.2}Simple Random Walk (SRW) on Undirected Graph}{40}{subsection.4.9.2}%
\contentsline {chapter}{\numberline {5} Countably infinite MC}{42}{chapter.5}%
\contentsline {section}{\numberline {5.1}Recurrence and Transience}{42}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Recurrent or Transient State}{42}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Recurrent or Transient Class}{43}{subsection.5.1.2}%
\contentsline {subsection}{\numberline {5.1.3}Lemma: Transient Class $\Leftrightarrow $ $\DOTSB \sum@ \slimits@ _{n=0}^\infty P_{i,i}^n<\infty $}{43}{subsection.5.1.3}%
\contentsline {subsection}{\numberline {5.1.4}Recurrence/Transience of Simple Random Walk on Lattice}{43}{subsection.5.1.4}%
\contentsline {subsection}{\numberline {5.1.5}Null and Positive Recurrence}{44}{subsection.5.1.5}%
\contentsline {subsection}{\numberline {5.1.6}Stationary Distribution and Limiting Distribution}{44}{subsection.5.1.6}%
\contentsline {section}{\numberline {5.2}Differences between Finite and (Countably) Infinite Markov Chains}{45}{section.5.2}%
\contentsline {chapter}{\numberline {6}Branching Process}{46}{chapter.6}%
\contentsline {section}{\numberline {6.1}Extinction Probability in a Branching Process}{46}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}Expectation $\mathbb {E}X_n=\mu ^n \mathbb {E}X_0$}{46}{subsection.6.1.1}%
\contentsline {subsection}{\numberline {6.1.2}Lemma: $\mu <1$ $\Rightarrow $ $P(extinction)=1$}{47}{subsection.6.1.2}%
\contentsline {subsection}{\numberline {6.1.3}Variance: $Var X_n=n\sigma ^2$ if $\mu =1$; $Var X_n=\sigma ^2\mu ^{n-1}\frac {\mu ^n-1}{\mu -1}$ if $\mu \neq 1$}{47}{subsection.6.1.3}%
\contentsline {subsection}{\numberline {6.1.4}Extinction probability $\rho =1$ if $\mu \leq 1$; $\rho <1$ if $\mu >1$}{47}{subsection.6.1.4}%
\contentsline {subsection}{\numberline {6.1.5}$G_n(s)=G_{n-1}(\psi (s))=\psi (\psi (\psi (\cdots \psi (s)\cdots )))=\psi (G_{n-1}(s))$}{49}{subsection.6.1.5}%
\contentsline {chapter}{\numberline {7}Time Reversible Markov Chains}{51}{chapter.7}%
\contentsline {section}{\numberline {7.1}Definition: Local Balance $\pi (i)P(i,j)=\pi (j)P(j,i),\forall i,j\in S$}{51}{section.7.1}%
\contentsline {section}{\numberline {7.2}Discussion about Local Balance}{51}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}Flow: $Flow(A,B)=\DOTSB \sum@ \slimits@ _{i\in A}\DOTSB \sum@ \slimits@ _{j\in B}\pi (i)P_{ij}$}{51}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}Lemma: $Flow(A,A^c)=Flow(A^c,A)$ for any subset $A\subset S$}{51}{subsection.7.2.2}%
\contentsline {subsection}{\numberline {7.2.3}Lemma: Local balance $\Rightarrow $ $\pi $ is stationary}{52}{subsection.7.2.3}%
\contentsline {subsection}{\numberline {7.2.4}Lemma: All stationary birth and death chains are reversible}{52}{subsection.7.2.4}%
\contentsline {section}{\numberline {7.3}Example: Random Walk on an Undirected Graph}{52}{section.7.3}%
\contentsline {chapter}{\numberline {8}Markov Chain Monte Carlo (MCMC)}{54}{chapter.8}%
\contentsline {section}{\numberline {8.1}Strong Law of Large Numbers for Markov Chains}{54}{section.8.1}%
\contentsline {section}{\numberline {8.2}Example of Designing MC}{54}{section.8.2}%
\contentsline {section}{\numberline {8.3}Metropolis Hastings Algorithm}{55}{section.8.3}%
\contentsline {subsection}{\numberline {8.3.1}Example of generate standard normal distribution with uniform}{55}{subsection.8.3.1}%
\contentsline {subsection}{\numberline {8.3.2}Without MCMC: Box Muller Transform}{56}{subsection.8.3.2}%
\contentsline {section}{\numberline {8.4}Gibbs Sampling}{56}{section.8.4}%
\contentsline {subsection}{\numberline {8.4.1} Systematic scan Gibbs sampler}{56}{subsection.8.4.1}%
\contentsline {subsection}{\numberline {8.4.2} Random Scan Gibbs sampler}{57}{subsection.8.4.2}%
\contentsline {subsection}{\numberline {8.4.3}Example: Bivariate Normal Distribution}{58}{subsection.8.4.3}%
\contentsline {subsection}{\numberline {8.4.4}Example: Potts model (Ising model)}{58}{subsection.8.4.4}%
\contentsline {section}{\numberline {8.5}A Linear Algebraic Condition for Convergence}{59}{section.8.5}%
\contentsline {chapter}{\numberline {9}Poisson Process}{61}{chapter.9}%
\contentsline {section}{\numberline {9.1}Basics of Poisson Process}{61}{section.9.1}%
\contentsline {subsection}{\numberline {9.1.1}Counting Process}{61}{subsection.9.1.1}%
\contentsline {subsection}{\numberline {9.1.2}Poisson Distribution}{61}{subsection.9.1.2}%
\contentsline {subsection}{\numberline {9.1.3}Definition of Poisson Process}{62}{subsection.9.1.3}%
\contentsline {section}{\numberline {9.2}Inter-Arrival Times}{62}{section.9.2}%
\contentsline {subsection}{\numberline {9.2.1}First arrival time: Exponential distribution $Exp(\lambda )$}{62}{subsection.9.2.1}%
\contentsline {subsection}{\numberline {9.2.2}$k^{th}$ arrival time: Gamma distribution $Gamma(n,\lambda )$}{63}{subsection.9.2.2}%
\contentsline {subsection}{\numberline {9.2.3}Memorylessness of the Exponential Random Variable}{64}{subsection.9.2.3}%
\contentsline {section}{\numberline {9.3}Conditioning on the number of arrivals in a Poisson Process: Uniform}{64}{section.9.3}%
\contentsline {section}{\numberline {9.4}Superposition}{67}{section.9.4}%
\contentsline {subsection}{\numberline {9.4.1}Independent Poisson variables: $\DOTSB \sum@ \slimits@ _{i=1}^n Y_i\sim \textnormal {Poi}\left (\DOTSB \sum@ \slimits@ _{i=1}^n\lambda _i\right )$}{67}{subsection.9.4.1}%
\contentsline {subsection}{\numberline {9.4.2}Superposition Theorem: PP with $\lambda _1$ + PP with $\lambda _2$ = PP with $\lambda _1+\lambda _2$}{68}{subsection.9.4.2}%
\contentsline {subsection}{\numberline {9.4.3}Probability of type 1 event before type 2 event: $\frac {\lambda _1}{\lambda _1+\lambda _2}$}{68}{subsection.9.4.3}%
\contentsline {section}{\numberline {9.5}Thinning: PP can be divided into two independent PP}{69}{section.9.5}%
\contentsline {section}{\numberline {9.6}Variants of Poisson process}{71}{section.9.6}%
\contentsline {subsection}{\numberline {9.6.1}Spatial Poisson Process (dimension $\geq 2$)}{71}{subsection.9.6.1}%
\contentsline {subsection}{\numberline {9.6.2}Non Homogeneous Poisson Process}{72}{subsection.9.6.2}%
\contentsline {chapter}{\numberline {10}Brownian Motion}{73}{chapter.10}%
\contentsline {section}{\numberline {10.1}Brownian Motion}{73}{section.10.1}%
\contentsline {subsection}{\numberline {10.1.1}Definition}{73}{subsection.10.1.1}%
\contentsline {subsection}{\numberline {10.1.2}Sufficient Condition for BM}{73}{subsection.10.1.2}%
\contentsline {subsection}{\numberline {10.1.3}Standard Brownian Motion and Transformations}{74}{subsection.10.1.3}%
\contentsline {subsection}{\numberline {10.1.4}Brownian Motion as a limit of Random Walk}{74}{subsection.10.1.4}%
\contentsline {section}{\numberline {10.2} Gaussian Process}{74}{section.10.2}%
\contentsline {section}{\numberline {10.3}Transformations and Properties}{75}{section.10.3}%
